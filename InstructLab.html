<!DOCTYPE html><html><head><meta charset="utf-8"><title>InstructLab.md</title><style>@charset "UTF-8";@import 'https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha2/katex.min.css';code{color:#c7254e;background-color:#f9f2f4;border-radius:4px}code,kbd{padding:2px 4px}kbd{color:#fff;background-color:#333;border-radius:3px;box-shadow:inset 0 -1px 0 rgba(0,0,0,.25)}kbd kbd{padding:0;font-size:100%;box-shadow:none}pre{display:block;margin:0 0 10px;word-break:break-all;word-wrap:break-word;color:#333;background-color:#f5f5f5;border:1px solid #ccc;border-radius:4px}pre code{padding:0;font-size:inherit;color:inherit;white-space:pre-wrap;background-color:transparent;border-radius:0}.pre-scrollable{max-height:340px;overflow-y:scroll}table{background-color:transparent}th{text-align:left}.table{width:100%;max-width:100%;margin-bottom:20px}.table>thead>tr>th{padding:8px;line-height:1.4285714;border-top:1px solid #ddd}.table>thead>tr>td,.table>tbody>tr>th,.table>tbody>tr>td,.table>tfoot>tr>th,.table>tfoot>tr>td{padding:8px;line-height:1.4285714;vertical-align:top;border-top:1px solid #ddd}.table>thead>tr>th{vertical-align:bottom;border-bottom:2px solid #ddd}.table>caption+thead>tr:first-child>th,.table>caption+thead>tr:first-child>td,.table>colgroup+thead>tr:first-child>th,.table>colgroup+thead>tr:first-child>td,.table>thead:first-child>tr:first-child>th,.table>thead:first-child>tr:first-child>td{border-top:0}.table>tbody+tbody{border-top:2px solid #ddd}.table .table{background-color:#fff}.table-condensed>thead>tr>th,.table-condensed>thead>tr>td,.table-condensed>tbody>tr>th,.table-condensed>tbody>tr>td,.table-condensed>tfoot>tr>th,.table-condensed>tfoot>tr>td{padding:5px}.table-bordered,.table-bordered>thead>tr>th,.table-bordered>thead>tr>td,.table-bordered>tbody>tr>th,.table-bordered>tbody>tr>td,.table-bordered>tfoot>tr>th,.table-bordered>tfoot>tr>td{border:1px solid #ddd}.table-bordered>thead>tr>th,.table-bordered>thead>tr>td{border-bottom-width:2px}.table-striped>tbody>tr:nth-child(odd)>td,.table-striped>tbody>tr:nth-child(odd)>th{background-color:#f9f9f9}.table-hover>tbody>tr:hover>td,.table-hover>tbody>tr:hover>th{background-color:#f5f5f5}table col[class*="col-"]{position:static;float:none;display:table-column}table td[class*="col-"],table th[class*="col-"]{position:static;float:none;display:table-cell}.table>thead>tr>td.active,.table>thead>tr>th.active,.table>thead>tr.active>td,.table>thead>tr.active>th,.table>tbody>tr>td.active,.table>tbody>tr>th.active,.table>tbody>tr.active>td,.table>tbody>tr.active>th,.table>tfoot>tr>td.active,.table>tfoot>tr>th.active,.table>tfoot>tr.active>td,.table>tfoot>tr.active>th{background-color:#f5f5f5}.table-hover>tbody>tr>td.active:hover,.table-hover>tbody>tr>th.active:hover,.table-hover>tbody>tr.active:hover>td,.table-hover>tbody>tr:hover>.active,.table-hover>tbody>tr.active:hover>th{background-color:#e8e8e8}.table>thead>tr>td.success,.table>thead>tr>th.success,.table>thead>tr.success>td,.table>thead>tr.success>th,.table>tbody>tr>td.success,.table>tbody>tr>th.success,.table>tbody>tr.success>td,.table>tbody>tr.success>th,.table>tfoot>tr>td.success,.table>tfoot>tr>th.success,.table>tfoot>tr.success>td,.table>tfoot>tr.success>th{background-color:#dff0d8}.table-hover>tbody>tr>td.success:hover,.table-hover>tbody>tr>th.success:hover,.table-hover>tbody>tr.success:hover>td,.table-hover>tbody>tr:hover>.success,.table-hover>tbody>tr.success:hover>th{background-color:#d0e9c6}.table>thead>tr>td.info,.table>thead>tr>th.info,.table>thead>tr.info>td,.table>thead>tr.info>th,.table>tbody>tr>td.info,.table>tbody>tr>th.info,.table>tbody>tr.info>td,.table>tbody>tr.info>th,.table>tfoot>tr>td.info,.table>tfoot>tr>th.info,.table>tfoot>tr.info>td,.table>tfoot>tr.info>th{background-color:#d9edf7}.table-hover>tbody>tr>td.info:hover,.table-hover>tbody>tr>th.info:hover,.table-hover>tbody>tr.info:hover>td,.table-hover>tbody>tr:hover>.info,.table-hover>tbody>tr.info:hover>th{background-color:#c4e3f3}.table>thead>tr>td.warning,.table>thead>tr>th.warning,.table>thead>tr.warning>td,.table>thead>tr.warning>th,.table>tbody>tr>td.warning,.table>tbody>tr>th.warning,.table>tbody>tr.warning>td,.table>tbody>tr.warning>th,.table>tfoot>tr>td.warning,.table>tfoot>tr>th.warning,.table>tfoot>tr.warning>td,.table>tfoot>tr.warning>th{background-color:#fcf8e3}.table-hover>tbody>tr>td.warning:hover,.table-hover>tbody>tr>th.warning:hover,.table-hover>tbody>tr.warning:hover>td,.table-hover>tbody>tr:hover>.warning,.table-hover>tbody>tr.warning:hover>th{background-color:#faf2cc}.table>thead>tr>td.danger,.table>thead>tr>th.danger,.table>thead>tr.danger>td,.table>thead>tr.danger>th,.table>tbody>tr>td.danger,.table>tbody>tr>th.danger,.table>tbody>tr.danger>td,.table>tbody>tr.danger>th,.table>tfoot>tr>td.danger,.table>tfoot>tr>th.danger,.table>tfoot>tr.danger>td,.table>tfoot>tr.danger>th{background-color:#f2dede}.table-hover>tbody>tr>td.danger:hover,.table-hover>tbody>tr>th.danger:hover,.table-hover>tbody>tr.danger:hover>td,.table-hover>tbody>tr:hover>.danger,.table-hover>tbody>tr.danger:hover>th{background-color:#ebcccc}fieldset{border:0;min-width:0}legend{display:block;width:100%;margin-bottom:20px;font-size:21px;line-height:inherit;color:#333;border-bottom:1px solid #e5e5e5}label{display:inline-block;max-width:100%;margin-bottom:5px;font-weight:700}input[type="radio"],input[type="checkbox"]{margin:4px 0 0;margin-top:1px \9;line-height:normal}input[type="file"]{display:block}input[type="range"]{display:block;width:100%}select[multiple],select[size]{height:auto}input[type="file"]:focus,input[type="radio"]:focus,input[type="checkbox"]:focus{outline:thin dotted;outline:5px auto -webkit-focus-ring-color;outline-offset:-2px}output{padding-top:7px}output,.form-control{display:block;font-size:14px;line-height:1.4285714;color:#555}.form-control{width:100%;height:34px;padding:6px 12px;background-color:#fff;background-image:none;border:1px solid #ccc;border-radius:4px;box-shadow:inset 0 1px 1px rgba(0,0,0,.075);transition:border-color ease-in-out .15s,box-shadow ease-in-out .15s}.form-control:focus{border-color:#66afe9;outline:0;box-shadow:inset 0 1px 1px rgba(0,0,0,.075),0 0 8px rgba(102,175,233,.6)}.form-control::-moz-placeholder{color:#777;opacity:1}.form-control:-ms-input-placeholder{color:#777}.form-control::-webkit-input-placeholder{color:#777}.form-control[disabled],.form-control[readonly],fieldset[disabled] .form-control{cursor:not-allowed;background-color:#eee;opacity:1}textarea.form-control{height:auto}input[type="date"],input[type="time"],input[type="datetime-local"],input[type="month"]{line-height:34px;line-height:1.4285714 \0}input[type="date"].input-sm,.form-horizontal .form-group-sm input.form-control[type="date"],.input-group-sm>input.form-control[type="date"],.input-group-sm>input.input-group-addon[type="date"],.input-group-sm>.input-group-btn>input.btn[type="date"],input[type="time"].input-sm,.form-horizontal .form-group-sm input.form-control[type="time"],.input-group-sm>input.form-control[type="time"],.input-group-sm>input.input-group-addon[type="time"],.input-group-sm>.input-group-btn>input.btn[type="time"],input[type="datetime-local"].input-sm,.form-horizontal .form-group-sm input.form-control[type="datetime-local"],.input-group-sm>input.form-control[type="datetime-local"],.input-group-sm>input.input-group-addon[type="datetime-local"],.input-group-sm>.input-group-btn>input.btn[type="datetime-local"],input[type="month"].input-sm,.form-horizontal .form-group-sm input.form-control[type="month"],.input-group-sm>input.form-control[type="month"],.input-group-sm>input.input-group-addon[type="month"],.input-group-sm>.input-group-btn>input.btn[type="month"]{line-height:30px}input[type="date"].input-lg,.form-horizontal .form-group-lg input.form-control[type="date"],.input-group-lg>input.form-control[type="date"],.input-group-lg>input.input-group-addon[type="date"],.input-group-lg>.input-group-btn>input.btn[type="date"],input[type="time"].input-lg,.form-horizontal .form-group-lg input.form-control[type="time"],.input-group-lg>input.form-control[type="time"],.input-group-lg>input.input-group-addon[type="time"],.input-group-lg>.input-group-btn>input.btn[type="time"],input[type="datetime-local"].input-lg,.form-horizontal .form-group-lg input.form-control[type="datetime-local"],.input-group-lg>input.form-control[type="datetime-local"],.input-group-lg>input.input-group-addon[type="datetime-local"],.input-group-lg>.input-group-btn>input.btn[type="datetime-local"],input[type="month"].input-lg,.form-horizontal .form-group-lg input.form-control[type="month"],.input-group-lg>input.form-control[type="month"],.input-group-lg>input.input-group-addon[type="month"],.input-group-lg>.input-group-btn>input.btn[type="month"]{line-height:46px}.form-group{margin-bottom:15px}.radio,.checkbox{position:relative;display:block;min-height:20px;margin-top:10px;margin-bottom:10px}.radio label,.checkbox label{padding-left:20px;margin-bottom:0;font-weight:400;cursor:pointer}.radio input[type="radio"],.radio-inline input[type="radio"],.checkbox input[type="checkbox"],.checkbox-inline input[type="checkbox"]{position:absolute;margin-left:-20px;margin-top:4px \9}.radio+.radio,.checkbox+.checkbox{margin-top:-5px}.radio-inline,.checkbox-inline{display:inline-block;padding-left:20px;margin-bottom:0;vertical-align:middle;font-weight:400;cursor:pointer}.radio-inline+.radio-inline,.checkbox-inline+.checkbox-inline{margin-top:0;margin-left:10px}input[type="radio"][disabled],input[type="radio"].disabled,fieldset[disabled] input[type="radio"],input[type="checkbox"][disabled],input[type="checkbox"].disabled,fieldset[disabled] input[type="checkbox"],.radio-inline.disabled,fieldset[disabled] .radio-inline,.checkbox-inline.disabled,fieldset[disabled] .checkbox-inline,.radio.disabled label,fieldset[disabled] .radio label,.checkbox.disabled label,fieldset[disabled] .checkbox label{cursor:not-allowed}.form-control-static{padding-top:7px;padding-bottom:7px;margin-bottom:0}.form-control-static.input-lg,.form-horizontal .form-group-lg .form-control-static.form-control,.input-group-lg>.form-control-static.form-control,.input-group-lg>.form-control-static.input-group-addon,.input-group-lg>.input-group-btn>.form-control-static.btn,.form-control-static.input-sm,.form-horizontal .form-group-sm .form-control-static.form-control,.input-group-sm>.form-control-static.form-control,.input-group-sm>.form-control-static.input-group-addon,.input-group-sm>.input-group-btn>.form-control-static.btn{padding-left:0;padding-right:0}.input-sm,.form-horizontal .form-group-sm .form-control,.input-group-sm>.form-control{height:30px;padding:5px 10px;font-size:12px;line-height:1.5;border-radius:3px}.input-group-sm>.input-group-addon{height:30px;line-height:1.5}.input-group-sm>.input-group-btn>.btn{height:30px;padding:5px 10px;font-size:12px;line-height:1.5;border-radius:3px}select.input-sm,.form-horizontal .form-group-sm select.form-control,.input-group-sm>select.form-control,.input-group-sm>select.input-group-addon,.input-group-sm>.input-group-btn>select.btn{height:30px;line-height:30px}textarea.input-sm,.form-horizontal .form-group-sm textarea.form-control,.input-group-sm>textarea.form-control,.input-group-sm>textarea.input-group-addon,.input-group-sm>.input-group-btn>textarea.btn,select[multiple].input-sm,.form-horizontal .form-group-sm select.form-control[multiple],.input-group-sm>select.form-control[multiple],.input-group-sm>select.input-group-addon[multiple],.input-group-sm>.input-group-btn>select.btn[multiple]{height:auto}.input-lg,.form-horizontal .form-group-lg .form-control,.input-group-lg>.form-control{height:46px;padding:10px 16px;font-size:18px;line-height:1.33;border-radius:6px}.input-group-lg>.input-group-addon{height:46px;line-height:1.33}.input-group-lg>.input-group-btn>.btn{height:46px;padding:10px 16px;font-size:18px;line-height:1.33;border-radius:6px}select.input-lg,.form-horizontal .form-group-lg select.form-control,.input-group-lg>select.form-control,.input-group-lg>select.input-group-addon,.input-group-lg>.input-group-btn>select.btn{height:46px;line-height:46px}textarea.input-lg,.form-horizontal .form-group-lg textarea.form-control,.input-group-lg>textarea.form-control,.input-group-lg>textarea.input-group-addon,.input-group-lg>.input-group-btn>textarea.btn,select[multiple].input-lg,.form-horizontal .form-group-lg select.form-control[multiple],.input-group-lg>select.form-control[multiple],.input-group-lg>select.input-group-addon[multiple],.input-group-lg>.input-group-btn>select.btn[multiple]{height:auto}.has-feedback{position:relative}.has-feedback .form-control{padding-right:42.5px}.form-control-feedback{position:absolute;top:25px;right:0;z-index:2;display:block;width:34px;height:34px;line-height:34px;text-align:center}.input-lg+.form-control-feedback,.form-horizontal .form-group-lg .form-control+.form-control-feedback,.input-group-lg>.form-control+.form-control-feedback,.input-group-lg>.input-group-addon+.form-control-feedback,.input-group-lg>.input-group-btn>.btn+.form-control-feedback{width:46px;height:46px;line-height:46px}.input-sm+.form-control-feedback,.form-horizontal .form-group-sm .form-control+.form-control-feedback,.input-group-sm>.form-control+.form-control-feedback,.input-group-sm>.input-group-addon+.form-control-feedback,.input-group-sm>.input-group-btn>.btn+.form-control-feedback{width:30px;height:30px;line-height:30px}.has-success .help-block,.has-success .control-label,.has-success .radio,.has-success .checkbox,.has-success .radio-inline,.has-success .checkbox-inline{color:#3c763d}.has-success .form-control{border-color:#3c763d;box-shadow:inset 0 1px 1px rgba(0,0,0,.075)}.has-success .form-control:focus{border-color:#2b542c;box-shadow:inset 0 1px 1px rgba(0,0,0,.075),0 0 6px #67b168}.has-success .input-group-addon{color:#3c763d;border-color:#3c763d;background-color:#dff0d8}.has-success .form-control-feedback{color:#3c763d}.has-warning .help-block,.has-warning .control-label,.has-warning .radio,.has-warning .checkbox,.has-warning .radio-inline,.has-warning .checkbox-inline{color:#8a6d3b}.has-warning .form-control{border-color:#8a6d3b;box-shadow:inset 0 1px 1px rgba(0,0,0,.075)}.has-warning .form-control:focus{border-color:#66512c;box-shadow:inset 0 1px 1px rgba(0,0,0,.075),0 0 6px #c0a16b}.has-warning .input-group-addon{color:#8a6d3b;border-color:#8a6d3b;background-color:#fcf8e3}.has-warning .form-control-feedback{color:#8a6d3b}.has-error .help-block,.has-error .control-label,.has-error .radio,.has-error .checkbox,.has-error .radio-inline,.has-error .checkbox-inline{color:#a94442}.has-error .form-control{border-color:#a94442;box-shadow:inset 0 1px 1px rgba(0,0,0,.075)}.has-error .form-control:focus{border-color:#843534;box-shadow:inset 0 1px 1px rgba(0,0,0,.075),0 0 6px #ce8483}.has-error .input-group-addon{color:#a94442;border-color:#a94442;background-color:#f2dede}.has-error .form-control-feedback{color:#a94442}.has-feedback label.sr-only~.form-control-feedback{top:0}.help-block{display:block;margin-top:5px;margin-bottom:10px;color:#737373}.form-horizontal .radio,.form-horizontal .checkbox,.form-horizontal .radio-inline,.form-horizontal .checkbox-inline{margin-top:0;margin-bottom:0;padding-top:7px}.form-horizontal .radio,.form-horizontal .checkbox{min-height:27px}.form-horizontal .form-group{margin-left:-15px;margin-right:-15px}.form-horizontal .form-group:before{content:" ";display:table}.form-horizontal .form-group:after{content:" ";display:table;clear:both}.form-horizontal .has-feedback .form-control-feedback{top:0;right:15px}.btn{display:inline-block;vertical-align:middle;cursor:pointer;background-image:none;border:1px solid transparent;white-space:nowrap;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.btn:focus,.btn:active:focus,.btn.active:focus{outline:thin dotted;outline:5px auto -webkit-focus-ring-color;outline-offset:-2px}.btn:hover,.btn:focus{color:#333;text-decoration:none}.btn:active,.btn.active{outline:0;background-image:none;box-shadow:inset 0 3px 5px rgba(0,0,0,.125)}.btn.disabled,.btn[disabled],fieldset[disabled] .btn{cursor:not-allowed;pointer-events:none;opacity:.65;filter:alpha(opacity=65);box-shadow:none}.btn-default{color:#333;background-color:#fff;border-color:#ccc}.btn-default:hover,.btn-default:focus,.btn-default:active,.btn-default.active,.open>.btn-default.dropdown-toggle{color:#333;background-color:#e6e6e6;border-color:#adadad}.btn-default:active,.btn-default.active,.open>.btn-default.dropdown-toggle{background-image:none}.btn-default.disabled,.btn-default.disabled:hover,.btn-default.disabled:focus,.btn-default.disabled:active,.btn-default.disabled.active,.btn-default[disabled],.btn-default[disabled]:hover,.btn-default[disabled]:focus,.btn-default[disabled]:active,.btn-default[disabled].active,fieldset[disabled] .btn-default,fieldset[disabled] .btn-default:hover,fieldset[disabled] .btn-default:focus,fieldset[disabled] .btn-default:active,fieldset[disabled] .btn-default.active{background-color:#fff;border-color:#ccc}.btn-default .badge{color:#fff;background-color:#333}.btn-primary{color:#fff;background-color:#428bca;border-color:#357ebd}.btn-primary:hover,.btn-primary:focus,.btn-primary:active,.btn-primary.active,.open>.btn-primary.dropdown-toggle{color:#fff;background-color:#3071a9;border-color:#285e8e}.btn-primary:active,.btn-primary.active,.open>.btn-primary.dropdown-toggle{background-image:none}.btn-primary.disabled,.btn-primary.disabled:hover,.btn-primary.disabled:focus,.btn-primary.disabled:active,.btn-primary.disabled.active,.btn-primary[disabled],.btn-primary[disabled]:hover,.btn-primary[disabled]:focus,.btn-primary[disabled]:active,.btn-primary[disabled].active,fieldset[disabled] .btn-primary,fieldset[disabled] .btn-primary:hover,fieldset[disabled] .btn-primary:focus,fieldset[disabled] .btn-primary:active,fieldset[disabled] .btn-primary.active{background-color:#428bca;border-color:#357ebd}.btn-primary .badge{color:#428bca;background-color:#fff}.btn-success{color:#fff;background-color:#5cb85c;border-color:#4cae4c}.btn-success:hover,.btn-success:focus,.btn-success:active,.btn-success.active,.open>.btn-success.dropdown-toggle{color:#fff;background-color:#449d44;border-color:#398439}.btn-success:active,.btn-success.active,.open>.btn-success.dropdown-toggle{background-image:none}.btn-success.disabled,.btn-success.disabled:hover,.btn-success.disabled:focus,.btn-success.disabled:active,.btn-success.disabled.active,.btn-success[disabled],.btn-success[disabled]:hover,.btn-success[disabled]:focus,.btn-success[disabled]:active,.btn-success[disabled].active,fieldset[disabled] .btn-success,fieldset[disabled] .btn-success:hover,fieldset[disabled] .btn-success:focus,fieldset[disabled] .btn-success:active,fieldset[disabled] .btn-success.active{background-color:#5cb85c;border-color:#4cae4c}.btn-success .badge{color:#5cb85c;background-color:#fff}.btn-info{color:#fff;background-color:#5bc0de;border-color:#46b8da}.btn-info:hover,.btn-info:focus,.btn-info:active,.btn-info.active,.open>.btn-info.dropdown-toggle{color:#fff;background-color:#31b0d5;border-color:#269abc}.btn-info:active,.btn-info.active,.open>.btn-info.dropdown-toggle{background-image:none}.btn-info.disabled,.btn-info.disabled:hover,.btn-info.disabled:focus,.btn-info.disabled:active,.btn-info.disabled.active,.btn-info[disabled],.btn-info[disabled]:hover,.btn-info[disabled]:focus,.btn-info[disabled]:active,.btn-info[disabled].active,fieldset[disabled] .btn-info,fieldset[disabled] .btn-info:hover,fieldset[disabled] .btn-info:focus,fieldset[disabled] .btn-info:active,fieldset[disabled] .btn-info.active{background-color:#5bc0de;border-color:#46b8da}.btn-info .badge{color:#5bc0de;background-color:#fff}.btn-warning{color:#fff;background-color:#f0ad4e;border-color:#eea236}.btn-warning:hover,.btn-warning:focus,.btn-warning:active,.btn-warning.active,.open>.btn-warning.dropdown-toggle{color:#fff;background-color:#ec971f;border-color:#d58512}.btn-warning:active,.btn-warning.active,.open>.btn-warning.dropdown-toggle{background-image:none}.btn-warning.disabled,.btn-warning.disabled:hover,.btn-warning.disabled:focus,.btn-warning.disabled:active,.btn-warning.disabled.active,.btn-warning[disabled],.btn-warning[disabled]:hover,.btn-warning[disabled]:focus,.btn-warning[disabled]:active,.btn-warning[disabled].active,fieldset[disabled] .btn-warning,fieldset[disabled] .btn-warning:hover,fieldset[disabled] .btn-warning:focus,fieldset[disabled] .btn-warning:active,fieldset[disabled] .btn-warning.active{background-color:#f0ad4e;border-color:#eea236}.btn-warning .badge{color:#f0ad4e;background-color:#fff}.btn-danger{color:#fff;background-color:#d9534f;border-color:#d43f3a}.btn-danger:hover,.btn-danger:focus,.btn-danger:active,.btn-danger.active,.open>.btn-danger.dropdown-toggle{color:#fff;background-color:#c9302c;border-color:#ac2925}.btn-danger:active,.btn-danger.active,.open>.btn-danger.dropdown-toggle{background-image:none}.btn-danger.disabled,.btn-danger.disabled:hover,.btn-danger.disabled:focus,.btn-danger.disabled:active,.btn-danger.disabled.active,.btn-danger[disabled],.btn-danger[disabled]:hover,.btn-danger[disabled]:focus,.btn-danger[disabled]:active,.btn-danger[disabled].active,fieldset[disabled] .btn-danger,fieldset[disabled] .btn-danger:hover,fieldset[disabled] .btn-danger:focus,fieldset[disabled] .btn-danger:active,fieldset[disabled] .btn-danger.active{background-color:#d9534f;border-color:#d43f3a}.btn-danger .badge{color:#d9534f;background-color:#fff}.btn-link{color:#428bca;font-weight:400;cursor:pointer;border-radius:0}.btn-link,.btn-link:active,.btn-link[disabled],fieldset[disabled] .btn-link{background-color:transparent;box-shadow:none}.btn-link,.btn-link:hover,.btn-link:focus,.btn-link:active{border-color:transparent}.btn-link:hover,.btn-link:focus{color:#2a6496;text-decoration:underline;background-color:transparent}.btn-link[disabled]:hover,.btn-link[disabled]:focus,fieldset[disabled] .btn-link:hover,fieldset[disabled] .btn-link:focus{color:#777;text-decoration:none}.btn-lg{padding:10px 16px;font-size:18px;line-height:1.33;border-radius:6px}.btn-sm{padding:5px 10px}.btn-sm,.btn-xs{font-size:12px;line-height:1.5;border-radius:3px}.btn-xs{padding:1px 5px}.btn-block{display:block;width:100%}.btn-block+.btn-block{margin-top:5px}input[type="submit"].btn-block,input[type="reset"].btn-block,input[type="button"].btn-block{width:100%}.fade{opacity:0;transition:opacity .15s linear}.fade.in{opacity:1}.collapse{display:none}.collapse.in{display:block}tr.collapse.in{display:table-row}tbody.collapse.in{display:table-row-group}.collapsing{position:relative;height:0;overflow:hidden;transition:height .35s ease}.input-group{position:relative;display:table;border-collapse:separate}.input-group[class*="col-"]{float:none;padding-left:0;padding-right:0}.input-group .form-control{position:relative;z-index:2;float:left;width:100%;margin-bottom:0}.input-group-addon,.input-group-btn,.input-group .form-control{display:table-cell}.input-group-addon:not(:first-child):not(:last-child),.input-group-btn:not(:first-child):not(:last-child),.input-group .form-control:not(:first-child):not(:last-child){border-radius:0}.input-group-addon{white-space:nowrap}.input-group-addon,.input-group-btn{width:1%;vertical-align:middle}.input-group-addon{padding:6px 12px;font-size:14px;font-weight:400;line-height:1;color:#555;text-align:center;background-color:#eee;border:1px solid #ccc;border-radius:4px}.input-group-addon.input-sm,.form-horizontal .form-group-sm .input-group-addon.form-control,.input-group-sm>.input-group-addon,.input-group-sm>.input-group-btn>.input-group-addon.btn{padding:5px 10px;font-size:12px;border-radius:3px}.input-group-addon.input-lg,.form-horizontal .form-group-lg .input-group-addon.form-control,.input-group-lg>.input-group-addon,.input-group-lg>.input-group-btn>.input-group-addon.btn{padding:10px 16px;font-size:18px;border-radius:6px}.input-group-addon input[type="radio"],.input-group-addon input[type="checkbox"]{margin-top:0}.input-group .form-control:first-child,.input-group-addon:first-child,.input-group-btn:first-child>.btn,.input-group-btn:first-child>.btn-group>.btn,.input-group-btn:first-child>.dropdown-toggle,.input-group-btn:last-child>.btn:not(:last-child):not(.dropdown-toggle),.input-group-btn:last-child>.btn-group:not(:last-child)>.btn{border-bottom-right-radius:0;border-top-right-radius:0}.input-group-addon:first-child{border-right:0}.input-group .form-control:last-child,.input-group-addon:last-child,.input-group-btn:last-child>.btn,.input-group-btn:last-child>.btn-group>.btn,.input-group-btn:last-child>.dropdown-toggle,.input-group-btn:first-child>.btn:not(:first-child),.input-group-btn:first-child>.btn-group:not(:first-child)>.btn{border-bottom-left-radius:0;border-top-left-radius:0}.input-group-addon:last-child{border-left:0}.input-group-btn{font-size:0;white-space:nowrap}.input-group-btn,.input-group-btn>.btn{position:relative}.input-group-btn>.btn+.btn{margin-left:-1px}.input-group-btn>.btn:hover,.input-group-btn>.btn:focus,.input-group-btn>.btn:active{z-index:2}.input-group-btn:first-child>.btn,.input-group-btn:first-child>.btn-group{margin-right:-1px}.input-group-btn:last-child>.btn,.input-group-btn:last-child>.btn-group{margin-left:-1px}.pagination{display:inline-block;padding-left:0;margin:20px 0;border-radius:4px}.pagination>li{display:inline}.pagination>li>a,.pagination>li>span{position:relative;float:left;padding:6px 12px;line-height:1.4285714;text-decoration:none;color:#428bca;background-color:#fff;border:1px solid #ddd;margin-left:-1px}.pagination>li:first-child>a,.pagination>li:first-child>span{margin-left:0;border-bottom-left-radius:4px;border-top-left-radius:4px}.pagination>li:last-child>a,.pagination>li:last-child>span{border-bottom-right-radius:4px;border-top-right-radius:4px}.pagination>li>a:hover,.pagination>li>a:focus,.pagination>li>span:hover,.pagination>li>span:focus{color:#2a6496;background-color:#eee;border-color:#ddd}.pagination>.active>a,.pagination>.active>a:hover,.pagination>.active>a:focus,.pagination>.active>span,.pagination>.active>span:hover,.pagination>.active>span:focus{z-index:2;color:#fff;background-color:#428bca;border-color:#428bca;cursor:default}.pagination>.disabled>span,.pagination>.disabled>span:hover,.pagination>.disabled>span:focus,.pagination>.disabled>a,.pagination>.disabled>a:hover,.pagination>.disabled>a:focus{color:#777;background-color:#fff;border-color:#ddd;cursor:not-allowed}.pagination-lg>li>a,.pagination-lg>li>span{padding:10px 16px;font-size:18px}.pagination-lg>li:first-child>a,.pagination-lg>li:first-child>span{border-bottom-left-radius:6px;border-top-left-radius:6px}.pagination-lg>li:last-child>a,.pagination-lg>li:last-child>span{border-bottom-right-radius:6px;border-top-right-radius:6px}.pagination-sm>li>a,.pagination-sm>li>span{padding:5px 10px;font-size:12px}.pagination-sm>li:first-child>a,.pagination-sm>li:first-child>span{border-bottom-left-radius:3px;border-top-left-radius:3px}.pagination-sm>li:last-child>a,.pagination-sm>li:last-child>span{border-bottom-right-radius:3px;border-top-right-radius:3px}.close{float:right;font-size:21px;font-weight:700;line-height:1;color:#000;text-shadow:0 1px 0 #fff;opacity:.2;filter:alpha(opacity=20)}.close:hover,.close:focus{color:#000;text-decoration:none;cursor:pointer;opacity:.5;filter:alpha(opacity=50)}button.close{padding:0;cursor:pointer;background:0 0;border:0;-webkit-appearance:none}.modal-open,.modal{overflow:hidden}.modal{display:none;position:fixed;top:0;right:0;bottom:0;left:0;z-index:1050;-webkit-overflow-scrolling:touch;outline:0}.modal.fade .modal-dialog{transform:translate3d(0,-25%,0);transition:transform .3s ease-out}.modal.in .modal-dialog{transform:translate3d(0,0,0)}.modal-open .modal{overflow-x:hidden;overflow-y:auto}.modal-dialog{position:relative;width:auto;margin:10px}.modal-content{position:relative;background-color:#fff;border:1px solid #999;border:1px solid rgba(0,0,0,.2);border-radius:6px;box-shadow:0 3px 9px rgba(0,0,0,.5);background-clip:padding-box;outline:0}.modal-backdrop{position:fixed;top:0;right:0;bottom:0;left:0;z-index:1040;background-color:#000}.modal-backdrop.fade{opacity:0;filter:alpha(opacity=0)}.modal-backdrop.in{opacity:.5;filter:alpha(opacity=50)}.modal-header{padding:15px;border-bottom:1px solid #e5e5e5;min-height:16.4285714px}.modal-header .close{margin-top:-2px}.modal-title{margin:0;line-height:1.4285714}.modal-body{position:relative;padding:15px}.modal-footer{padding:15px;text-align:right;border-top:1px solid #e5e5e5}.modal-footer:before,.modal-footer:after{content:" ";display:table}.modal-footer:after{clear:both}.modal-footer .btn+.btn{margin-left:5px;margin-bottom:0}.modal-footer .btn-group .btn+.btn{margin-left:-1px}.modal-footer .btn-block+.btn-block{margin-left:0}.modal-scrollbar-measure{position:absolute;top:-9999px;width:50px;height:50px;overflow:scroll}.clearfix:before,.clearfix:after{content:" ";display:table}.clearfix:after{clear:both}.center-block{display:block;margin-left:auto;margin-right:auto}.pull-right{float:right!important}.pull-left{float:left!important}.hide{display:none!important}.show{display:block!important}.invisible{visibility:hidden}.text-hide{font:0/0 a;color:transparent;text-shadow:none;background-color:transparent;border:0}.hidden{display:none!important;visibility:hidden!important}.affix{position:fixed;transform:translate3d(0,0,0)}.hljs{display:block;overflow-x:auto;padding:.5em;background:#002b36;color:#839496;-webkit-text-size-adjust:none}.hljs-comment,.hljs-template_comment,.diff .hljs-header,.hljs-doctype,.hljs-pi,.lisp .hljs-string,.hljs-javadoc{color:#586e75}.hljs-keyword,.hljs-winutils,.method,.hljs-addition,.css .hljs-tag,.hljs-request,.hljs-status,.nginx .hljs-title{color:#859900}.hljs-number,.hljs-command,.hljs-string,.hljs-tag .hljs-value,.hljs-rules .hljs-value,.hljs-phpdoc,.hljs-dartdoc,.tex .hljs-formula,.hljs-regexp,.hljs-hexcolor,.hljs-link_url{color:#2aa198}.hljs-title,.hljs-localvars,.hljs-chunk,.hljs-decorator,.hljs-built_in,.hljs-identifier,.vhdl .hljs-literal,.hljs-id,.css .hljs-function{color:#268bd2}.hljs-attribute,.hljs-variable,.lisp .hljs-body,.smalltalk .hljs-number,.hljs-constant,.hljs-class .hljs-title,.hljs-parent,.hljs-type,.hljs-link_reference{color:#b58900}.hljs-preprocessor,.hljs-preprocessor .hljs-keyword,.hljs-pragma,.hljs-shebang,.hljs-symbol,.hljs-symbol .hljs-string,.diff .hljs-change,.hljs-special,.hljs-attr_selector,.hljs-subst,.hljs-cdata,.css .hljs-pseudo,.hljs-header{color:#cb4b16}.hljs-deletion,.hljs-important{color:#dc322f}.hljs-link_label{color:#6c71c4}.tex .hljs-formula{background:#073642}*,*:before,*:after{box-sizing:border-box}html{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background:0 0}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:700}dfn{font-style:italic}h1{margin:.67em 0}mark{background:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sup{top:-.5em}sub{bottom:-.25em}images{border:0}svg:not(:root){overflow:hidden}figure{margin:1em 40px}hr{box-sizing:content-box;height:0}pre{overflow:auto}code,kbd{font-size:1em}code,kbd,pre,samp{font-family:monospace,monospace}samp{font-size:1em}button,input,optgroup,select,textarea{color:inherit;font:inherit;margin:0}button{overflow:visible}button,select{text-transform:none}button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}input{line-height:normal}input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0;margin-right:5px}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{height:auto}input[type="search"]{-webkit-appearance:textfield;box-sizing:content-box}input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}legend{border:0;padding:0}textarea{overflow:auto}optgroup{font-weight:700}table{border-collapse:collapse;border-spacing:0}.debug{background-color:#ffc0cb!important}.ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.ir{background-color:transparent;border:0;overflow:hidden}.ir::before{content:'';display:block;height:150%;width:0}html{font-size:.875em;background:#fff;color:#373D49}html,body{font-family:Georgia,Cambria,serif;height:100%}body{font-size:1rem;font-weight:400;line-height:2rem}ul,ol{margin-bottom:.83999rem;padding-top:.16001rem}li{font-feature-settings:'kern' 1,'onum' 1,'liga' 1;margin-left:1rem}li>ul,li>ol{margin-bottom:0}p{padding-top:.66001rem;font-feature-settings:'kern' 1,'onum' 1,'liga' 1;margin-top:0}p,pre{margin-bottom:1.33999rem}pre{font-size:1rem;padding:.66001rem 9.5px 9.5px;line-height:2rem;background:linear-gradient(to bottom,#fff 0,#fff .75rem,#f5f7fa .75rem,#f5f7fa 2.75rem,#fff 2.75rem,#fff 4rem);background-size:100% 4rem;border-color:#D3DAEA}blockquote{margin:0}blockquote p{font-size:1rem;margin-bottom:.33999rem;font-style:italic;padding:.66001rem 1rem 1rem;border-left:3px solid #A0AABF}th,td{padding:12px}h1,h2,h3,h4,h5,h6{font-family:"Source Sans Pro","Helvetica Neue",Helvetica,Arial,sans-serif;font-feature-settings:'dlig' 1,'liga' 1,'lnum' 1,'kern' 1;font-style:normal;font-weight:600;margin-top:0}h1{line-height:3rem;font-size:2.0571429rem;margin-bottom:.21999rem;padding-top:.78001rem}h2{font-size:1.953125rem;margin-bottom:.1835837rem;padding-top:.8164163rem}h2,h3{line-height:3rem}h3{font-size:1.6457143rem;margin-bottom:.07599rem;padding-top:.92401rem}h4{font-size:1.5625rem;margin-bottom:.546865rem;padding-top:.453135rem}h5{font-size:1.25rem;margin-bottom:-.56251rem;padding-top:.56251rem}h6{font-size:1rem;margin-bottom:-.65001rem;padding-top:.65001rem}a{cursor:pointer;color:#35D7BB;text-decoration:none}a:hover,a:focus{border-bottom-color:#35D7BB;color:#dff9f4}img{height:auto;max-width:100%}dt{font-style:italic;font-weight:600}.g{display:block}.g:after{clear:both;content:'';display:table}.g-b{float:left;margin:0;width:100%}.g{margin-left:-16px;margin-right:-16px}.g-b{padding-left:16px;padding-right:16px}.g-b--center{display:block;float:none;margin:0 auto}.g-b--right{float:right}.g-b--1of1{width:100%}.g-b--1of2,.g-b--2of4,.g-b--3of6,.g-b--4of8,.g-b--5of10,.g-b--6of12{width:50%}.g-b--1of3,.g-b--2of6,.g-b--4of12{width:33.333%}.g-b--2of3,.g-b--4of6,.g-b--8of12{width:66.666%}.g-b--1of4,.g-b--2of8,.g-b--3of12{width:25%}.g-b--3of4,.g-b--6of8,.g-b--9of12{width:75%}.g-b--1of5,.g-b--2of10{width:20%}.g-b--2of5,.g-b--4of10{width:40%}.g-b--3of5,.g-b--6of10{width:60%}.g-b--4of5,.g-b--8of10{width:80%}.g-b--1of6,.g-b--2of12{width:16.666%}.g-b--5of6,.g-b--10of12{width:83.333%}.g-b--1of8{width:12.5%}.g-b--3of8{width:37.5%}.g-b--5of8{width:62.5%}.g-b--7of8{width:87.5%}.g-b--1of10{width:10%}.g-b--3of10{width:30%}.g-b--7of10{width:70%}.g-b--9of10{width:90%}.g-b--1of12{width:8.333%}.g-b--5of12{width:41.666%}.g-b--7of12{width:58.333%}.g-b--11of12{width:91.666%}.g-b--push--1of1{margin-left:100%}.g-b--push--1of2,.g-b--push--2of4,.g-b--push--3of6,.g-b--push--4of8,.g-b--push--5of10,.g-b--push--6of12{margin-left:50%}.g-b--push--1of3,.g-b--push--2of6,.g-b--push--4of12{margin-left:33.333%}.g-b--push--2of3,.g-b--push--4of6,.g-b--push--8of12{margin-left:66.666%}.g-b--push--1of4,.g-b--push--2of8,.g-b--push--3of12{margin-left:25%}.g-b--push--3of4,.g-b--push--6of8,.g-b--push--9of12{margin-left:75%}.g-b--push--1of5,.g-b--push--2of10{margin-left:20%}.g-b--push--2of5,.g-b--push--4of10{margin-left:40%}.g-b--push--3of5,.g-b--push--6of10{margin-left:60%}.g-b--push--4of5,.g-b--push--8of10{margin-left:80%}.g-b--push--1of6,.g-b--push--2of12{margin-left:16.666%}.g-b--push--5of6,.g-b--push--10of12{margin-left:83.333%}.g-b--push--1of8{margin-left:12.5%}.g-b--push--3of8{margin-left:37.5%}.g-b--push--5of8{margin-left:62.5%}.g-b--push--7of8{margin-left:87.5%}.g-b--push--1of10{margin-left:10%}.g-b--push--3of10{margin-left:30%}.g-b--push--7of10{margin-left:70%}.g-b--push--9of10{margin-left:90%}.g-b--push--1of12{margin-left:8.333%}.g-b--push--5of12{margin-left:41.666%}.g-b--push--7of12{margin-left:58.333%}.g-b--push--11of12{margin-left:91.666%}.g-b--pull--1of1{margin-right:100%}.g-b--pull--1of2,.g-b--pull--2of4,.g-b--pull--3of6,.g-b--pull--4of8,.g-b--pull--5of10,.g-b--pull--6of12{margin-right:50%}.g-b--pull--1of3,.g-b--pull--2of6,.g-b--pull--4of12{margin-right:33.333%}.g-b--pull--2of3,.g-b--pull--4of6,.g-b--pull--8of12{margin-right:66.666%}.g-b--pull--1of4,.g-b--pull--2of8,.g-b--pull--3of12{margin-right:25%}.g-b--pull--3of4,.g-b--pull--6of8,.g-b--pull--9of12{margin-right:75%}.g-b--pull--1of5,.g-b--pull--2of10{margin-right:20%}.g-b--pull--2of5,.g-b--pull--4of10{margin-right:40%}.g-b--pull--3of5,.g-b--pull--6of10{margin-right:60%}.g-b--pull--4of5,.g-b--pull--8of10{margin-right:80%}.g-b--pull--1of6,.g-b--pull--2of12{margin-right:16.666%}.g-b--pull--5of6,.g-b--pull--10of12{margin-right:83.333%}.g-b--pull--1of8{margin-right:12.5%}.g-b--pull--3of8{margin-right:37.5%}.g-b--pull--5of8{margin-right:62.5%}.g-b--pull--7of8{margin-right:87.5%}.g-b--pull--1of10{margin-right:10%}.g-b--pull--3of10{margin-right:30%}.g-b--pull--7of10{margin-right:70%}.g-b--pull--9of10{margin-right:90%}.g-b--pull--1of12{margin-right:8.333%}.g-b--pull--5of12{margin-right:41.666%}.g-b--pull--7of12{margin-right:58.333%}.g-b--pull--11of12{margin-right:91.666%}.splashscreen{position:fixed;top:0;left:0;width:100%;height:100%;background-color:#373D49;z-index:22}.splashscreen-dillinger{width:260px;height:auto;display:block;margin:0 auto;padding-bottom:3rem}.splashscreen p{font-size:1.25rem;padding-top:.56251rem;font-family:"Source Sans Pro","Helvetica Neue",Helvetica,Arial,sans-serif;font-weight:400;text-align:center;max-width:500px;margin:0 auto;color:#FFF}.sp-center{position:relative;transform:translateY(-50%);top:50%}.open-menu>.wrapper{overflow-x:hidden}.page{margin:0 auto;position:relative;top:0;left:0;width:100%;height:100%;z-index:2;transition:all .25s ease-in-out;background-color:#fff;padding-top:51px;will-change:left}.open-menu .page{left:270px}.title{line-height:1rem;font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem;font-weight:500;color:#A0AABF;letter-spacing:1px;text-transform:uppercase;padding-left:16px;padding-right:16px;margin-top:1rem}.split-preview .title{padding-left:0}.title-document{line-height:1rem;font-size:1.25rem;margin-bottom:.89999rem;padding-top:.10001rem;font-weight:400;font-family:"Ubuntu Mono",Monaco;color:#373D49;padding-left:16px;padding-right:16px;width:80%;min-width:300px;outline:0;border:none}.icon{display:block;margin:0 auto;width:36px;height:36px;border-radius:3px;text-align:center}.icon svg{display:inline-block;margin-left:auto;margin-right:auto}.icon-preview{background-color:#373D49;line-height:40px}.icon-preview svg{width:19px;height:12px}.icon-settings{background-color:#373D49;line-height:44px}.icon-settings svg{width:18px;height:18px}.icon-link{width:16px;height:16px;line-height:1;margin-right:24px;text-align:right}.navbar{background-color:#373D49;height:51px;width:100%;position:fixed;top:0;left:0;z-index:6;transition:all .25s ease-in-out;will-change:left}.navbar:after{content:"";display:table;clear:both}.open-menu .navbar{left:270px}.navbar-brand{float:left;margin:0 0 0 24px;padding:0;line-height:42px}.navbar-brand svg{width:85px;height:11px}.nav-left{float:left}.nav-right{float:right}.nav-sidebar{width:100%}.menu{list-style:none;margin:0;padding:0}.menu a{border:0;color:#A0AABF;font-family:"Source Sans Pro","Helvetica Neue",Helvetica,Arial,sans-serif;outline:none;text-transform:uppercase}.menu a:hover{color:#35D7BB}.menu .menu-item{border:0;display:none;float:left;margin:0;position:relative}.menu .menu-item>a{display:block;font-size:12px;height:51px;letter-spacing:1px;line-height:51px;padding:0 24px}.menu .menu-item--settings,.menu .menu-item--preview,.menu .menu-item--save-to.in-sidebar,.menu .menu-item--import-from.in-sidebar,.menu .menu-item--link-unlink.in-sidebar,.menu .menu-item--documents.in-sidebar{display:block}.menu .menu-item--documents{padding-bottom:1rem}.menu .menu-item.open>a{background-color:#1D212A}.menu .menu-item-icon>a{height:auto;padding:0}.menu .menu-item-icon:hover>a{background-color:transparent}.menu .menu-link.open i{background-color:#1D212A}.menu .menu-link.open g{fill:#35D7BB}.menu .menu-link-preview,.menu .menu-link-settings{margin-top:8px;width:51px}.menu-sidebar{width:100%}.menu-sidebar .menu-item{float:none;margin-bottom:1px;width:100%}.menu-sidebar .menu-item.open>a{background-color:#373D49}.menu-sidebar .open .caret{transform:rotate(180deg)}.menu-sidebar>.menu-item:hover .dropdown a,.menu-sidebar>.menu-item:hover .settings a{background-color:transparent}.menu-sidebar .menu-link{background-color:#373D49;font-weight:600}.menu-sidebar .menu-link:after{content:"";display:table;clear:both}.menu-sidebar .menu-link>span{float:left}.menu-sidebar .menu-link>.caret{float:right;text-align:right;top:22px}.menu-sidebar .dropdown,.menu-sidebar .settings{background-color:transparent;position:static;width:100%}.dropdown{position:absolute;right:0;top:51px;width:188px}.dropdown,.settings{display:none;background-color:#1D212A}.dropdown{padding:0}.dropdown,.settings,.sidebar-list{list-style:none;margin:0}.sidebar-list{padding:0}.dropdown li{margin:32px 0;padding:0 0 0 32px}.dropdown li,.settings li{line-height:1}.sidebar-list li{line-height:1;margin:32px 0;padding:0 0 0 32px}.dropdown a{color:#D0D6E2}.dropdown a,.settings a,.sidebar-list a{display:block;text-transform:none}.sidebar-list a{color:#D0D6E2}.dropdown a:after,.settings a:after,.sidebar-list a:after{content:"";display:table;clear:both}.dropdown .icon,.settings .icon,.sidebar-list .icon{float:right}.open .dropdown,.open .settings,.open .sidebar-list{display:block}.open .dropdown.collapse,.open .collapse.settings,.open .sidebar-list.collapse{display:none}.open .dropdown.collapse.in,.open .collapse.in.settings,.open .sidebar-list.collapse.in{display:block}.dropdown .unlinked .icon,.settings .unlinked .icon,.sidebar-list .unlinked .icon{opacity:.3}.dropdown.documents li,.documents.settings li,.sidebar-list.documents li{background-image:url("../img/icons/file.svg");background-position:240px center;background-repeat:no-repeat;background-size:14px 16px;padding:3px 32px}.dropdown.documents li.octocat,.documents.settings li.octocat,.sidebar-list.documents li.octocat{background-image:url("../img/icons/octocat.svg");background-position:234px center;background-size:24px 24px}.dropdown.documents li:last-child,.documents.settings li:last-child,.sidebar-list.documents li:last-child{margin-bottom:1rem}.dropdown.documents li.active a,.documents.settings li.active a,.sidebar-list.documents li.active a{color:#35D7BB}.settings{position:fixed;top:67px;right:16px;border-radius:3px;width:288px;background-color:#373D49;padding:16px;z-index:7}.show-settings .settings{display:block}.settings .has-checkbox{float:left}.settings form{display:-ms-flexbox;display:flex;-ms-flex-direction:row;flex-direction:row;-ms-flex-pack:justify;justify-content:space-between}.settings input{width:20%}.settings a{font-size:1.25rem;font-family:"Source Sans Pro","Helvetica Neue",Helvetica,Arial,sans-serif;font-weight:400;-webkit-font-smoothing:antialiased;line-height:28px;color:#D0D6E2}.settings a:after{content:"";display:table;clear:both}.settings a:hover{color:#35D7BB}.settings li{border-bottom:1px solid #4F535B;margin:0;padding:16px 0}.settings li:last-child{border-bottom:none}.brand{border:none;display:block}.brand:hover g{fill:#35D7BB}.toggle{display:block;float:left;height:16px;padding:25px 16px 26px;width:40px}.toggle span:after,.toggle span:before{content:'';left:0;position:absolute;top:-6px}.toggle span:after{top:6px}.toggle span{display:block;position:relative}.toggle span,.toggle span:after,.toggle span:before{-webkit-backface-visibility:hidden;backface-visibility:hidden;background-color:#D3DAEA;height:2px;transition:all .3s;width:20px}.open-menu .toggle span{background-color:transparent}.open-menu .toggle span:before{transform:rotate(45deg) translate(3px,3px)}.open-menu .toggle span:after{transform:rotate(-45deg) translate(5px,-6px)}.caret{display:inline-block;width:0;height:0;margin-left:6px;vertical-align:middle;position:relative;top:-1px;border-top:4px solid;border-right:4px solid transparent;border-left:4px solid transparent}.sidebar{overflow:auto;height:100%;padding-right:15px;padding-bottom:15px;width:285px}.sidebar-wrapper{-webkit-overflow-scrolling:touch;background-color:#2B2F36;left:0;height:100%;overflow-y:hidden;position:fixed;top:0;width:285px;z-index:1}.sidebar-branding{width:160px;padding:0;margin:16px auto}.header{border-bottom:1px solid #E8E8E8;position:relative}.words,.characters{line-height:1rem;font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem;font-weight:500;font-family:"Source Sans Pro","Helvetica Neue",Helvetica,Arial,sans-serif;letter-spacing:1px;text-transform:uppercase;z-index:5;position:absolute;right:16px;top:0}.words span,.characters span{color:#A0AABF}.words .counter,.characters .counter{color:#000}.words+.characters{top:22px}.mr10{margin-right:10px}.btn{text-align:center;display:inline-block;width:100%;text-transform:uppercase;font-weight:600;font-family:"Source Sans Pro","Helvetica Neue",Helvetica,Arial,sans-serif;font-size:14px;text-shadow:0 1px 0 #1b8b77;padding:16px 24px;background-color:#35D7BB;border-radius:3px;margin:0 auto 16px;line-height:1;color:#fff;transition:all .15s linear;-webkit-font-smoothing:antialiased}.btn--new,.btn--save{display:block;width:238px}.btn--new:hover,.btn--new:focus,.btn--save:hover,.btn--save:focus{color:#fff;border-bottom-color:transparent;box-shadow:0 1px 3px #24b59c;text-shadow:0 1px 0 #24b59c}.btn--save{background-color:#4A5261;text-shadow:0 1px 1px #1e2127}.btn--save:hover,.btn--save:focus{color:#fff;border-bottom-color:transparent;box-shadow:0 1px 5px #08090a;text-shadow:none}.btn--delete{display:block;width:238px;background-color:transparent;font-size:12px;text-shadow:none}.btn--delete:hover,.btn--delete:focus{color:#fff;border-bottom-color:transparent;text-shadow:0 1px 0 #08090a;opacity:.8}.btn--delete-modal,.btn--ok,.btn--close{border-top:0;background-color:#4A5261;text-shadow:0 1px 0 #08090a;margin:0}.btn--delete-modal:hover,.btn--delete-modal:focus,.btn--ok:hover,.btn--ok:focus,.btn--close:hover,.btn--close:focus{color:#fff;background-color:#292d36;text-shadow:none}.btn--delete-modal{display:inline;width:auto}.overlay{position:absolute;top:0;left:0;width:100%;height:100%;background-color:rgba(55,61,73,.8);transition:all .25s ease-in-out;transition-timing-function:ease-out;will-change:left,opacity,visibility;z-index:5;opacity:0;visibility:hidden}.show-settings .overlay{visibility:visible;opacity:1}.switch{float:right;line-height:1}.switch input{display:none}.switch small{display:inline-block;cursor:pointer;padding:0 24px 0 0;transition:all ease .2s;background-color:#2B2F36;border-color:#2B2F36}.switch small,.switch small:before{border-radius:30px;box-shadow:inset 0 0 2px 0 #14171F}.switch small:before{display:block;content:'';width:28px;height:28px;background:#fff}.switch.checked small{padding-right:0;padding-left:24px;background-color:#35D7BB;box-shadow:none}.modal--dillinger.about .modal-dialog{font-size:1.25rem;max-width:500px}.modal--dillinger.scope .modal-dialog{max-width:300px;margin:5rem auto}.modal--dillinger .modal-dialog{max-width:600px;width:auto;margin:5rem auto}.modal--dillinger .modal-content{background:#373D49;border-radius:3px;box-shadow:0 2px 5px 0 #2C3B59;color:#fff;font-family:"Source Sans Pro","Helvetica Neue",Helvetica,Arial,sans-serif;font-weight:400;padding:2rem}.modal--dillinger ul{list-style-type:disc;margin:1rem 0;padding:0 0 0 1rem}.modal--dillinger li{padding:0;margin:0}.modal--dillinger .modal-header{border:0;padding:0}.modal--dillinger .modal-body{padding:0}.modal--dillinger .modal-footer{border:0;padding:0}.modal--dillinger .close{color:#fff;opacity:1}.modal-backdrop{background-color:#373D49}.pagination--dillinger{padding:0!important;margin:1.5rem 0!important;display:-ms-flexbox;display:flex;-ms-flex-pack:justify;justify-content:space-between;-ms-flex-direction:row;flex-direction:row;-ms-flex-align:center;align-items:center;-ms-flex-line-pack:stretch;align-content:stretch}.pagination--dillinger li{display:-ms-flexbox;display:flex;-ms-flex-positive:1;flex-grow:1;text-align:center}.pagination--dillinger li:first-child>a,.pagination--dillinger li.disabled>a,.pagination--dillinger li.disabled>a:hover,.pagination--dillinger li.disabled>a:focus,.pagination--dillinger li>a{background-color:transparent;border-color:#4F535B;border-right-color:transparent}.pagination--dillinger li.active>a,.pagination--dillinger li.active>a:hover,.pagination--dillinger li.active>a:focus{border-color:#4A5261;background-color:#4A5261;color:#fff}.pagination--dillinger li>a{float:none;color:#fff;width:100%;display:block;text-align:center;margin:0;border-right-color:transparent;padding:6px}.pagination--dillinger li>a:hover,.pagination--dillinger li>a:focus{border-color:#35D7BB;background-color:#35D7BB;color:#fff}.pagination--dillinger li:last-child a{border-color:#4F535B}.pagination--dillinger li:first-child a{border-right-color:transparent}.diNotify{position:absolute;z-index:9999;left:0;right:0;top:0;margin:0 auto;max-width:400px;text-align:center;transition:top .5s ease-in-out,opacity .5s ease-in-out;visibility:hidden}.diNotify-body{-webkit-font-smoothing:antialiased;background-color:#35D7BB;background:#666E7F;border-radius:3px;color:#fff;font-family:"Source Sans Pro","Helvetica Neue",Helvetica,Arial,sans-serif;font-weight:400;overflow:hidden;padding:1rem 2rem .5rem;display:-ms-flexbox;display:flex;-ms-flex-align:baseline;align-items:baseline;-ms-flex-pack:center;justify-content:center}.diNotify-icon{display:block;width:16px;height:16px;line-height:16px;position:relative;top:3px}.diNotify-message{padding-left:1rem}.zen-wrapper{position:fixed;top:0;left:0;right:0;bottom:0;width:100%;height:100%;z-index:10;background-color:#FFF;opacity:0;transition:opacity .25s ease-in-out}.zen-wrapper.on{opacity:1}.enter-zen-mode{background-image:url("../img/icons/enter-zen.svg");right:.5rem;top:.313rem;display:none}.enter-zen-mode,.close-zen-mode{font:0/0 a;color:transparent;text-shadow:none;background-color:transparent;border:0;background-repeat:no-repeat;width:32px;height:32px;display:block;position:absolute}.close-zen-mode{background-image:url("../img/icons/exit-zen.svg");right:1rem;top:1rem}.zen-page{position:relative;top:0;bottom:0;z-index:11;height:100%;width:100%}#zen{font-size:1.25rem;width:300px;height:80%;margin:0 auto;position:relative;top:10%}#zen:before,#zen:after{content:"";position:absolute;height:10%;width:100%;z-index:12;pointer-events:none}#preview .table{width:auto}.ui-resizable{position:relative}.ui-resizable-handle{position:absolute;font-size:.1px;z-index:99999;display:block}.ui-resizable-e{background-color:#666;border-right:8px solid #e8e8e8;border-left:1px solid #222;width:10px;z-index:88!important;position:relative}.ui-resizable-e:after{content:"-";display:block;position:absolute;top:calc(50% - 16px);left:0;height:25px;width:2px;background-color:rgba(0,0,0,.4);margin:3px}#editor{cursor:ew-resize;position:relative;z-index:auto}.profile-pic{float:left;width:250px}#_default_ a::before{color:#A0AABF}#_default_ img{display:none}#_default_ #_default_{display:block;float:left;max-width:38%;word-wrap:break-word}#_default_ .default-ad{display:none}#_default_ ._default_{display:block}#_default_ a{color:#35d7bb;text-decoration:none}#_default_ a:hover{color:#8ae8d8}#_default_ .default-image{display:none}#_default_ .default-title:after{content:""}#_default_ .default-title,#_default_ .default-text,#_default_ .default-description{display:inline}#_default_ .default-title{position:relative;font-weight:600;display:none}#_default_ a:before{position:relative;top:0;padding:5px;color:#a0aabf;content:"Ad";text-transform:uppercase;font-size:8px;font-family:Verdana,sans-serif}#_default_{display:block;float:left;max-width:38%;word-wrap:break-word}#_default_ ._default_{display:block;font-size:.75rem;height:51px;letter-spacing:1px;line-height:1rem;padding:18px 24px}body{max-width:1024px;margin:0 auto;overflow:auto;padding:2%}.split{overflow:scroll;padding:0!important;-webkit-overflow-scrolling:touch}.split-editor{padding-left:0;padding-right:0;position:relative;z-index:3}.show-preview .split-editor{display:none}.split-preview{background-color:#fff;display:none;top:0;position:relative;z-index:4}.show-preview .split-preview{display:block}#editor{font-size:1rem;font-family:"Ubuntu Mono",Monaco;font-weight:400;line-height:2rem;width:100%;height:100%}#editor .ace_gutter{-webkit-font-smoothing:antialiased}.editor-header{width:50%;float:left;border-bottom:1px solid #E8E8E8;position:relative}.editor-header--first{border-right:1px solid #E8E8E8}.editor-header .title{display:inline-block}.preview-html{padding:15px}.preview-html a{color:#A0AABF;text-decoration:underline}.preview-src{white-space:normal}.preview-mode-toggle-src{background-image:url("../img/icons/code.svg")}.preview-mode-toggle-src,.preview-mode-toggle-html{font:0/0 a;color:transparent;text-shadow:none;background-color:transparent;border:0;background-repeat:no-repeat;width:32px;height:32px;display:block;position:absolute;right:.5rem;top:.5rem;display:none}.preview-mode-toggle-html{background-image:url("../img/icons/eye.svg")}.sr-only{visibility:hidden;text-overflow:110%;overflow:hidden;top:-100px;position:absolute}.mnone{margin:0!important}@media screen and (min-width:27.5em){html{font-size:.875em}body{font-size:1rem}ul,ol{margin-bottom:.83999rem;padding-top:.16001rem}p{padding-top:.66001rem}p,pre{margin-bottom:1.33999rem}pre,blockquote p{font-size:1rem;padding-top:.66001rem}blockquote p{margin-bottom:.33999rem}h1{font-size:2.0571429rem;margin-bottom:.21999rem;padding-top:.78001rem}h2{font-size:1.953125rem;margin-bottom:.1835837rem;padding-top:.8164163rem}h3{font-size:1.6457143rem;margin-bottom:.07599rem;padding-top:.92401rem}h4{font-size:1.5625rem;margin-bottom:.546865rem;padding-top:.453135rem}h5{font-size:1.25rem;margin-bottom:-.56251rem;padding-top:.56251rem}h6{font-size:1rem;margin-bottom:-.65001rem;padding-top:.65001rem}.g{margin-left:-16px;margin-right:-16px}.g-b{padding-left:16px;padding-right:16px}.g-b--m1of1{width:100%}.g-b--m1of2,.g-b--m2of4,.g-b--m3of6,.g-b--m4of8,.g-b--m5of10,.g-b--m6of12{width:50%}.g-b--m1of3,.g-b--m2of6,.g-b--m4of12{width:33.333%}.g-b--m2of3,.g-b--m4of6,.g-b--m8of12{width:66.666%}.g-b--m1of4,.g-b--m2of8,.g-b--m3of12{width:25%}.g-b--m3of4,.g-b--m6of8,.g-b--m9of12{width:75%}.g-b--m1of5,.g-b--m2of10{width:20%}.g-b--m2of5,.g-b--m4of10{width:40%}.g-b--m3of5,.g-b--m6of10{width:60%}.g-b--m4of5,.g-b--m8of10{width:80%}.g-b--m1of6,.g-b--m2of12{width:16.666%}.g-b--m5of6,.g-b--m10of12{width:83.333%}.g-b--m1of8{width:12.5%}.g-b--m3of8{width:37.5%}.g-b--m5of8{width:62.5%}.g-b--m7of8{width:87.5%}.g-b--m1of10{width:10%}.g-b--m3of10{width:30%}.g-b--m7of10{width:70%}.g-b--m9of10{width:90%}.g-b--m1of12{width:8.333%}.g-b--m5of12{width:41.666%}.g-b--m7of12{width:58.333%}.g-b--m11of12{width:91.666%}.g-b--push--m1of1{margin-left:100%}.g-b--push--m1of2,.g-b--push--m2of4,.g-b--push--m3of6,.g-b--push--m4of8,.g-b--push--m5of10,.g-b--push--m6of12{margin-left:50%}.g-b--push--m1of3,.g-b--push--m2of6,.g-b--push--m4of12{margin-left:33.333%}.g-b--push--m2of3,.g-b--push--m4of6,.g-b--push--m8of12{margin-left:66.666%}.g-b--push--m1of4,.g-b--push--m2of8,.g-b--push--m3of12{margin-left:25%}.g-b--push--m3of4,.g-b--push--m6of8,.g-b--push--m9of12{margin-left:75%}.g-b--push--m1of5,.g-b--push--m2of10{margin-left:20%}.g-b--push--m2of5,.g-b--push--m4of10{margin-left:40%}.g-b--push--m3of5,.g-b--push--m6of10{margin-left:60%}.g-b--push--m4of5,.g-b--push--m8of10{margin-left:80%}.g-b--push--m1of6,.g-b--push--m2of12{margin-left:16.666%}.g-b--push--m5of6,.g-b--push--m10of12{margin-left:83.333%}.g-b--push--m1of8{margin-left:12.5%}.g-b--push--m3of8{margin-left:37.5%}.g-b--push--m5of8{margin-left:62.5%}.g-b--push--m7of8{margin-left:87.5%}.g-b--push--m1of10{margin-left:10%}.g-b--push--m3of10{margin-left:30%}.g-b--push--m7of10{margin-left:70%}.g-b--push--m9of10{margin-left:90%}.g-b--push--m1of12{margin-left:8.333%}.g-b--push--m5of12{margin-left:41.666%}.g-b--push--m7of12{margin-left:58.333%}.g-b--push--m11of12{margin-left:91.666%}.g-b--pull--m1of1{margin-right:100%}.g-b--pull--m1of2,.g-b--pull--m2of4,.g-b--pull--m3of6,.g-b--pull--m4of8,.g-b--pull--m5of10,.g-b--pull--m6of12{margin-right:50%}.g-b--pull--m1of3,.g-b--pull--m2of6,.g-b--pull--m4of12{margin-right:33.333%}.g-b--pull--m2of3,.g-b--pull--m4of6,.g-b--pull--m8of12{margin-right:66.666%}.g-b--pull--m1of4,.g-b--pull--m2of8,.g-b--pull--m3of12{margin-right:25%}.g-b--pull--m3of4,.g-b--pull--m6of8,.g-b--pull--m9of12{margin-right:75%}.g-b--pull--m1of5,.g-b--pull--m2of10{margin-right:20%}.g-b--pull--m2of5,.g-b--pull--m4of10{margin-right:40%}.g-b--pull--m3of5,.g-b--pull--m6of10{margin-right:60%}.g-b--pull--m4of5,.g-b--pull--m8of10{margin-right:80%}.g-b--pull--m1of6,.g-b--pull--m2of12{margin-right:16.666%}.g-b--pull--m5of6,.g-b--pull--m10of12{margin-right:83.333%}.g-b--pull--m1of8{margin-right:12.5%}.g-b--pull--m3of8{margin-right:37.5%}.g-b--pull--m5of8{margin-right:62.5%}.g-b--pull--m7of8{margin-right:87.5%}.g-b--pull--m1of10{margin-right:10%}.g-b--pull--m3of10{margin-right:30%}.g-b--pull--m7of10{margin-right:70%}.g-b--pull--m9of10{margin-right:90%}.g-b--pull--m1of12{margin-right:8.333%}.g-b--pull--m5of12{margin-right:41.666%}.g-b--pull--m7of12{margin-right:58.333%}.g-b--pull--m11of12{margin-right:91.666%}.splashscreen p{font-size:1.25rem;margin-bottom:1.43749rem;padding-top:.56251rem}.title{font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem}.title-document{margin-bottom:.89999rem;padding-top:.10001rem}.title-document,.settings a{font-size:1.25rem}.words,.characters{font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem}.modal--dillinger.about .modal-dialog,#zen{font-size:1.25rem}#zen{width:400px}#editor{font-size:1rem}}@media screen and (min-width:46.25em){html{font-size:.875em}body{font-size:1rem}ul,ol{margin-bottom:.83999rem;padding-top:.16001rem}p{padding-top:.66001rem}p,pre{margin-bottom:1.33999rem}pre,blockquote p{font-size:1rem;padding-top:.66001rem}blockquote p{margin-bottom:.33999rem}h1{font-size:2.0571429rem;margin-bottom:.21999rem;padding-top:.78001rem}h2{font-size:1.953125rem;margin-bottom:.1835837rem;padding-top:.8164163rem}h3{font-size:1.6457143rem;margin-bottom:.07599rem;padding-top:.92401rem}h4{font-size:1.5625rem;margin-bottom:.546865rem;padding-top:.453135rem}h5{font-size:1.25rem;margin-bottom:-.56251rem;padding-top:.56251rem}h6{font-size:1rem;margin-bottom:-.65001rem;padding-top:.65001rem}.g{margin-left:-16px;margin-right:-16px}.g-b{padding-left:16px;padding-right:16px}.g-b--t1of1{width:100%}.g-b--t1of2,.g-b--t2of4,.g-b--t3of6,.g-b--t4of8,.g-b--t5of10,.g-b--t6of12{width:50%}.g-b--t1of3,.g-b--t2of6,.g-b--t4of12{width:33.333%}.g-b--t2of3,.g-b--t4of6,.g-b--t8of12{width:66.666%}.g-b--t1of4,.g-b--t2of8,.g-b--t3of12{width:25%}.g-b--t3of4,.g-b--t6of8,.g-b--t9of12{width:75%}.g-b--t1of5,.g-b--t2of10{width:20%}.g-b--t2of5,.g-b--t4of10{width:40%}.g-b--t3of5,.g-b--t6of10{width:60%}.g-b--t4of5,.g-b--t8of10{width:80%}.g-b--t1of6,.g-b--t2of12{width:16.666%}.g-b--t5of6,.g-b--t10of12{width:83.333%}.g-b--t1of8{width:12.5%}.g-b--t3of8{width:37.5%}.g-b--t5of8{width:62.5%}.g-b--t7of8{width:87.5%}.g-b--t1of10{width:10%}.g-b--t3of10{width:30%}.g-b--t7of10{width:70%}.g-b--t9of10{width:90%}.g-b--t1of12{width:8.333%}.g-b--t5of12{width:41.666%}.g-b--t7of12{width:58.333%}.g-b--t11of12{width:91.666%}.g-b--push--t1of1{margin-left:100%}.g-b--push--t1of2,.g-b--push--t2of4,.g-b--push--t3of6,.g-b--push--t4of8,.g-b--push--t5of10,.g-b--push--t6of12{margin-left:50%}.g-b--push--t1of3,.g-b--push--t2of6,.g-b--push--t4of12{margin-left:33.333%}.g-b--push--t2of3,.g-b--push--t4of6,.g-b--push--t8of12{margin-left:66.666%}.g-b--push--t1of4,.g-b--push--t2of8,.g-b--push--t3of12{margin-left:25%}.g-b--push--t3of4,.g-b--push--t6of8,.g-b--push--t9of12{margin-left:75%}.g-b--push--t1of5,.g-b--push--t2of10{margin-left:20%}.g-b--push--t2of5,.g-b--push--t4of10{margin-left:40%}.g-b--push--t3of5,.g-b--push--t6of10{margin-left:60%}.g-b--push--t4of5,.g-b--push--t8of10{margin-left:80%}.g-b--push--t1of6,.g-b--push--t2of12{margin-left:16.666%}.g-b--push--t5of6,.g-b--push--t10of12{margin-left:83.333%}.g-b--push--t1of8{margin-left:12.5%}.g-b--push--t3of8{margin-left:37.5%}.g-b--push--t5of8{margin-left:62.5%}.g-b--push--t7of8{margin-left:87.5%}.g-b--push--t1of10{margin-left:10%}.g-b--push--t3of10{margin-left:30%}.g-b--push--t7of10{margin-left:70%}.g-b--push--t9of10{margin-left:90%}.g-b--push--t1of12{margin-left:8.333%}.g-b--push--t5of12{margin-left:41.666%}.g-b--push--t7of12{margin-left:58.333%}.g-b--push--t11of12{margin-left:91.666%}.g-b--pull--t1of1{margin-right:100%}.g-b--pull--t1of2,.g-b--pull--t2of4,.g-b--pull--t3of6,.g-b--pull--t4of8,.g-b--pull--t5of10,.g-b--pull--t6of12{margin-right:50%}.g-b--pull--t1of3,.g-b--pull--t2of6,.g-b--pull--t4of12{margin-right:33.333%}.g-b--pull--t2of3,.g-b--pull--t4of6,.g-b--pull--t8of12{margin-right:66.666%}.g-b--pull--t1of4,.g-b--pull--t2of8,.g-b--pull--t3of12{margin-right:25%}.g-b--pull--t3of4,.g-b--pull--t6of8,.g-b--pull--t9of12{margin-right:75%}.g-b--pull--t1of5,.g-b--pull--t2of10{margin-right:20%}.g-b--pull--t2of5,.g-b--pull--t4of10{margin-right:40%}.g-b--pull--t3of5,.g-b--pull--t6of10{margin-right:60%}.g-b--pull--t4of5,.g-b--pull--t8of10{margin-right:80%}.g-b--pull--t1of6,.g-b--pull--t2of12{margin-right:16.666%}.g-b--pull--t5of6,.g-b--pull--t10of12{margin-right:83.333%}.g-b--pull--t1of8{margin-right:12.5%}.g-b--pull--t3of8{margin-right:37.5%}.g-b--pull--t5of8{margin-right:62.5%}.g-b--pull--t7of8{margin-right:87.5%}.g-b--pull--t1of10{margin-right:10%}.g-b--pull--t3of10{margin-right:30%}.g-b--pull--t7of10{margin-right:70%}.g-b--pull--t9of10{margin-right:90%}.g-b--pull--t1of12{margin-right:8.333%}.g-b--pull--t5of12{margin-right:41.666%}.g-b--pull--t7of12{margin-right:58.333%}.g-b--pull--t11of12{margin-right:91.666%}.splashscreen-dillinger{width:500px}.splashscreen p{font-size:1.25rem;margin-bottom:1.43749rem;padding-top:.56251rem}.title{font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem}.title-document{font-size:1.25rem;margin-bottom:.89999rem;padding-top:.10001rem}.menu .menu-item--save-to,.menu .menu-item--import-from{display:block}.menu .menu-item--preview,.menu .menu-item--save-to.in-sidebar,.menu .menu-item--import-from.in-sidebar{display:none}.settings a{font-size:1.25rem}.words,.characters{font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem}.modal--dillinger.about .modal-dialog{font-size:1.25rem}.enter-zen-mode{display:block}.close-zen-mode{right:3rem;top:3rem}#zen{font-size:1.25rem;width:500px}.split-editor{border-right:1px solid #E8E8E8;float:left;padding-right:16px;width:50%}.show-preview .split-editor{display:block}.split-preview{display:block;float:right;position:relative;top:0;width:50%}#editor{font-size:1rem}.preview-mode-toggle-src,.preview-mode-toggle-html{display:block}}@media screen and (min-width:62.5em){html{font-size:.875em}body{font-size:1rem}ul,ol{margin-bottom:.83999rem;padding-top:.16001rem}p{padding-top:.66001rem}p,pre{margin-bottom:1.33999rem}pre,blockquote p{font-size:1rem;padding-top:.66001rem}blockquote p{margin-bottom:.33999rem}h1{font-size:2.0571429rem;margin-bottom:.21999rem;padding-top:.78001rem}h2{font-size:1.953125rem;margin-bottom:.1835837rem;padding-top:.8164163rem}h3{font-size:1.6457143rem;margin-bottom:.07599rem;padding-top:.92401rem}h4{font-size:1.5625rem;margin-bottom:.546865rem;padding-top:.453135rem}h5{font-size:1.25rem;margin-bottom:-.56251rem;padding-top:.56251rem}h6{font-size:1rem;margin-bottom:-.65001rem;padding-top:.65001rem}.g{margin-left:-16px;margin-right:-16px}.g-b{padding-left:16px;padding-right:16px}.g-b--d1of1{width:100%}.g-b--d1of2,.g-b--d2of4,.g-b--d3of6,.g-b--d4of8,.g-b--d5of10,.g-b--d6of12{width:50%}.g-b--d1of3,.g-b--d2of6,.g-b--d4of12{width:33.333%}.g-b--d2of3,.g-b--d4of6,.g-b--d8of12{width:66.666%}.g-b--d1of4,.g-b--d2of8,.g-b--d3of12{width:25%}.g-b--d3of4,.g-b--d6of8,.g-b--d9of12{width:75%}.g-b--d1of5,.g-b--d2of10{width:20%}.g-b--d2of5,.g-b--d4of10{width:40%}.g-b--d3of5,.g-b--d6of10{width:60%}.g-b--d4of5,.g-b--d8of10{width:80%}.g-b--d1of6,.g-b--d2of12{width:16.666%}.g-b--d5of6,.g-b--d10of12{width:83.333%}.g-b--d1of8{width:12.5%}.g-b--d3of8{width:37.5%}.g-b--d5of8{width:62.5%}.g-b--d7of8{width:87.5%}.g-b--d1of10{width:10%}.g-b--d3of10{width:30%}.g-b--d7of10{width:70%}.g-b--d9of10{width:90%}.g-b--d1of12{width:8.333%}.g-b--d5of12{width:41.666%}.g-b--d7of12{width:58.333%}.g-b--d11of12{width:91.666%}.g-b--push--d1of1{margin-left:100%}.g-b--push--d1of2,.g-b--push--d2of4,.g-b--push--d3of6,.g-b--push--d4of8,.g-b--push--d5of10,.g-b--push--d6of12{margin-left:50%}.g-b--push--d1of3,.g-b--push--d2of6,.g-b--push--d4of12{margin-left:33.333%}.g-b--push--d2of3,.g-b--push--d4of6,.g-b--push--d8of12{margin-left:66.666%}.g-b--push--d1of4,.g-b--push--d2of8,.g-b--push--d3of12{margin-left:25%}.g-b--push--d3of4,.g-b--push--d6of8,.g-b--push--d9of12{margin-left:75%}.g-b--push--d1of5,.g-b--push--d2of10{margin-left:20%}.g-b--push--d2of5,.g-b--push--d4of10{margin-left:40%}.g-b--push--d3of5,.g-b--push--d6of10{margin-left:60%}.g-b--push--d4of5,.g-b--push--d8of10{margin-left:80%}.g-b--push--d1of6,.g-b--push--d2of12{margin-left:16.666%}.g-b--push--d5of6,.g-b--push--d10of12{margin-left:83.333%}.g-b--push--d1of8{margin-left:12.5%}.g-b--push--d3of8{margin-left:37.5%}.g-b--push--d5of8{margin-left:62.5%}.g-b--push--d7of8{margin-left:87.5%}.g-b--push--d1of10{margin-left:10%}.g-b--push--d3of10{margin-left:30%}.g-b--push--d7of10{margin-left:70%}.g-b--push--d9of10{margin-left:90%}.g-b--push--d1of12{margin-left:8.333%}.g-b--push--d5of12{margin-left:41.666%}.g-b--push--d7of12{margin-left:58.333%}.g-b--push--d11of12{margin-left:91.666%}.g-b--pull--d1of1{margin-right:100%}.g-b--pull--d1of2,.g-b--pull--d2of4,.g-b--pull--d3of6,.g-b--pull--d4of8,.g-b--pull--d5of10,.g-b--pull--d6of12{margin-right:50%}.g-b--pull--d1of3,.g-b--pull--d2of6,.g-b--pull--d4of12{margin-right:33.333%}.g-b--pull--d2of3,.g-b--pull--d4of6,.g-b--pull--d8of12{margin-right:66.666%}.g-b--pull--d1of4,.g-b--pull--d2of8,.g-b--pull--d3of12{margin-right:25%}.g-b--pull--d3of4,.g-b--pull--d6of8,.g-b--pull--d9of12{margin-right:75%}.g-b--pull--d1of5,.g-b--pull--d2of10{margin-right:20%}.g-b--pull--d2of5,.g-b--pull--d4of10{margin-right:40%}.g-b--pull--d3of5,.g-b--pull--d6of10{margin-right:60%}.g-b--pull--d4of5,.g-b--pull--d8of10{margin-right:80%}.g-b--pull--d1of6,.g-b--pull--d2of12{margin-right:16.666%}.g-b--pull--d5of6,.g-b--pull--d10of12{margin-right:83.333%}.g-b--pull--d1of8{margin-right:12.5%}.g-b--pull--d3of8{margin-right:37.5%}.g-b--pull--d5of8{margin-right:62.5%}.g-b--pull--d7of8{margin-right:87.5%}.g-b--pull--d1of10{margin-right:10%}.g-b--pull--d3of10{margin-right:30%}.g-b--pull--d7of10{margin-right:70%}.g-b--pull--d9of10{margin-right:90%}.g-b--pull--d1of12{margin-right:8.333%}.g-b--pull--d5of12{margin-right:41.666%}.g-b--pull--d7of12{margin-right:58.333%}.g-b--pull--d11of12{margin-right:91.666%}.splashscreen-dillinger{width:700px}.splashscreen p{font-size:1.25rem;margin-bottom:1.43749rem;padding-top:.56251rem}.title{font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem}.title-document{font-size:1.25rem;margin-bottom:.89999rem;padding-top:.10001rem}.menu .menu-item--export-as{display:block}.menu .menu-item--preview{display:none}.settings a{font-size:1.25rem}.words,.characters{font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem}.modal--dillinger.about .modal-dialog,#zen{font-size:1.25rem}#zen{width:700px}#editor{font-size:1rem}}@media screen and (min-width:87.5em){html{font-size:.875em}body{font-size:1rem}ul,ol{margin-bottom:.83999rem;padding-top:.16001rem}p{padding-top:.66001rem}p,pre{margin-bottom:1.33999rem}pre,blockquote p{font-size:1rem;padding-top:.66001rem}blockquote p{margin-bottom:.33999rem}h1{font-size:2.0571429rem;margin-bottom:.21999rem;padding-top:.78001rem}h2{font-size:1.953125rem;margin-bottom:.1835837rem;padding-top:.8164163rem}h3{font-size:1.6457143rem;margin-bottom:.07599rem;padding-top:.92401rem}h4{font-size:1.5625rem;margin-bottom:.546865rem;padding-top:.453135rem}h5{font-size:1.25rem;margin-bottom:-.56251rem;padding-top:.56251rem}h6{font-size:1rem;margin-bottom:-.65001rem;padding-top:.65001rem}.splashscreen-dillinger{width:800px}.splashscreen p{font-size:1.25rem;margin-bottom:1.43749rem;padding-top:.56251rem}.title{font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem}.title-document{margin-bottom:.89999rem;padding-top:.10001rem}.title-document,.settings a{font-size:1.25rem}.words,.characters{font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem}.modal--dillinger.about .modal-dialog,#zen{font-size:1.25rem}#editor{font-size:1rem}}@media (min-width:768px){.form-inline .form-group{display:inline-block;margin-bottom:0;vertical-align:middle}.form-inline .form-control{display:inline-block;width:auto;vertical-align:middle}.form-inline .input-group{display:inline-table;vertical-align:middle}.form-inline .input-group .input-group-addon,.form-inline .input-group .input-group-btn,.form-inline .input-group .form-control{width:auto}.form-inline .input-group>.form-control{width:100%}.form-inline .control-label{margin-bottom:0;vertical-align:middle}.form-inline .radio,.form-inline .checkbox{display:inline-block;margin-top:0;margin-bottom:0;vertical-align:middle}.form-inline .radio label,.form-inline .checkbox label{padding-left:0}.form-inline .radio input[type="radio"],.form-inline .checkbox input[type="checkbox"]{position:relative;margin-left:0}.form-inline .has-feedback .form-control-feedback{top:0}.form-horizontal .control-label{text-align:right;margin-bottom:0;padding-top:7px}.form-horizontal .form-group-lg .control-label{padding-top:14.3px}.form-horizontal .form-group-sm .control-label{padding-top:6px}.modal-dialog{width:600px;margin:30px auto}.modal-content{box-shadow:0 5px 15px rgba(0,0,0,.5)}.modal-sm{width:300px}}@media (min-width:992px){.modal-lg{width:900px}}@media screen and (max-width:1200px){#_default_{max-width:30%}#_default_ ._default_{font-size:.825rem;line-height:.875rem;padding:12px 12px 6px 24px;text-align:justify}}@media screen and (max-width:1100px){#_default_{max-width:27%}#_default_ ._default_{font-size:.8rem;line-height:.85rem;padding:12px 6px 6px 24px;text-align:justify}}@media screen and (max-width:1000px){#_default_{max-width:24%}#_default_ ._default_{font-size:.775rem;line-height:.8rem;padding:12px 6px 6px 24px;text-align:justify}}@media screen and (max-width:900px){#_default_{max-width:30%}}@media screen and (max-width:767px){.table-responsive{width:100%;margin-bottom:15px;overflow-y:hidden;overflow-x:auto;-ms-overflow-style:-ms-autohiding-scrollbar;border:1px solid #ddd;-webkit-overflow-scrolling:touch}.table-responsive>.table{margin-bottom:0}.table-responsive>.table>thead>tr>th,.table-responsive>.table>thead>tr>td,.table-responsive>.table>tbody>tr>th,.table-responsive>.table>tbody>tr>td,.table-responsive>.table>tfoot>tr>th,.table-responsive>.table>tfoot>tr>td{white-space:nowrap}.table-responsive>.table-bordered{border:0}.table-responsive>.table-bordered>thead>tr>th:first-child,.table-responsive>.table-bordered>thead>tr>td:first-child,.table-responsive>.table-bordered>tbody>tr>th:first-child,.table-responsive>.table-bordered>tbody>tr>td:first-child,.table-responsive>.table-bordered>tfoot>tr>th:first-child,.table-responsive>.table-bordered>tfoot>tr>td:first-child{border-left:0}.table-responsive>.table-bordered>thead>tr>th:last-child,.table-responsive>.table-bordered>thead>tr>td:last-child,.table-responsive>.table-bordered>tbody>tr>th:last-child,.table-responsive>.table-bordered>tbody>tr>td:last-child,.table-responsive>.table-bordered>tfoot>tr>th:last-child,.table-responsive>.table-bordered>tfoot>tr>td:last-child{border-right:0}.table-responsive>.table-bordered>tbody>tr:last-child>th,.table-responsive>.table-bordered>tbody>tr:last-child>td,.table-responsive>.table-bordered>tfoot>tr:last-child>th,.table-responsive>.table-bordered>tfoot>tr:last-child>td{border-bottom:0}}@media screen and (max-width:720px){#_default_{max-width:60%}#_default_ ._default_{font-size:.75rem;line-height:1rem;padding:12px 24px}}@media screen and (max-width:620px){#_default_{max-width:50%}#_default_ ._default_{font-size:.66rem;letter-spacing:1px;line-height:1rem;padding:10px 24px}}@media screen and (max-width:520px){#_default_ ._default_{font-size:.4rem;line-height:.875rem;padding:6px 12px 6px 24px;text-align:justify}}@media screen and (max-width:460px){#_default_{display:none}}@media screen and (max-width:46.1875em){.editor-header{display:none}.editor-header--first{display:block;width:100%}}</style></head><body id="preview">
<h1 class="code-line" data-line-start="0" data-line-end="1"><a id="InstructLab_0"></a>InstructLab</h1>
<p class="has-line-data" data-line-start="2" data-line-end="3"><em>ChatGPT</em> (LLM)  LLM  </p>
<h3 class="code-line" data-line-start="5" data-line-end="6"><a id="RAG__5"></a>RAG </h3>
<p class="has-line-data" data-line-start="7" data-line-end="8"></p>
<p class="has-line-data" data-line-start="9" data-line-end="10">LlamaMixtral</p>
<ol>
<li class="has-line-data" data-line-start="11" data-line-end="12">LLMs</li>
<li class="has-line-data" data-line-start="12" data-line-end="13">/</li>
<li class="has-line-data" data-line-start="13" data-line-end="15"></li>
</ol>
<p class="has-line-data" data-line-start="15" data-line-end="16"><em>HuggingFace</em><em>LLM</em>   </p>
<p class="has-line-data" data-line-start="17" data-line-end="18"><strong></strong></p>
<p class="has-line-data" data-line-start="20" data-line-end="21"><strong></strong><strong></strong></p>
<p class="has-line-data" data-line-start="22" data-line-end="23"></p>
<p class="has-line-data" data-line-start="24" data-line-end="25">![]</p>
<p class="has-line-data" data-line-start="26" data-line-end="27"></p>
<p class="has-line-data" data-line-start="28" data-line-end="29"></p>
<p class="has-line-data" data-line-start="30" data-line-end="31"></p>
<p class="has-line-data" data-line-start="32" data-line-end="33"><a href="https://instructlab.ai/">InstructLab</a></p>
<h2 class="code-line" data-line-start="34" data-line-end="35"><a id="InstructLab__34"></a><strong>InstructLab </strong></h2>
<p class="has-line-data" data-line-start="36" data-line-end="37">![InstructLab  Hugging Face ]</p>
<p class="has-line-data" data-line-start="38" data-line-end="39">InstructLab  Hugging Face </p>
<p class="has-line-data" data-line-start="40" data-line-end="41">InstructLab  AI  LLM   </p>
<p class="has-line-data" data-line-start="42" data-line-end="43">InstructLab  LLM  InstructLab IBM Research  InstructLab <a href="https://research.ibm.com/blog/granite-code-models-open-source"></a><a href="https://www.ibm.com/blog/building-ai-for-business-ibms-granite-foundation-models/">Granite </a></p>
<p class="has-line-data" data-line-start="44" data-line-end="45"><strong></strong><em>LLM</em></p>
<p class="has-line-data" data-line-start="46" data-line-end="47">InstructLab <strong></strong></p>
<p class="has-line-data" data-line-start="48" data-line-end="49"><strong></strong>InstructLab LLM </p>
<p class="has-line-data" data-line-start="50" data-line-end="51"><strong></strong><a href="https://research.ibm.com/blog/LLM-generated-data">LAB</a> InstructLab  LLM </p>
<p class="has-line-data" data-line-start="52" data-line-end="53">InstructLab </p>
<p class="has-line-data" data-line-start="54" data-line-end="55">**<a href="https://www.zhihu.com/search?q=transformer%E6%A8%A1%E5%9E%8B&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22699500296%22%7D">transformer</a>, LLM, **</p>
<p class="has-line-data" data-line-start="56" data-line-end="57">![<strong>L</strong>arge-scale<strong>A</strong>lignment for chat<strong>B</strong>ots]</p>
<p class="has-line-data" data-line-start="58" data-line-end="59"><strong>L</strong>arge-scale<strong>A</strong>lignment for chat<strong>B</strong>ots</p>
<hr>
<h3 class="code-line" data-line-start="62" data-line-end="63"><a id="Install_with_Apple_Metal_on_M1M2M3_Macs_62"></a><strong>Install with Apple Metal on M1/M2/M3 Macs</strong></h3>
<pre><code class="has-line-data" data-line-start="65" data-line-end="70" class="language-bash">python3 -m venv --upgrade-deps venv
<span class="hljs-built_in">source</span> venv/bin/activate
pip cache remove llama_cpp_python
pip install instructlab
</code></pre>
<pre><code class="has-line-data" data-line-start="72" data-line-end="83" class="language-bash">/Users/yehua/instructlab

~/instructlab  <span class="hljs-built_in">source</span> venv/bin/activate
(venv)  ~/instructlab  ilab chat
/Users/yehua/instructlab/venv/lib/python3.<span class="hljs-number">9</span>/site-packages/urllib3/__init__.py:<span class="hljs-number">35</span>: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL <span class="hljs-number">1.1</span>.<span class="hljs-number">1</span>+, currently the <span class="hljs-string">'ssl'</span> module is compiled with <span class="hljs-string">'LibreSSL 2.8.3'</span>. See: https://github.com/urllib3/urllib3/issues/<span class="hljs-number">3020</span>
  warnings.warn(
 system 
 Welcome to InstructLab Chat w/ MODELS/MERLINITE-<span class="hljs-number">7</span>B-LAB-Q4_K_M.GGUF (<span class="hljs-built_in">type</span> /h <span class="hljs-keyword">for</span> <span class="hljs-built_in">help</span>)                                                                                  

&gt;&gt;&gt;                                                                                                                                                           [S][default]
</code></pre>
<h3 class="code-line" data-line-start="84" data-line-end="85"><a id="_IBM_Granite__84"></a> IBM Granite </h3>
<p class="has-line-data" data-line-start="86" data-line-end="87"> HF Token = <code>xxxxxxxxxxxxxxxxxx</code></p>
<p class="has-line-data" data-line-start="88" data-line-end="89"><a href="https://huggingface.co/settings/tokens">https://huggingface.co/settings/tokens</a></p>
<p class="has-line-data" data-line-start="90" data-line-end="91">HF Token</p>
<pre><code class="has-line-data" data-line-start="93" data-line-end="95" class="language-bash">HF_TOKEN=&lt;YOUR HUGGINGFACE TOKEN GOES HERE&gt; ilab download --repository=TheBloke/Mixtral-<span class="hljs-number">8</span>x7B-Instruct-v0.<span class="hljs-number">1</span>-GGUF --filename=mixtral-<span class="hljs-number">8</span>x7b-instruct-v0.<span class="hljs-number">1</span>.Q4_K_M.gguf
</code></pre>
<p class="has-line-data" data-line-start="96" data-line-end="97">model</p>
<pre><code class="has-line-data" data-line-start="99" data-line-end="102" class="language-bash">$ ilab download --repository instructlab/granite-<span class="hljs-number">7</span>b-lab-GGUF --filename granite-<span class="hljs-number">7</span>b-lab-Q4_K_M.gguf <span class="hljs-comment">#IBM Granite</span>
$ ilab download --repository QuantFactory/Qwen2-<span class="hljs-number">7</span>B-Instruct-deccp-GGUF --filename Qwen2-<span class="hljs-number">7</span>B-Instruct-deccp.Q4_K_M.gguf <span class="hljs-comment">#</span>
</code></pre>
<p class="has-line-data" data-line-start="103" data-line-end="104">CPU</p>
<pre><code class="has-line-data" data-line-start="106" data-line-end="109" class="language-bash">$ ilab serve --model-path models/granite-<span class="hljs-number">7</span>b-lab-Q4_K_M.gguf --num-threads <span class="hljs-number">14</span> --max-ctx-size <span class="hljs-number">20480</span>
$ ilab chat -m models/granite-<span class="hljs-number">7</span>b-lab-Q4_K_M.gguf
</code></pre>
<h3 class="code-line" data-line-start="110" data-line-end="111"><a id="_Knowledge__taxonomies_110"></a> Knowledge  taxonomies</h3>
<p class="has-line-data" data-line-start="112" data-line-end="113">The LAB method is driven by <a href="https://github.com/instructlab/taxonomy">taxonomies</a>, which are largely created manually and with care.</p>
<p class="has-line-data" data-line-start="114" data-line-end="115"><code>qna.yaml</code></p>
<pre><code class="has-line-data" data-line-start="117" data-line-end="131" class="language-yaml">version: 2
task_description: &lt;string&gt;
created_by: &lt;string&gt;
seed_examples: #
  - question: &lt;string&gt;
    answer: |
      &lt;multi-line string&gt;
  - context: |
      &lt;multi-line string&gt;
    question: &lt;string&gt;
    answer: |
      &lt;multi-line string&gt;
  ...
</code></pre>
<p class="has-line-data" data-line-start="132" data-line-end="133"><strong></strong></p>
<pre><code class="has-line-data" data-line-start="135" data-line-end="161" class="language-yaml">version: 2
task_description: Teach the Large Language Model about the movie Oppenheimer
created_by: IBM Ecosystems Engineering SI Lab
domain: movie
seed_examples:
- answer: |
    The movie Oppenheimer was written, directed, and produced by Christopher Nolan1.
  question: Who directed the movie Oppenheimer?
- answer: |
    The movie follows the life of J. Robert Oppenheimer, the American theoretical physicist who helped develop the first nuclear weapons during World War II.
  question: What is the movie Oppenheimer about?
- answer: |
    The movie starred Cillian Murphy as Oppenheimer, alongside Robert Downey Jr. as the United States Atomic Energy Commission member Lewis Strauss. The ensemble supporting cast includes Emily Blunt, Matt Damon, Florence Pugh, Josh Hartnett, Casey Affleck, Rami Malek, and Kenneth Branagh
  question: Who starred in the movie Oppenheimer?
- answer: |
    The movie Oppenheimer was released on July 21, 2023
  question: When was the movie Oppenheimer released?
- answer: |
    The movie Oppenheimer received critical acclaim and won seven Academy Awards, including Best Picture, Best Director for Nolan, Best Actor for Murphy and Best Supporting Actor for Downey. It grossed over $976 million worldwide, becoming the third-highest-grossing film of 2023, the highest-grossing World War II-related film, the highest-grossing biographical film and the second-highest-grossing R-rated film
  question: How was the movie Oppenheimer received?
document:
  repo: https://github.com/mikeyehua/instructlab.git
  commit: 76a2773
  patterns:
  - Oppenheimer.md
</code></pre>
<p class="has-line-data" data-line-start="162" data-line-end="163"><code>  qna.yaml  attribution.txt taxonomy/knowledge/movies/oppenheimer </code></p>
<p class="has-line-data" data-line-start="164" data-line-end="165"><code>document</code></p>
<p class="has-line-data" data-line-start="166" data-line-end="167"><a href="https://github.com/mikeyehua/instructlab/blob/main/Oppenheimer.md">instructlab/Oppenheimer.md at main  mikeyehua/instructlab</a></p>
<p class="has-line-data" data-line-start="168" data-line-end="169"><a href="https://github.com/syedaameena/InstructLab-Skill/blob/main/QnA.yaml?utm_source=ibm_developer&amp;utm_content=in_content_link&amp;utm_id=tutorials_awb-tuning-pretrained-llms-instructlab">https://github.com/syedaameena/InstructLab-Skill/blob/main/QnA.yaml?utm_source=ibm_developer&amp;utm_content=in_content_link&amp;utm_id=tutorials_awb-tuning-pretrained-llms-instructlab</a></p>
<ul>
<li class="has-line-data" data-line-start="170" data-line-end="175"><code>document</code>: The source of your knowledge contribution.
<ul>
<li class="has-line-data" data-line-start="171" data-line-end="172"><code>repo</code>: The URL to your repository that holds your knowledge markdown files.</li>
<li class="has-line-data" data-line-start="172" data-line-end="173"><code>commit</code>: The SHA of the commit in your repository with your knowledge markdown files.</li>
<li class="has-line-data" data-line-start="173" data-line-end="175"><code>patterns</code>: A list of glob patterns specifying the markdown files in your repository. Any glob pattern that starts with``, such as<code>.md</code>, must be quoted due to YAML rules. For example,<code>&quot;*.md&quot;</code>.</li>
</ul>
</li>
</ul>
<p class="has-line-data" data-line-start="175" data-line-end="177">&lt;aside&gt;<br>
 YAML </p>
<p class="has-line-data" data-line-start="178" data-line-end="179"><a href="https://www.yamllint.com/">https://www.yamllint.com/</a></p>
<p class="has-line-data" data-line-start="180" data-line-end="181">&lt;/aside&gt;</p>
<p class="has-line-data" data-line-start="182" data-line-end="183"><strong>Knowledge: YAML examples</strong></p>
<p class="has-line-data" data-line-start="184" data-line-end="186">&lt;aside&gt;<br>
 <strong></strong></p>
<blockquote>
<p class="has-line-data" data-line-start="187" data-line-end="188">Who starred in the movie Oppenheimer?</p>
</blockquote>
<p class="has-line-data" data-line-start="190" data-line-end="192">elapsed 9.513 seconds -<br>
The movie Oppenheimer is <strong>an upcoming film</strong> adaptation of the Pulitzer Prize-winning biography American Prometheus: The Life of J. Robert Oppenheimer by Kira Davis. Cillian Murphy has been cast as the title character, J. Robert Oppenheimer. Other notable stars who have joined the cast include Matt Damon as Leslie Groves, the head of the Yale project and the father of Oppenheimers wife, Emily Blunt as Katherine Manning Oppenheimer, the wife of J. Robert Oppenheimer, and Alden Ehrenreich as Donald Oppenheimer, their son.</p>
<p class="has-line-data" data-line-start="193" data-line-end="194">The film is directed by Christopher Nolan, known for his work on blockbuster movies like Batman Begins, The Dark Knight, and Interstellar. Oppenheimer is set to explore the life of J. Robert Oppenheimer, the man who led the project that developed the first atomic bomb during World War II. The movie is expected to delve into the moral complexities of nuclear power and the human cost of scientific progress.</p>
<p class="has-line-data" data-line-start="195" data-line-end="196">While there isnt much information available about the plot yet, I can provide you with some context on the historical figures and events that Oppenheimer might depict. The film will likely cover Oppenheimers early life, his education at the prestigious Institute for Advanced Studies in Princeton, New Jersey, and his work at the Los Alamos National Laboratory during the Manhattan Project. Additionally, the movie may explore Oppenheimers struggles with the moral implications of nuclear weapons and his eventual condemnation by the U.S. government during the Red Scare.</p>
<p class="has-line-data" data-line-start="197" data-line-end="198"><strong>Stay tuned for more updates</strong> on Oppenheimer as filming progresses and <strong>more information becomes available</strong>. If you have any questions about the cast, directors, or historical context of the movie, please let me know!</p>
<blockquote>
<p class="has-line-data" data-line-start="199" data-line-end="200">What are the release dates for Oppenheimer movie?</p>
</blockquote>
<p class="has-line-data" data-line-start="202" data-line-end="204">elapsed 5.935 seconds -<br>
As of my last update, the release date for Oppenheimer <strong>is not yet set</strong>. The film was announced in 2021, and production has recently started. Christopher Nolan, the director, often takes his time to perfect his films before releasing them. Given the complexity of the subject matter and the intricate plot, its likely that Oppenheimer will require a considerable amount of post-production work.</p>
<p class="has-line-data" data-line-start="205" data-line-end="206"><strong>Once filming is complete and the movie is ready for release</strong>, the production team will announce an official date. Typically, this happens several months before the release date. I recommend checking official film news sources and the films official website for the most up-to-date information on the release date.</p>
<p class="has-line-data" data-line-start="207" data-line-end="208">In the meantime, you can look forward to other movies released by Christopher Nolan or explore other fascinating historical figures and events that have been brought to the big screen. For example, Dunkirk (2017) and Interstellar (2014) are two of Nolans previous films that showcase his ability to create immersive and thought-provoking cinematic experiences.</p>
<p class="has-line-data" data-line-start="209" data-line-end="210">&lt;/aside&gt;</p>
<h3 class="code-line" data-line-start="211" data-line-end="212"><a id="_211"></a></h3>
<p class="has-line-data" data-line-start="213" data-line-end="214"><code>ilab diff</code> </p>
<h3 class="code-line" data-line-start="215" data-line-end="216"><a id="_215"></a></h3>
<p class="has-line-data" data-line-start="217" data-line-end="218"> <code>ilab generate</code>  taxonomies  knowledge </p>
<pre><code class="has-line-data" data-line-start="220" data-line-end="222" class="language-yaml">ilab generate --model models/granite-7b-lab-Q4_K_M.gguf --num-instructions 100 --num-cpus 20 --server-ctx-size 20480
</code></pre>
<p class="has-line-data" data-line-start="223" data-line-end="224">Apple SiliconGPU</p>
<p class="has-line-data" data-line-start="225" data-line-end="226"> <code>ctrl + c</code><code>qna.yaml</code><code>generate</code></p>
<p class="has-line-data" data-line-start="227" data-line-end="228"> M1 Mac Pro  21  ETA</p>
<p class="has-line-data" data-line-start="230" data-line-end="231"></p>
<pre><code class="has-line-data" data-line-start="233" data-line-end="241" class="language-yaml">INFO 2024-06-20 15:37:52,612 generate_data.py:608 101 instructions generated, 10 discarded due to format (see generated/discarded_granite-7b-lab-Q4_K_M_2024-06-20T15_16_44.log), 2 discarded due to rouge score
INFO 2024-06-20 15:37:52,612 generate_data.py:612 Generation took **1270.44s

-rw-r--r--  1 yehua  staff     9242  6 20 15:36 discarded_granite-7b-lab-Q4_K_M_2024-06-20T15_16_44.log #
-rw-r--r--  1 yehua  staff  6280617  6 20 15:37 generated_granite-7b-lab-Q4_K_M_2024-06-20T15_16_44.json #json
-rw-r--r--  1 yehua  staff     2511  6 20 15:37 test_granite-7b-lab-Q4_K_M_2024-06-20T15_16_44.jsonl #jsonl
-rw-r--r--  1 yehua  staff    94763  6 20 15:37 train_granite-7b-lab-Q4_K_M_2024-06-20T15_16_44.jsonl #jsonl**
</code></pre>
<h3 class="code-line" data-line-start="242" data-line-end="243"><a id="_242"></a></h3>
<p class="has-line-data" data-line-start="244" data-line-end="245"></p>
<p class="has-line-data" data-line-start="246" data-line-end="247"><code>ilab</code><code>GGUF</code> CLI  <code>cuda-toolkit</code> NVIDIA GPU </p>
<p class="has-line-data" data-line-start="248" data-line-end="249"><strong><em></em></strong>- </p>
<p class="has-line-data" data-line-start="250" data-line-end="251"><code>ilab train --gguf-model-path models/granite-7b-lab-Q4_K_M.gguf</code></p>
<p class="has-line-data" data-line-start="252" data-line-end="253"><code>ilab train --gguf-model-path models/granite-7b-lab-Q4_K_M.gguf --device 'cuda'</code></p>
<pre><code class="has-line-data" data-line-start="255" data-line-end="314" class="language-yaml">ilab train --gguf-model-path models/granite-7b-lab-Q4_K_M.gguf
/Users/yehua/instructlab/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
[INFO] Loading
model-00003-of-00003.safetensors: 100%|| 4.54G/4.54G [05:06&lt;00:00, 8.66MB/s]
model-00001-of-00003.safetensors: 100%|| 4.94G/4.94G [05:09&lt;00:00, 8.71MB/s]
model-00002-of-00003.safetensors: 100%|| 5.00G/5.00G [05:18&lt;00:00, 9.35MB/s]
Fetching 11 files: 100%|| 11/11 [05:20&lt;00:00, 29.18s/it]
/Users/yehua/instructlab/venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.| 5.00G/5.00G [05:18&lt;00:00, 25.9MB/s]
  warnings.warn(
config.json: 100%|| 644/644 [00:00&lt;00:00, 628kB/s]
tokenizer_config.json: 100%|| 2.33k/2.33k [00:00&lt;00:00, 1.47MB/s]
tokenizer.model: 100%|| 493k/493k [00:00&lt;00:00, 668kB/s]
tokenizer.json: 100%|| 1.80M/1.80M [00:00&lt;00:00, 2.10MB/s]
added_tokens.json: 100%|| 119/119 [00:00&lt;00:00, 67.5kB/s]
special_tokens_map.json: 100%|| 655/655 [00:00&lt;00:00, 207kB/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
dtype=mlx.core.float16
[INFO] Quantizing
Using model_type='mistral'
Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total parameters 1244.079M
Trainable parameters 1.704M
Loading datasets
Training
Epoch 1: Iter 1: Val loss 2.087, Val took 26.183s
Iter 010: Train loss 1.848, It/sec 0.167, Tokens/sec 132.733
Epoch 1: Iter 10: Val loss 1.232, Val took 25.780s
Iter 10: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-010.npz.
Iter 020: Train loss 1.253, It/sec 0.195, Tokens/sec 133.049
Epoch 1: Iter 20: Val loss 1.053, Val took 25.881s
Iter 20: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-020.npz.
Iter 030: Train loss 0.924, It/sec 0.156, Tokens/sec 116.944
Epoch 2: Iter 30: Val loss 0.977, Val took 26.585s
Iter 30: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-030.npz.
Iter 040: Train loss 0.932, It/sec 0.167, Tokens/sec 121.478
Epoch 2: Iter 40: Val loss 0.937, Val took 26.198s
Iter 40: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-040.npz.
Iter 050: Train loss 0.763, It/sec 0.158, Tokens/sec 117.815
Epoch 3: Iter 50: Val loss 0.924, Val took 26.306s
Iter 50: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-050.npz.
Iter 060: Train loss 0.787, It/sec 0.165, Tokens/sec 119.941
Epoch 3: Iter 60: Val loss 0.903, Val took 27.079s
Iter 60: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-060.npz.
Iter 070: Train loss 0.668, It/sec 0.158, Tokens/sec 117.039
Epoch 4: Iter 70: Val loss 0.927, Val took 26.463s
Iter 70: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-070.npz.
Iter 080: Train loss 0.655, It/sec 0.132, Tokens/sec 96.649
Epoch 4: Iter 80: Val loss 0.911, Val took 26.960s
Iter 80: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-080.npz.
Iter 090: Train loss 0.599, It/sec 0.149, Tokens/sec 110.820
Epoch 5: Iter 90: Val loss 0.967, Val took 26.356s
Iter 90: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-090.npz.
Iter 100: Train loss 0.542, It/sec 0.152, Tokens/sec 111.259
Epoch 5: Iter 100: Val loss 0.947, Val took 28.326s
Iter 100: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-100.npz.
</code></pre>
<p class="has-line-data" data-line-start="315" data-line-end="316">  M1 MacBook Pro  100  30  ETA</p>
<p class="has-line-data" data-line-start="317" data-line-end="318"><code>ilab</code><code>instructlab-merlinite-7b-lab</code></p>
<pre><code class="has-line-data" data-line-start="320" data-line-end="323" class="language-yaml">drwxr-xr-x  14 yehua  staff   448  6 20 18:20 instructlab-merlinite-7b-lab
drwxr-xr-x  20 yehua  staff   640  6 20 18:36 instructlab-merlinite-7b-lab-mlx-q
</code></pre>
<h3 class="code-line" data-line-start="324" data-line-end="325"><a id="_324"></a></h3>
<p class="has-line-data" data-line-start="326" data-line-end="327"><code>ilab test</code></p>
<p class="has-line-data" data-line-start="328" data-line-end="329"><code> ilab test --data-dir my-data --model-dir models/ibm/merlinite-7b</code></p>
<p class="has-line-data" data-line-start="330" data-line-end="331"><code>ilab test --data-dir ./taxonomy_data --model-dir instructlab-merlinite-7b-lab-mlx-q</code></p>
<pre><code class="has-line-data" data-line-start="333" data-line-end="497" class="language-yaml">ilab test

system prompt: You are an AI language model developed by IBM Research. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.
[1]
 user prompt: Who directed the movie Oppenheimer?
expected output: The movie Oppenheimer was written, directed, and produced by Christopher Nolan1.

-----model output BEFORE training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
LoRA init skipped
Total parameters 1242.375M
Trainable parameters 0.000M
Loading datasets
LoRA loading skipped
Generating
==========
Christopher Nolan
==========

-----model output AFTER training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total parameters 1244.079M
Trainable parameters 1.704M
Loading datasets
Generating
==========
Christopher Nolan directed the movie Oppenheimer. He is known for his involvement in the production of the film and was chosen by the studio to take on this project.
==========
[2]
 user prompt: What is the movie Oppenheimer about?
expected output: The movie follows the life of J. Robert Oppenheimer, the American theoretical physicist who helped develop the first nuclear weapons during World War II.

-----model output BEFORE training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
LoRA init skipped
Total parameters 1242.375M
Trainable parameters 0.000M
Loading datasets
LoRA loading skipped
Generating
==========
&quot;Oppenheimer&quot; is a film that delves into the life and times of J. Robert Oppenheimer, the brilliant physicist who led the Manhattan Project during World War II. The movie offers a gripping and thought-provoking portrayal of Oppenheimer's personal and professional journey while working on the development of the atomic bomb. Here are some key aspects of the movie and the historical context that you might find useful for your article:

1.
==========

-----model output AFTER training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total parameters 1244.079M
Trainable parameters 1.704M
Loading datasets
Generating
==========
&quot;Oppenheimer&quot; is a biographical film directed by Christopher Nolan that tells the story of J. Robert Oppenheimer, the father of the atomic bomb and the director of the Los Alamos Laboratory during World War II. The movie explores Oppenheimer's personal life, his contributions to the development of nuclear weapons, and the impact of his actions on the world.

The film portrays Oppenheimer's guilt over the destruction caused by the
==========
[3]
 user prompt: Who starred in the movie Oppenheimer?
expected output: The movie starred Cillian Murphy as Oppenheimer, alongside Robert Downey Jr. as the United States Atomic Energy Commission member Lewis Strauss. The ensemble supporting cast includes Emily Blunt, Matt Damon, Florence Pugh, Josh Hartnett, Casey Affleck, Rami Malek, and Kenneth Branagh

-----model output BEFORE training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
LoRA init skipped
Total parameters 1242.375M
Trainable parameters 0.000M
Loading datasets
LoRA loading skipped
Generating
==========
&quot;Oppenheimer&quot; is a historical drama film directed by Christopher Nolan, with an impressive cast and crew. The lead role of Dr. J. Robert Oppenheimer, the physicist who oversaw the Manhattan Project, is portrayed by a well-known actor, Cillian Murphy. Additionally, the film features other esteemed actors, including Emily Blunt as Katherine, Olivia Thirlby as Leona Woods, Matthew Goode as Klaus Fuch
==========

-----model output AFTER training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total parameters 1244.079M
Trainable parameters 1.704M
Loading datasets
Generating
==========
Cillian Murphy, Matt Damon, Emily Blunt, Robert Downey Jr., Gary Oldman, and Benny Safdie.
==========
[4]
 user prompt: When was the movie Oppenheimer released?
expected output: The movie Oppenheimer was released on July 21, 2023

-----model output BEFORE training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
LoRA init skipped
Total parameters 1242.375M
Trainable parameters 0.000M
Loading datasets
LoRA loading skipped
Generating
==========
I'm sorry for the confusion, but I couldn't find information about the release date of the movie &quot;Oppenheimer.&quot; Would you like me to search for more details about the movie or answer a different question?
==========

-----model output AFTER training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total parameters 1244.079M
Trainable parameters 1.704M
Loading datasets
Generating
==========
The movie was released in theaters on July 21, 2023.
==========
[5]
 user prompt: How was the movie Oppenheimer received?
expected output: The movie Oppenheimer received critical acclaim and won seven Academy Awards, including Best Picture, Best Director for Nolan, Best Actor for Murphy and Best Supporting Actor for Downey. It grossed over $976 million worldwide, becoming the third-highest-grossing film of 2023, the highest-grossing World War II-related film, the highest-grossing biographical film and the second-highest-grossing R-rated film

-----model output BEFORE training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
LoRA init skipped
Total parameters 1242.375M
Trainable parameters 0.000M
Loading datasets
LoRA loading skipped
Generating
==========
The movie &quot;Oppenheimer&quot; has received mixed reviews from critics and audiences alike. While some laud the film for its powerful performances and thought-provoking themes, others have criticized it for its pacing and structure.

Critics have praised the movie's direction, cinematography, and visual effects. The performances of the lead actors, particularly Cillian Murphy as J. Robert Oppenheimer, have been widely praised. However, some have criticized
==========

-----model output AFTER training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total parameters 1244.079M
Trainable parameters 1.704M
Loading datasets
Generating
==========
The movie &quot;Oppenheimer&quot; received mixed reviews from critics and audiences. It was praised for its visual effects and historical accuracy, but criticized for its length and slow pacing. Some viewers felt that the film could have been more engaging and emotionally resonant.
</code></pre>
<h3 class="code-line" data-line-start="498" data-line-end="499"><a id="_498"></a><strong></strong></h3>
<p class="has-line-data" data-line-start="500" data-line-end="501"><code>GGUF</code></p>
<p class="has-line-data" data-line-start="502" data-line-end="503"></p>
<p class="has-line-data" data-line-start="504" data-line-end="505"><code>ilab convert</code></p>
<pre><code class="has-line-data" data-line-start="507" data-line-end="1481" class="language-bash">ilab convert

Loading pretrained model
Using model_<span class="hljs-built_in">type</span>=<span class="hljs-string">'mistral'</span>
Special tokens have been added <span class="hljs-keyword">in</span> the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
INFO <span class="hljs-number">2024</span>-<span class="hljs-number">06</span>-<span class="hljs-number">20</span> <span class="hljs-number">18</span>:<span class="hljs-number">48</span>:<span class="hljs-number">22</span>,<span class="hljs-number">055</span> lab.py:<span class="hljs-number">1343</span> deleting instructlab-merlinite-<span class="hljs-number">7</span>b-lab-mlx-q...
[INFO] Loading
Special tokens have been added <span class="hljs-keyword">in</span> the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
dtype=&lt;class <span class="hljs-string">'numpy.float16'</span>&gt;
INFO <span class="hljs-number">2024</span>-<span class="hljs-number">06</span>-<span class="hljs-number">20</span> <span class="hljs-number">18</span>:<span class="hljs-number">50</span>:<span class="hljs-number">26</span>,<span class="hljs-number">422</span> lab.py:<span class="hljs-number">1352</span> deleting instructlab-merlinite-<span class="hljs-number">7</span>b-lab-mlx-q-fused...
Loading model file instructlab-merlinite-<span class="hljs-number">7</span>b-lab-trained/model.safetensors
params = Params(n_vocab=<span class="hljs-number">32008</span>, n_embd=<span class="hljs-number">4096</span>, n_layer=<span class="hljs-number">32</span>, n_ctx=<span class="hljs-number">32768</span>, n_ff=<span class="hljs-number">14336</span>, n_head=<span class="hljs-number">32</span>, n_head_kv=<span class="hljs-number">8</span>, n_experts=None, n_experts_used=None, f_norm_eps=<span class="hljs-number">1</span>e-<span class="hljs-number">05</span>, rope_scaling_<span class="hljs-built_in">type</span>=None, f_rope_freq_base=<span class="hljs-number">10000.0</span>, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath(<span class="hljs-string">'instructlab-merlinite-7b-lab-trained'</span>))
Found vocab files: {<span class="hljs-string">'spm'</span>: PosixPath(<span class="hljs-string">'instructlab-merlinite-7b-lab-trained/tokenizer.model'</span>), <span class="hljs-string">'bpe'</span>: None, <span class="hljs-string">'hfft'</span>: PosixPath(<span class="hljs-string">'instructlab-merlinite-7b-lab-trained/tokenizer.json'</span>)}
Loading vocab file PosixPath(<span class="hljs-string">'instructlab-merlinite-7b-lab-trained/tokenizer.model'</span>), <span class="hljs-built_in">type</span> <span class="hljs-string">'spm'</span>
Vocab info: &lt;SentencePieceVocab with <span class="hljs-number">32000</span> base tokens and <span class="hljs-number">5</span> added tokens&gt;
Special vocab info: &lt;SpecialVocab with <span class="hljs-number">0</span> merges, special tokens {<span class="hljs-string">'bos'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'eos'</span>: <span class="hljs-number">32000</span>, <span class="hljs-string">'unk'</span>: <span class="hljs-number">0</span>, <span class="hljs-string">'pad'</span>: <span class="hljs-number">32001</span>}, add special tokens {<span class="hljs-string">'bos'</span>: False, <span class="hljs-string">'eos'</span>: False}&gt;
Permuting layer <span class="hljs-number">0</span>
Permuting layer <span class="hljs-number">1</span>
Permuting layer <span class="hljs-number">2</span>
Permuting layer <span class="hljs-number">3</span>
Permuting layer <span class="hljs-number">4</span>
Permuting layer <span class="hljs-number">5</span>
Permuting layer <span class="hljs-number">6</span>
Permuting layer <span class="hljs-number">7</span>
Permuting layer <span class="hljs-number">8</span>
Permuting layer <span class="hljs-number">9</span>
Permuting layer <span class="hljs-number">10</span>
Permuting layer <span class="hljs-number">11</span>
Permuting layer <span class="hljs-number">12</span>
Permuting layer <span class="hljs-number">13</span>
Permuting layer <span class="hljs-number">14</span>
Permuting layer <span class="hljs-number">15</span>
Permuting layer <span class="hljs-number">16</span>
Permuting layer <span class="hljs-number">17</span>
Permuting layer <span class="hljs-number">18</span>
Permuting layer <span class="hljs-number">19</span>
Permuting layer <span class="hljs-number">20</span>
Permuting layer <span class="hljs-number">21</span>
Permuting layer <span class="hljs-number">22</span>
Permuting layer <span class="hljs-number">23</span>
Permuting layer <span class="hljs-number">24</span>
Permuting layer <span class="hljs-number">25</span>
Permuting layer <span class="hljs-number">26</span>
Permuting layer <span class="hljs-number">27</span>
Permuting layer <span class="hljs-number">28</span>
Permuting layer <span class="hljs-number">29</span>
Permuting layer <span class="hljs-number">30</span>
Permuting layer <span class="hljs-number">31</span>
lm_head.weight                                   -&gt; output.weight                            | F16    | [<span class="hljs-number">32008</span>, <span class="hljs-number">4096</span>]
model.embed_tokens.weight                        -&gt; token_embd.weight                        | F16    | [<span class="hljs-number">32008</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">0</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">0</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">0</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">0</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">0</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">0</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">0</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">0</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">0</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">0</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">0</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">0</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">0</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">0</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">0</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">0</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">0</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">0</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">1</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">1</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">1</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">1</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">1</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">1</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">1</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">1</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">1</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">1</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">1</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">1</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">1</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">1</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">1</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">1</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">1</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">1</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">10</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">10</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">10</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">10</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">10</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">10</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">10</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">10</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">10</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">10</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">10</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">10</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">10</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">10</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">10</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">10</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">10</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">10</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">11</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">11</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">11</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">11</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">11</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">11</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">11</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">11</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">11</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">11</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">11</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">11</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">11</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">11</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">11</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">11</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">11</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">11</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">12</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">12</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">12</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">12</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">12</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">12</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">12</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">12</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">12</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">12</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">12</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">12</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">12</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">12</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">12</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">12</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">12</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">12</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">13</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">13</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">13</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">13</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">13</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">13</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">13</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">13</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">13</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">13</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">13</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">13</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">13</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">13</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">13</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">13</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">13</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">13</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">14</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">14</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">14</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">14</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">14</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">14</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">14</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">14</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">14</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">14</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">14</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">14</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">14</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">14</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">14</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">14</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">14</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">14</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">15</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">15</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">15</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">15</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">15</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">15</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">15</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">15</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">15</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">15</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">15</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">15</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">15</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">15</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">15</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">15</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">15</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">15</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">16</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">16</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">16</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">16</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">16</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">16</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">16</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">16</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">16</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">16</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">16</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">16</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">16</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">16</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">16</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">16</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">16</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">16</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">17</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">17</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">17</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">17</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">17</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">17</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">17</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">17</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">17</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">17</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">17</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">17</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">17</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">17</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">17</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">17</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">17</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">17</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">18</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">18</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">18</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">18</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">18</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">18</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">18</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">18</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">18</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">18</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">18</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">18</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">18</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">18</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">18</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">18</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">18</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">18</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">19</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">19</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">19</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">19</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">19</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">19</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">19</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">19</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">19</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">19</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">19</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">19</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">19</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">19</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">19</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">19</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">19</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">19</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">2</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">2</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">2</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">2</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">2</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">2</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">2</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">2</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">2</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">2</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">2</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">2</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">2</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">2</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">2</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">2</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">2</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">2</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">20</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">20</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">20</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">20</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">20</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">20</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">20</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">20</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">20</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">20</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">20</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">20</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">20</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">20</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">20</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">20</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">20</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">20</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">21</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">21</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">21</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">21</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">21</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">21</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">21</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">21</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">21</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">21</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">21</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">21</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">21</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">21</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">21</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">21</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">21</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">21</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">22</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">22</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">22</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">22</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">22</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">22</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">22</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">22</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">22</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">22</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">22</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">22</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">22</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">22</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">22</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">22</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">22</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">22</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">23</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">23</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">23</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">23</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">23</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">23</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">23</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">23</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">23</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">23</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">23</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">23</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">23</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">23</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">23</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">23</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">23</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">23</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">24</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">24</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">24</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">24</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">24</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">24</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">24</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">24</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">24</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">24</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">24</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">24</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">24</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">24</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">24</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">24</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">24</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">24</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">25</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">25</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">25</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">25</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">25</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">25</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">25</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">25</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">25</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">25</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">25</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">25</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">25</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">25</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">25</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">25</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">25</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">25</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">26</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">26</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">26</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">26</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">26</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">26</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">26</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">26</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">26</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">26</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">26</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">26</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">26</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">26</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">26</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">26</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">26</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">26</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">27</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">27</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">27</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">27</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">27</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">27</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">27</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">27</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">27</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">27</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">27</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">27</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">27</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">27</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">27</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">27</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">27</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">27</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">28</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">28</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">28</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">28</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">28</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">28</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">28</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">28</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">28</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">28</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">28</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">28</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">28</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">28</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">28</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">28</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">28</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">28</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">29</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">29</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">29</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">29</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">29</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">29</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">29</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">29</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">29</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">29</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">29</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">29</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">29</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">29</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">29</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">29</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">29</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">29</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">3</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">3</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">3</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">3</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">3</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">3</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">3</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">3</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">3</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">3</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">3</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">3</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">3</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">3</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">3</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">3</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">3</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">3</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">30</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">30</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">30</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">30</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">30</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">30</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">30</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">30</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">30</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">30</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">30</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">30</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">30</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">30</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">30</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">30</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">30</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">30</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">31</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">31</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">31</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">31</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">31</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">31</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">31</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">31</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">31</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">31</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">31</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">31</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">31</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">31</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">31</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">31</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">31</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">31</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">4</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">4</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">4</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">4</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">4</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">4</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">4</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">4</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">4</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">4</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">4</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">4</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">4</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">4</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">4</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">4</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">4</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">4</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">5</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">5</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">5</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">5</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">5</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">5</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">5</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">5</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">5</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">5</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">5</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">5</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">5</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">5</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">5</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">5</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">5</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">5</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">6</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">6</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">6</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">6</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">6</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">6</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">6</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">6</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">6</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">6</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">6</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">6</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">6</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">6</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">6</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">6</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">6</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">6</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">7</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">7</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">7</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">7</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">7</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">7</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">7</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">7</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">7</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">7</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">7</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">7</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">7</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">7</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">7</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">7</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">7</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">7</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">8</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">8</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">8</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">8</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">8</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">8</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">8</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">8</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">8</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">8</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">8</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">8</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">8</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">8</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">8</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">8</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">8</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">8</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">9</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">9</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">9</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">9</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">9</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">9</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">9</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">9</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">9</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">9</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">9</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">9</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">9</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">9</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">9</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">9</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">9</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">9</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.norm.weight                                -&gt; output_norm.weight                       | F16    | [<span class="hljs-number">4096</span>]
Writing instructlab-merlinite-<span class="hljs-number">7</span>b-lab-trained/instructlab-merlinite-<span class="hljs-number">7</span>b-lab.gguf, format <span class="hljs-number">1</span>
Padding vocab with <span class="hljs-number">3</span> token(s) - &lt;dummy00001&gt; through &lt;dummy00003&gt;
gguf: This GGUF file is <span class="hljs-keyword">for</span> Little Endian only
gguf: Setting special token <span class="hljs-built_in">type</span> bos to <span class="hljs-number">1</span>
gguf: Setting special token <span class="hljs-built_in">type</span> eos to <span class="hljs-number">32000</span>
gguf: Setting special token <span class="hljs-built_in">type</span> unk to <span class="hljs-number">0</span>
gguf: Setting special token <span class="hljs-built_in">type</span> pad to <span class="hljs-number">32001</span>
gguf: Setting add_bos_token to False
gguf: Setting add_eos_token to False
gguf: Setting chat_template to {% <span class="hljs-keyword">for</span> message <span class="hljs-keyword">in</span> messages %}{% <span class="hljs-keyword">if</span> message[<span class="hljs-string">'role'</span>] == <span class="hljs-string">'system'</span> %}{{<span class="hljs-string">'&lt;|system|&gt;'</span>+ <span class="hljs-string">'
'</span> + message[<span class="hljs-string">'content'</span>] + <span class="hljs-string">'
'</span>}}{% <span class="hljs-keyword">elif</span> message[<span class="hljs-string">'role'</span>] == <span class="hljs-string">'user'</span> %}{{<span class="hljs-string">'&lt;|user|&gt;'</span> + <span class="hljs-string">'
'</span> + message[<span class="hljs-string">'content'</span>] + <span class="hljs-string">'
'</span>}}{% <span class="hljs-keyword">elif</span> message[<span class="hljs-string">'role'</span>] == <span class="hljs-string">'assistant'</span> %}{{<span class="hljs-string">'&lt;|assistant|&gt;'</span> + <span class="hljs-string">'
'</span> + message[<span class="hljs-string">'content'</span>] + <span class="hljs-string">'&lt;|endoftext|&gt;'</span> + (<span class="hljs-string">''</span> <span class="hljs-keyword">if</span> loop.last <span class="hljs-keyword">else</span> <span class="hljs-string">'
'</span>)}}{% endif %}{% endfor %}
[  <span class="hljs-number">1</span>/<span class="hljs-number">291</span>] Writing tensor output.weight                          | size  <span class="hljs-number">32008</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">0</span>
[  <span class="hljs-number">2</span>/<span class="hljs-number">291</span>] Writing tensor token_embd.weight                      | size  <span class="hljs-number">32008</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">0</span>
[  <span class="hljs-number">3</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">0</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">0</span>
[  <span class="hljs-number">4</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">0</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">0</span>
[  <span class="hljs-number">5</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">0</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">0</span>
[  <span class="hljs-number">6</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">0</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">0</span>
[  <span class="hljs-number">7</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">0</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">0</span>
[  <span class="hljs-number">8</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">0</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">0</span>
[  <span class="hljs-number">9</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">0</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">10</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">0</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">11</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">0</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">12</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">1</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">13</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">1</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">14</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">1</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">15</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">1</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">16</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">1</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">17</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">1</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">18</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">1</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">19</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">1</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">20</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">1</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">21</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">10</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">22</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">10</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">23</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">10</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">24</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">10</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">25</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">10</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">26</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">10</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">27</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">10</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">28</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">10</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">29</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">10</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">30</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">11</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">31</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">11</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">32</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">11</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">33</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">11</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">34</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">11</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">35</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">11</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">36</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">11</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">37</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">11</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">38</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">11</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">39</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">12</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">40</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">12</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">41</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">12</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">42</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">12</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">43</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">12</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">44</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">12</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">45</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">12</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">46</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">12</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">47</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">12</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">48</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">13</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">49</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">13</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">50</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">13</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">51</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">13</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">52</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">13</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">53</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">13</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">54</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">13</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">55</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">13</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">56</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">13</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">57</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">14</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">58</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">14</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">59</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">14</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">60</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">14</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">61</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">14</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">62</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">14</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">63</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">14</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">64</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">14</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">65</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">14</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">66</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">15</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">67</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">15</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">68</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">15</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">69</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">15</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">70</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">15</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">71</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">15</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">72</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">15</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">73</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">15</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">74</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">15</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">75</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">16</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">76</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">16</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">77</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">16</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">78</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">16</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">79</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">16</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">80</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">16</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">81</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">16</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">82</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">16</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">83</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">16</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">84</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">17</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">85</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">17</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">86</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">17</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">87</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">17</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">88</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">17</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">89</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">17</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">90</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">17</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">91</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">17</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">92</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">17</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">93</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">18</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">94</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">18</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[ <span class="hljs-number">95</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">18</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[ <span class="hljs-number">96</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">18</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[ <span class="hljs-number">97</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">18</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">6</span>
[ <span class="hljs-number">98</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">18</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[ <span class="hljs-number">99</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">18</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">100</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">18</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">101</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">18</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">102</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">19</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">103</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">19</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">104</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">19</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">105</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">19</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">106</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">19</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">107</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">19</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">108</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">19</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">109</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">19</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">110</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">19</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">111</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">2</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">112</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">2</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">113</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">2</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">114</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">2</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">115</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">2</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">116</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">2</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">117</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">2</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">118</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">2</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">119</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">2</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">120</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">20</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">121</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">20</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">8</span>
[<span class="hljs-number">122</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">20</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">8</span>
[<span class="hljs-number">123</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">20</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">8</span>
[<span class="hljs-number">124</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">20</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">8</span>
[<span class="hljs-number">125</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">20</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">8</span>
[<span class="hljs-number">126</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">20</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">8</span>
[<span class="hljs-number">127</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">20</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">8</span>
[<span class="hljs-number">128</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">20</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">8</span>
[<span class="hljs-number">129</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">21</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">8</span>
[<span class="hljs-number">130</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">21</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">9</span>
[<span class="hljs-number">131</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">21</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">9</span>
[<span class="hljs-number">132</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">21</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">9</span>
[<span class="hljs-number">133</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">21</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">9</span>
[<span class="hljs-number">134</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">21</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">9</span>
[<span class="hljs-number">135</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">21</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">9</span>
[<span class="hljs-number">136</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">21</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">9</span>
[<span class="hljs-number">137</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">21</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">9</span>
[<span class="hljs-number">138</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">22</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">9</span>
[<span class="hljs-number">139</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">22</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">10</span>
[<span class="hljs-number">140</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">22</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">10</span>
[<span class="hljs-number">141</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">22</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">10</span>
[<span class="hljs-number">142</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">22</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">10</span>
[<span class="hljs-number">143</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">22</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">10</span>
[<span class="hljs-number">144</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">22</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">10</span>
[<span class="hljs-number">145</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">22</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">10</span>
[<span class="hljs-number">146</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">22</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">10</span>
[<span class="hljs-number">147</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">23</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">10</span>
[<span class="hljs-number">148</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">23</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">11</span>
[<span class="hljs-number">149</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">23</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">11</span>
[<span class="hljs-number">150</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">23</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">11</span>
[<span class="hljs-number">151</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">23</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">11</span>
[<span class="hljs-number">152</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">23</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">11</span>
[<span class="hljs-number">153</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">23</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">11</span>
[<span class="hljs-number">154</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">23</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">11</span>
[<span class="hljs-number">155</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">23</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">11</span>
[<span class="hljs-number">156</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">24</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">11</span>
[<span class="hljs-number">157</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">24</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">12</span>
[<span class="hljs-number">158</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">24</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">12</span>
[<span class="hljs-number">159</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">24</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">12</span>
[<span class="hljs-number">160</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">24</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">12</span>
[<span class="hljs-number">161</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">24</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">12</span>
[<span class="hljs-number">162</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">24</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">12</span>
[<span class="hljs-number">163</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">24</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">12</span>
[<span class="hljs-number">164</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">24</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">12</span>
[<span class="hljs-number">165</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">25</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">12</span>
[<span class="hljs-number">166</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">25</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">13</span>
[<span class="hljs-number">167</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">25</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">13</span>
[<span class="hljs-number">168</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">25</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">13</span>
[<span class="hljs-number">169</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">25</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">13</span>
[<span class="hljs-number">170</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">25</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">13</span>
[<span class="hljs-number">171</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">25</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">13</span>
[<span class="hljs-number">172</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">25</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">13</span>
[<span class="hljs-number">173</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">25</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">13</span>
[<span class="hljs-number">174</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">26</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">13</span>
[<span class="hljs-number">175</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">26</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">14</span>
[<span class="hljs-number">176</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">26</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">14</span>
[<span class="hljs-number">177</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">26</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">14</span>
[<span class="hljs-number">178</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">26</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">14</span>
[<span class="hljs-number">179</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">26</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">14</span>
[<span class="hljs-number">180</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">26</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">14</span>
[<span class="hljs-number">181</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">26</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">14</span>
[<span class="hljs-number">182</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">26</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">14</span>
[<span class="hljs-number">183</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">27</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">14</span>
[<span class="hljs-number">184</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">27</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">15</span>
[<span class="hljs-number">185</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">27</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">15</span>
[<span class="hljs-number">186</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">27</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">15</span>
[<span class="hljs-number">187</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">27</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">15</span>
[<span class="hljs-number">188</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">27</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">15</span>
[<span class="hljs-number">189</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">27</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">15</span>
[<span class="hljs-number">190</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">27</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">15</span>
[<span class="hljs-number">191</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">27</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">15</span>
[<span class="hljs-number">192</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">28</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">15</span>
[<span class="hljs-number">193</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">28</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">16</span>
[<span class="hljs-number">194</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">28</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">16</span>
[<span class="hljs-number">195</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">28</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">16</span>
[<span class="hljs-number">196</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">28</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">16</span>
[<span class="hljs-number">197</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">28</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">16</span>
[<span class="hljs-number">198</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">28</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">16</span>
[<span class="hljs-number">199</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">28</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">16</span>
[<span class="hljs-number">200</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">28</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">16</span>
[<span class="hljs-number">201</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">29</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">16</span>
[<span class="hljs-number">202</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">29</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">17</span>
[<span class="hljs-number">203</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">29</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">17</span>
[<span class="hljs-number">204</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">29</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">17</span>
[<span class="hljs-number">205</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">29</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">17</span>
[<span class="hljs-number">206</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">29</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">17</span>
[<span class="hljs-number">207</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">29</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">17</span>
[<span class="hljs-number">208</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">29</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">17</span>
[<span class="hljs-number">209</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">29</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">17</span>
[<span class="hljs-number">210</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">3</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">17</span>
[<span class="hljs-number">211</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">3</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">18</span>
[<span class="hljs-number">212</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">3</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">18</span>
[<span class="hljs-number">213</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">3</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">18</span>
[<span class="hljs-number">214</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">3</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">18</span>
[<span class="hljs-number">215</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">3</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">18</span>
[<span class="hljs-number">216</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">3</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">18</span>
[<span class="hljs-number">217</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">3</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">18</span>
[<span class="hljs-number">218</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">3</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">18</span>
[<span class="hljs-number">219</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">30</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">18</span>
[<span class="hljs-number">220</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">30</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">19</span>
[<span class="hljs-number">221</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">30</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">19</span>
[<span class="hljs-number">222</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">30</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">19</span>
[<span class="hljs-number">223</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">30</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">19</span>
[<span class="hljs-number">224</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">30</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">19</span>
[<span class="hljs-number">225</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">30</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">19</span>
[<span class="hljs-number">226</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">30</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">19</span>
[<span class="hljs-number">227</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">30</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">19</span>
[<span class="hljs-number">228</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">31</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">19</span>
[<span class="hljs-number">229</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">31</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">20</span>
[<span class="hljs-number">230</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">31</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">20</span>
[<span class="hljs-number">231</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">31</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">20</span>
[<span class="hljs-number">232</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">31</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">20</span>
[<span class="hljs-number">233</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">31</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">20</span>
[<span class="hljs-number">234</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">31</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">20</span>
[<span class="hljs-number">235</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">31</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">20</span>
[<span class="hljs-number">236</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">31</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">20</span>
[<span class="hljs-number">237</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">4</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">20</span>
[<span class="hljs-number">238</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">4</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">21</span>
[<span class="hljs-number">239</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">4</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">21</span>
[<span class="hljs-number">240</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">4</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">21</span>
[<span class="hljs-number">241</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">4</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">21</span>
[<span class="hljs-number">242</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">4</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">21</span>
[<span class="hljs-number">243</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">4</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">21</span>
[<span class="hljs-number">244</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">4</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">21</span>
[<span class="hljs-number">245</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">4</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">21</span>
[<span class="hljs-number">246</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">5</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">21</span>
[<span class="hljs-number">247</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">5</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">22</span>
[<span class="hljs-number">248</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">5</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">22</span>
[<span class="hljs-number">249</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">5</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">22</span>
[<span class="hljs-number">250</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">5</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">22</span>
[<span class="hljs-number">251</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">5</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">22</span>
[<span class="hljs-number">252</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">5</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">22</span>
[<span class="hljs-number">253</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">5</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">22</span>
[<span class="hljs-number">254</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">5</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">22</span>
[<span class="hljs-number">255</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">6</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">22</span>
[<span class="hljs-number">256</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">6</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">23</span>
[<span class="hljs-number">257</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">6</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">23</span>
[<span class="hljs-number">258</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">6</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">23</span>
[<span class="hljs-number">259</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">6</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">23</span>
[<span class="hljs-number">260</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">6</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">23</span>
[<span class="hljs-number">261</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">6</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">23</span>
[<span class="hljs-number">262</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">6</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">23</span>
[<span class="hljs-number">263</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">6</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">23</span>
[<span class="hljs-number">264</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">7</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">23</span>
[<span class="hljs-number">265</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">7</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">24</span>
[<span class="hljs-number">266</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">7</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">24</span>
[<span class="hljs-number">267</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">7</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">24</span>
[<span class="hljs-number">268</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">7</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">24</span>
[<span class="hljs-number">269</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">7</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">24</span>
[<span class="hljs-number">270</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">7</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">24</span>
[<span class="hljs-number">271</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">7</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">24</span>
[<span class="hljs-number">272</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">7</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">24</span>
[<span class="hljs-number">273</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">8</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">24</span>
[<span class="hljs-number">274</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">8</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">25</span>
[<span class="hljs-number">275</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">8</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">25</span>
[<span class="hljs-number">276</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">8</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">25</span>
[<span class="hljs-number">277</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">8</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">25</span>
[<span class="hljs-number">278</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">8</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">25</span>
[<span class="hljs-number">279</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">8</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">25</span>
[<span class="hljs-number">280</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">8</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">25</span>
[<span class="hljs-number">281</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">8</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">25</span>
[<span class="hljs-number">282</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">9</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">25</span>
[<span class="hljs-number">283</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">9</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">26</span>
[<span class="hljs-number">284</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">9</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">26</span>
[<span class="hljs-number">285</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">9</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">26</span>
[<span class="hljs-number">286</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">9</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">26</span>
[<span class="hljs-number">287</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">9</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">26</span>
[<span class="hljs-number">288</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">9</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">26</span>
[<span class="hljs-number">289</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">9</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">26</span>
[<span class="hljs-number">290</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">9</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">26</span>
[<span class="hljs-number">291</span>/<span class="hljs-number">291</span>] Writing tensor output_norm.weight                     | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">26</span>
Wrote instructlab-merlinite-<span class="hljs-number">7</span>b-lab-trained/instructlab-merlinite-<span class="hljs-number">7</span>b-lab.gguf
INFO <span class="hljs-number">2024</span>-<span class="hljs-number">06</span>-<span class="hljs-number">20</span> <span class="hljs-number">18</span>:<span class="hljs-number">50</span>:<span class="hljs-number">54</span>,<span class="hljs-number">938</span> lab.py:<span class="hljs-number">1362</span> deleting safetensors files from instructlab-merlinite-<span class="hljs-number">7</span>b-lab-trained...
main: build = <span class="hljs-number">1</span> (<span class="hljs-number">784</span>e11d)
main: built with Apple clang version <span class="hljs-number">15.0</span>.<span class="hljs-number">0</span> (clang-<span class="hljs-number">1500.0</span>.<span class="hljs-number">40.1</span>) <span class="hljs-keyword">for</span> arm64-apple-darwin23.<span class="hljs-number">4.0</span>
main: quantizing <span class="hljs-string">'instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab.gguf'</span> to <span class="hljs-string">'instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab-Q4_K_M.gguf'</span> as Q4_K_M
llama_model_loader: loaded meta data with <span class="hljs-number">23</span> key-value pairs and <span class="hljs-number">291</span> tensors from instructlab-merlinite-<span class="hljs-number">7</span>b-lab-trained/instructlab-merlinite-<span class="hljs-number">7</span>b-lab.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides <span class="hljs-keyword">do</span> not apply <span class="hljs-keyword">in</span> this output.
llama_model_loader: - kv   <span class="hljs-number">0</span>:                       general.architecture str              = llama
llama_model_loader: - kv   <span class="hljs-number">1</span>:                               general.name str              = .
llama_model_loader: - kv   <span class="hljs-number">2</span>:                       llama.context_length u32              = <span class="hljs-number">32768</span>
llama_model_loader: - kv   <span class="hljs-number">3</span>:                     llama.embedding_length u32              = <span class="hljs-number">4096</span>
llama_model_loader: - kv   <span class="hljs-number">4</span>:                          llama.block_count u32              = <span class="hljs-number">32</span>
llama_model_loader: - kv   <span class="hljs-number">5</span>:                  llama.feed_forward_length u32              = <span class="hljs-number">14336</span>
llama_model_loader: - kv   <span class="hljs-number">6</span>:                 llama.rope.dimension_count u32              = <span class="hljs-number">128</span>
llama_model_loader: - kv   <span class="hljs-number">7</span>:                 llama.attention.head_count u32              = <span class="hljs-number">32</span>
llama_model_loader: - kv   <span class="hljs-number">8</span>:              llama.attention.head_count_kv u32              = <span class="hljs-number">8</span>
llama_model_loader: - kv   <span class="hljs-number">9</span>:     llama.attention.layer_norm_rms_epsilon f32              = <span class="hljs-number">0.000010</span>
llama_model_loader: - kv  <span class="hljs-number">10</span>:                       llama.rope.freq_base f32              = <span class="hljs-number">10000.000000</span>
llama_model_loader: - kv  <span class="hljs-number">11</span>:                          general.file_<span class="hljs-built_in">type</span> u32              = <span class="hljs-number">1</span>
llama_model_loader: - kv  <span class="hljs-number">12</span>:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  <span class="hljs-number">13</span>:                      tokenizer.ggml.tokens arr[str,<span class="hljs-number">32008</span>]   = [<span class="hljs-string">"&lt;unk&gt;"</span>, <span class="hljs-string">"&lt;s&gt;"</span>, <span class="hljs-string">"&lt;/s&gt;"</span>, <span class="hljs-string">"&lt;0x00&gt;"</span>, <span class="hljs-string">"&lt;...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32008]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32008]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32001
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 735584 bytes
[   1/ 291]                        output.weight - [ 4096, 32008,     1,     1], type =    f16, converting to q6_K .. size =   250.06 MiB -&gt;   102.56 MiB
[   2/ 291]                    token_embd.weight - [ 4096, 32008,     1,     1], type =    f16, converting to q4_K .. size =   250.06 MiB -&gt;    70.33 MiB
[   3/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[   4/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[   5/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[   6/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[   7/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[   8/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[   9/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  10/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  11/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[  12/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  13/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[  14/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  15/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  16/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  17/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  18/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  19/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  20/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[  21/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  22/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[  23/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  24/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  25/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  26/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  27/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  28/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  29/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[  30/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  31/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[  32/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  33/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  34/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  35/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  36/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  37/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  38/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[  39/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  40/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  41/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  42/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  43/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  44/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  45/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  46/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  47/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  48/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  49/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  50/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  51/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  52/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  53/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  54/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  55/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  56/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  57/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  58/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[  59/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  60/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  61/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  62/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  63/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  64/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  65/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[  66/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  67/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  68/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  69/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  70/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  71/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  72/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  73/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  74/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  75/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  76/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  77/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  78/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  79/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  80/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  81/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  82/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  83/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  84/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  85/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[  86/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  87/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  88/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  89/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  90/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  91/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  92/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[  93/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  94/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  95/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  96/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  97/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  98/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  99/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 100/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 101/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 102/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 103/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 104/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 105/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 106/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 107/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 108/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 109/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 110/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 111/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 112/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 113/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 114/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 115/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 116/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 117/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 118/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 119/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 120/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 121/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 122/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 123/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 124/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 125/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 126/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 127/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 128/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 129/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 130/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 131/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 132/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 133/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 134/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 135/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 136/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 137/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 138/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 139/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 140/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 141/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 142/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 143/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 144/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 145/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 146/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 147/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 148/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 149/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 150/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 151/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 152/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 153/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 154/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 155/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 156/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 157/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 158/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 159/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 160/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 161/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 162/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 163/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 164/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 165/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 166/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 167/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 168/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 169/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 170/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 171/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 172/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 173/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 174/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 175/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 176/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 177/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 178/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 179/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 180/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 181/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 182/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 183/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 184/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 185/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 186/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 187/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 188/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 189/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 190/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 191/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 192/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 193/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 194/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 195/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 196/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 197/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 198/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 199/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 200/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 201/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 202/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 203/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 204/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 205/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 206/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 207/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 208/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 209/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 210/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 211/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 212/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 213/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 214/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 215/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 216/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 217/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 218/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 219/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 220/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 221/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 222/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 223/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 224/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 225/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 226/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 227/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 228/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 229/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 230/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 231/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 232/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 233/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 234/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 235/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 236/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 237/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 238/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 239/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 240/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 241/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 242/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 243/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 244/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 245/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 246/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 247/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 248/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 249/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 250/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 251/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 252/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 253/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 254/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 255/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 256/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 257/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 258/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 259/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 260/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 261/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 262/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 263/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 264/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 265/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 266/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 267/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 268/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 269/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 270/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 271/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 272/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 273/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 274/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 275/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 276/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 277/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 278/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 279/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 280/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 281/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 282/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 283/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 284/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 285/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 286/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 287/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 288/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 289/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 290/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
llama_model_quantize_internal: model size  = 13813.14 MB
llama_model_quantize_internal: quant size  =  4165.41 MB
INFO 2024-06-20 18:51:53,586 lab.py:1372 deleting instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab.gguf...
</span></code></pre>
<p class="has-line-data" data-line-start="1482" data-line-end="1483"> gguf <code>ilab</code><code>instructlab-merlinite-7b-lab-trained</code></p>
<p class="has-line-data" data-line-start="1484" data-line-end="1486">&lt;aside&gt;<br>
 <code>Llama.cpp</code> GPU</p>
<p class="has-line-data" data-line-start="1487" data-line-end="1488">&lt;/aside&gt;</p>
<h3 class="code-line" data-line-start="1489" data-line-end="1490"><a id="_1489"></a><strong></strong></h3>
<p class="has-line-data" data-line-start="1491" data-line-end="1492"></p>
<p class="has-line-data" data-line-start="1493" data-line-end="1494"><code>ilab serve --model-path instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab-Q4_K_M.gguf --num-threads 14 --max-ctx-size 20480</code></p>
<p class="has-line-data" data-line-start="1495" data-line-end="1496"></p>
<p class="has-line-data" data-line-start="1497" data-line-end="1498"><code>ilab chat -gm -m instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab-Q4_K_M.gguf</code></p>
<p class="has-line-data" data-line-start="1499" data-line-end="1501">&lt;aside&gt;<br>
 Greedy decoding greedy decoding </p>
<p class="has-line-data" data-line-start="1502" data-line-end="1503">&lt;/aside&gt;</p>
<hr>
<p class="has-line-data" data-line-start="1506" data-line-end="1508"><strong></strong><br>
<code>ilab chat -m models/granite-7b-lab-Q4_K_M.gguf</code></p>
<p class="has-line-data" data-line-start="1509" data-line-end="1510"><strong>Who starred in the movie Oppenheimer?</strong></p>
<p class="has-line-data" data-line-start="1511" data-line-end="1512">The movie Oppenheimer is <strong>an upcoming film</strong> adaptation of the Pulitzer Prize-winning biography American Prometheus: The Life of J. Robert Oppenheimer by Kira Davis. Cillian Murphy has been cast as the title character, J. Robert Oppenheimer. Other notable stars who have joined the cast include Matt Damon as Leslie Groves, the head of the Yale project and the father of Oppenheimers wife, Emily Blunt as Katherine Manning Oppenheimer, the wife of J. Robert Oppenheimer, and Alden Ehrenreich as Donald Oppenheimer, their son.</p>
<p class="has-line-data" data-line-start="1513" data-line-end="1514">The film is directed by Christopher Nolan, known for his work on blockbuster movies like Batman Begins, The Dark Knight, and Interstellar. Oppenheimer is set to explore the life of J. Robert Oppenheimer, the man who led the project that developed the first atomic bomb during World War II. The movie is expected to delve into the moral complexities of nuclear power and the human cost of scientific progress.</p>
<p class="has-line-data" data-line-start="1515" data-line-end="1516">While there isnt much information available about the plot yet, I can provide you with some context on the historical figures and events that Oppenheimer might depict. The film will likely cover Oppenheimers early life, his education at the prestigious Institute for Advanced Studies in Princeton, New Jersey, and his work at the Los Alamos National Laboratory during the Manhattan Project. Additionally, the movie may explore Oppenheimers struggles with the moral implications of nuclear weapons and his eventual condemnation by the U.S. government during the Red Scare.</p>
<p class="has-line-data" data-line-start="1517" data-line-end="1518"><strong>Stay tuned for more updates</strong> on Oppenheimer as filming progresses and <strong>more information becomes available</strong>. If you have any questions about the cast, directors, or historical context of the movie, please let me know!</p>
<p class="has-line-data" data-line-start="1519" data-line-end="1520"><strong>What are the release dates for Oppenheimer movie?</strong></p>
<p class="has-line-data" data-line-start="1521" data-line-end="1522">As of my last update, the release date for Oppenheimer <strong>is not yet set</strong>. The film was announced in 2021, and production has recently started. Christopher Nolan, the director, often takes his time to perfect his films before releasing them. Given the complexity of the subject matter and the intricate plot, its likely that Oppenheimer will require a considerable amount of post-production work.</p>
<p class="has-line-data" data-line-start="1523" data-line-end="1524"><strong>Once filming is complete and the movie is ready for release</strong>, the production team will announce an official date. Typically, this happens several months before the release date. I recommend checking official film news sources and the films official website for the most up-to-date information on the release date.</p>
<p class="has-line-data" data-line-start="1525" data-line-end="1526">In the meantime, you can look forward to other movies released by Christopher Nolan or explore other fascinating historical figures and events that have been brought to the big screen. For example, Dunkirk (2017) and Interstellar (2014) are two of Nolans previous films that showcase his ability to create immersive and thought-provoking cinematic experiences.</p>
<p class="has-line-data" data-line-start="1527" data-line-end="1529"><strong>knowledge</strong><br>
<code>ilab chat -gm -m instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab-Q4_K_M.gguf</code></p>
<p class="has-line-data" data-line-start="1530" data-line-end="1531"><strong>Who starred in the movie Oppenheimer?</strong></p>
<p class="has-line-data" data-line-start="1532" data-line-end="1534">Sean Penn portrayed J. Robert Oppenheimer in the movie Oppenheimer. He was nominated for an Academy Award <br>
 for Best Actor for his performance.</p>
<p class="has-line-data" data-line-start="1535" data-line-end="1536"><strong>What are the release dates for Oppenheimer movie?</strong></p>
<p class="has-line-data" data-line-start="1537" data-line-end="1538">The film Oppenheimer was released on March 17, 2023, in the United States after being shown at the Cannes Film Festival in 2022. It has been available to stream on HBO Max since its release.</p>
<pre><code class="has-line-data" data-line-start="1540" data-line-end="1556" class="language-bash">
 system 
 Welcome to InstructLab Chat w/ INSTRUCTLAB-MERLINITE-<span class="hljs-number">7</span>B-LAB-TRAINED/INSTRUCTLAB-MERLINITE-<span class="hljs-number">7</span>B-LAB-Q4_K_M.GGUF (<span class="hljs-built_in">type</span> /h <span class="hljs-keyword">for</span> <span class="hljs-built_in">help</span>)                                                                      

&gt;&gt;&gt; Who starred <span class="hljs-keyword">in</span> the movie Oppenheimer?                                                                                                                                                   [S][default]
 instructlab-merlinite-<span class="hljs-number">7</span>b-lab-trained/instructlab-merlinite-<span class="hljs-number">7</span>b-lab-Q4_K_M.gguf 
 Oppenheimer was written and directed by James Schamus, and it features Daniel Day-Lewis <span class="hljs-keyword">in</span> the title role. Other notable actors <span class="hljs-keyword">in</span> this film include Emily Blunt as Katherine <span class="hljs-string">"Kitty"</span> Oppenheimer,   
 Robert Wagner as General Leslie Groves, and Michael Nyqvist as Albert Einstein. This movie is a historical drama that depicts the life of J. Robert Oppenheimer during his time at Los Alamos, <span class="hljs-built_in">where</span> 
 he led the development of the atomic bomb during World War II.                                                                                                                                       
 elapsed <span class="hljs-number">2.466</span> seconds 
&gt;&gt;&gt; What are the release dates <span class="hljs-keyword">for</span> Oppenheimer movie?                                                                                                                                       [S][default]
 instructlab-merlinite-<span class="hljs-number">7</span>b-lab-trained/instructlab-merlinite-<span class="hljs-number">7</span>b-lab-Q4_K_M.gguf 
 The Oppenheimer movie was released <span class="hljs-keyword">in</span> <span class="hljs-number">2023</span>. It premiered on March <span class="hljs-number">14</span>th and started to stream on Hulu on April <span class="hljs-number">7</span>th.                                                                                   
 elapsed <span class="hljs-number">0.829</span> seconds 
&gt;&gt;&gt;
</code></pre>
<p class="has-line-data" data-line-start="1557" data-line-end="1558"></p>
<p class="has-line-data" data-line-start="1559" data-line-end="1560"><a href="https://developer.ibm.com/tutorials/awb-train-open-source-llms-collected-knowledge-instructlab/">IBM Developer</a></p>
</body></html>