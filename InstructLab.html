<!DOCTYPE html><html><head><meta charset="utf-8"><title>InstructLab.md</title><style>@charset "UTF-8";@import 'https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0-alpha2/katex.min.css';code{color:#c7254e;background-color:#f9f2f4;border-radius:4px}code,kbd{padding:2px 4px}kbd{color:#fff;background-color:#333;border-radius:3px;box-shadow:inset 0 -1px 0 rgba(0,0,0,.25)}kbd kbd{padding:0;font-size:100%;box-shadow:none}pre{display:block;margin:0 0 10px;word-break:break-all;word-wrap:break-word;color:#333;background-color:#f5f5f5;border:1px solid #ccc;border-radius:4px}pre code{padding:0;font-size:inherit;color:inherit;white-space:pre-wrap;background-color:transparent;border-radius:0}.pre-scrollable{max-height:340px;overflow-y:scroll}table{background-color:transparent}th{text-align:left}.table{width:100%;max-width:100%;margin-bottom:20px}.table>thead>tr>th{padding:8px;line-height:1.4285714;border-top:1px solid #ddd}.table>thead>tr>td,.table>tbody>tr>th,.table>tbody>tr>td,.table>tfoot>tr>th,.table>tfoot>tr>td{padding:8px;line-height:1.4285714;vertical-align:top;border-top:1px solid #ddd}.table>thead>tr>th{vertical-align:bottom;border-bottom:2px solid #ddd}.table>caption+thead>tr:first-child>th,.table>caption+thead>tr:first-child>td,.table>colgroup+thead>tr:first-child>th,.table>colgroup+thead>tr:first-child>td,.table>thead:first-child>tr:first-child>th,.table>thead:first-child>tr:first-child>td{border-top:0}.table>tbody+tbody{border-top:2px solid #ddd}.table .table{background-color:#fff}.table-condensed>thead>tr>th,.table-condensed>thead>tr>td,.table-condensed>tbody>tr>th,.table-condensed>tbody>tr>td,.table-condensed>tfoot>tr>th,.table-condensed>tfoot>tr>td{padding:5px}.table-bordered,.table-bordered>thead>tr>th,.table-bordered>thead>tr>td,.table-bordered>tbody>tr>th,.table-bordered>tbody>tr>td,.table-bordered>tfoot>tr>th,.table-bordered>tfoot>tr>td{border:1px solid #ddd}.table-bordered>thead>tr>th,.table-bordered>thead>tr>td{border-bottom-width:2px}.table-striped>tbody>tr:nth-child(odd)>td,.table-striped>tbody>tr:nth-child(odd)>th{background-color:#f9f9f9}.table-hover>tbody>tr:hover>td,.table-hover>tbody>tr:hover>th{background-color:#f5f5f5}table col[class*="col-"]{position:static;float:none;display:table-column}table td[class*="col-"],table th[class*="col-"]{position:static;float:none;display:table-cell}.table>thead>tr>td.active,.table>thead>tr>th.active,.table>thead>tr.active>td,.table>thead>tr.active>th,.table>tbody>tr>td.active,.table>tbody>tr>th.active,.table>tbody>tr.active>td,.table>tbody>tr.active>th,.table>tfoot>tr>td.active,.table>tfoot>tr>th.active,.table>tfoot>tr.active>td,.table>tfoot>tr.active>th{background-color:#f5f5f5}.table-hover>tbody>tr>td.active:hover,.table-hover>tbody>tr>th.active:hover,.table-hover>tbody>tr.active:hover>td,.table-hover>tbody>tr:hover>.active,.table-hover>tbody>tr.active:hover>th{background-color:#e8e8e8}.table>thead>tr>td.success,.table>thead>tr>th.success,.table>thead>tr.success>td,.table>thead>tr.success>th,.table>tbody>tr>td.success,.table>tbody>tr>th.success,.table>tbody>tr.success>td,.table>tbody>tr.success>th,.table>tfoot>tr>td.success,.table>tfoot>tr>th.success,.table>tfoot>tr.success>td,.table>tfoot>tr.success>th{background-color:#dff0d8}.table-hover>tbody>tr>td.success:hover,.table-hover>tbody>tr>th.success:hover,.table-hover>tbody>tr.success:hover>td,.table-hover>tbody>tr:hover>.success,.table-hover>tbody>tr.success:hover>th{background-color:#d0e9c6}.table>thead>tr>td.info,.table>thead>tr>th.info,.table>thead>tr.info>td,.table>thead>tr.info>th,.table>tbody>tr>td.info,.table>tbody>tr>th.info,.table>tbody>tr.info>td,.table>tbody>tr.info>th,.table>tfoot>tr>td.info,.table>tfoot>tr>th.info,.table>tfoot>tr.info>td,.table>tfoot>tr.info>th{background-color:#d9edf7}.table-hover>tbody>tr>td.info:hover,.table-hover>tbody>tr>th.info:hover,.table-hover>tbody>tr.info:hover>td,.table-hover>tbody>tr:hover>.info,.table-hover>tbody>tr.info:hover>th{background-color:#c4e3f3}.table>thead>tr>td.warning,.table>thead>tr>th.warning,.table>thead>tr.warning>td,.table>thead>tr.warning>th,.table>tbody>tr>td.warning,.table>tbody>tr>th.warning,.table>tbody>tr.warning>td,.table>tbody>tr.warning>th,.table>tfoot>tr>td.warning,.table>tfoot>tr>th.warning,.table>tfoot>tr.warning>td,.table>tfoot>tr.warning>th{background-color:#fcf8e3}.table-hover>tbody>tr>td.warning:hover,.table-hover>tbody>tr>th.warning:hover,.table-hover>tbody>tr.warning:hover>td,.table-hover>tbody>tr:hover>.warning,.table-hover>tbody>tr.warning:hover>th{background-color:#faf2cc}.table>thead>tr>td.danger,.table>thead>tr>th.danger,.table>thead>tr.danger>td,.table>thead>tr.danger>th,.table>tbody>tr>td.danger,.table>tbody>tr>th.danger,.table>tbody>tr.danger>td,.table>tbody>tr.danger>th,.table>tfoot>tr>td.danger,.table>tfoot>tr>th.danger,.table>tfoot>tr.danger>td,.table>tfoot>tr.danger>th{background-color:#f2dede}.table-hover>tbody>tr>td.danger:hover,.table-hover>tbody>tr>th.danger:hover,.table-hover>tbody>tr.danger:hover>td,.table-hover>tbody>tr:hover>.danger,.table-hover>tbody>tr.danger:hover>th{background-color:#ebcccc}fieldset{border:0;min-width:0}legend{display:block;width:100%;margin-bottom:20px;font-size:21px;line-height:inherit;color:#333;border-bottom:1px solid #e5e5e5}label{display:inline-block;max-width:100%;margin-bottom:5px;font-weight:700}input[type="radio"],input[type="checkbox"]{margin:4px 0 0;margin-top:1px \9;line-height:normal}input[type="file"]{display:block}input[type="range"]{display:block;width:100%}select[multiple],select[size]{height:auto}input[type="file"]:focus,input[type="radio"]:focus,input[type="checkbox"]:focus{outline:thin dotted;outline:5px auto -webkit-focus-ring-color;outline-offset:-2px}output{padding-top:7px}output,.form-control{display:block;font-size:14px;line-height:1.4285714;color:#555}.form-control{width:100%;height:34px;padding:6px 12px;background-color:#fff;background-image:none;border:1px solid #ccc;border-radius:4px;box-shadow:inset 0 1px 1px rgba(0,0,0,.075);transition:border-color ease-in-out .15s,box-shadow ease-in-out .15s}.form-control:focus{border-color:#66afe9;outline:0;box-shadow:inset 0 1px 1px rgba(0,0,0,.075),0 0 8px rgba(102,175,233,.6)}.form-control::-moz-placeholder{color:#777;opacity:1}.form-control:-ms-input-placeholder{color:#777}.form-control::-webkit-input-placeholder{color:#777}.form-control[disabled],.form-control[readonly],fieldset[disabled] .form-control{cursor:not-allowed;background-color:#eee;opacity:1}textarea.form-control{height:auto}input[type="date"],input[type="time"],input[type="datetime-local"],input[type="month"]{line-height:34px;line-height:1.4285714 \0}input[type="date"].input-sm,.form-horizontal .form-group-sm input.form-control[type="date"],.input-group-sm>input.form-control[type="date"],.input-group-sm>input.input-group-addon[type="date"],.input-group-sm>.input-group-btn>input.btn[type="date"],input[type="time"].input-sm,.form-horizontal .form-group-sm input.form-control[type="time"],.input-group-sm>input.form-control[type="time"],.input-group-sm>input.input-group-addon[type="time"],.input-group-sm>.input-group-btn>input.btn[type="time"],input[type="datetime-local"].input-sm,.form-horizontal .form-group-sm input.form-control[type="datetime-local"],.input-group-sm>input.form-control[type="datetime-local"],.input-group-sm>input.input-group-addon[type="datetime-local"],.input-group-sm>.input-group-btn>input.btn[type="datetime-local"],input[type="month"].input-sm,.form-horizontal .form-group-sm input.form-control[type="month"],.input-group-sm>input.form-control[type="month"],.input-group-sm>input.input-group-addon[type="month"],.input-group-sm>.input-group-btn>input.btn[type="month"]{line-height:30px}input[type="date"].input-lg,.form-horizontal .form-group-lg input.form-control[type="date"],.input-group-lg>input.form-control[type="date"],.input-group-lg>input.input-group-addon[type="date"],.input-group-lg>.input-group-btn>input.btn[type="date"],input[type="time"].input-lg,.form-horizontal .form-group-lg input.form-control[type="time"],.input-group-lg>input.form-control[type="time"],.input-group-lg>input.input-group-addon[type="time"],.input-group-lg>.input-group-btn>input.btn[type="time"],input[type="datetime-local"].input-lg,.form-horizontal .form-group-lg input.form-control[type="datetime-local"],.input-group-lg>input.form-control[type="datetime-local"],.input-group-lg>input.input-group-addon[type="datetime-local"],.input-group-lg>.input-group-btn>input.btn[type="datetime-local"],input[type="month"].input-lg,.form-horizontal .form-group-lg input.form-control[type="month"],.input-group-lg>input.form-control[type="month"],.input-group-lg>input.input-group-addon[type="month"],.input-group-lg>.input-group-btn>input.btn[type="month"]{line-height:46px}.form-group{margin-bottom:15px}.radio,.checkbox{position:relative;display:block;min-height:20px;margin-top:10px;margin-bottom:10px}.radio label,.checkbox label{padding-left:20px;margin-bottom:0;font-weight:400;cursor:pointer}.radio input[type="radio"],.radio-inline input[type="radio"],.checkbox input[type="checkbox"],.checkbox-inline input[type="checkbox"]{position:absolute;margin-left:-20px;margin-top:4px \9}.radio+.radio,.checkbox+.checkbox{margin-top:-5px}.radio-inline,.checkbox-inline{display:inline-block;padding-left:20px;margin-bottom:0;vertical-align:middle;font-weight:400;cursor:pointer}.radio-inline+.radio-inline,.checkbox-inline+.checkbox-inline{margin-top:0;margin-left:10px}input[type="radio"][disabled],input[type="radio"].disabled,fieldset[disabled] input[type="radio"],input[type="checkbox"][disabled],input[type="checkbox"].disabled,fieldset[disabled] input[type="checkbox"],.radio-inline.disabled,fieldset[disabled] .radio-inline,.checkbox-inline.disabled,fieldset[disabled] .checkbox-inline,.radio.disabled label,fieldset[disabled] .radio label,.checkbox.disabled label,fieldset[disabled] .checkbox label{cursor:not-allowed}.form-control-static{padding-top:7px;padding-bottom:7px;margin-bottom:0}.form-control-static.input-lg,.form-horizontal .form-group-lg .form-control-static.form-control,.input-group-lg>.form-control-static.form-control,.input-group-lg>.form-control-static.input-group-addon,.input-group-lg>.input-group-btn>.form-control-static.btn,.form-control-static.input-sm,.form-horizontal .form-group-sm .form-control-static.form-control,.input-group-sm>.form-control-static.form-control,.input-group-sm>.form-control-static.input-group-addon,.input-group-sm>.input-group-btn>.form-control-static.btn{padding-left:0;padding-right:0}.input-sm,.form-horizontal .form-group-sm .form-control,.input-group-sm>.form-control{height:30px;padding:5px 10px;font-size:12px;line-height:1.5;border-radius:3px}.input-group-sm>.input-group-addon{height:30px;line-height:1.5}.input-group-sm>.input-group-btn>.btn{height:30px;padding:5px 10px;font-size:12px;line-height:1.5;border-radius:3px}select.input-sm,.form-horizontal .form-group-sm select.form-control,.input-group-sm>select.form-control,.input-group-sm>select.input-group-addon,.input-group-sm>.input-group-btn>select.btn{height:30px;line-height:30px}textarea.input-sm,.form-horizontal .form-group-sm textarea.form-control,.input-group-sm>textarea.form-control,.input-group-sm>textarea.input-group-addon,.input-group-sm>.input-group-btn>textarea.btn,select[multiple].input-sm,.form-horizontal .form-group-sm select.form-control[multiple],.input-group-sm>select.form-control[multiple],.input-group-sm>select.input-group-addon[multiple],.input-group-sm>.input-group-btn>select.btn[multiple]{height:auto}.input-lg,.form-horizontal .form-group-lg .form-control,.input-group-lg>.form-control{height:46px;padding:10px 16px;font-size:18px;line-height:1.33;border-radius:6px}.input-group-lg>.input-group-addon{height:46px;line-height:1.33}.input-group-lg>.input-group-btn>.btn{height:46px;padding:10px 16px;font-size:18px;line-height:1.33;border-radius:6px}select.input-lg,.form-horizontal .form-group-lg select.form-control,.input-group-lg>select.form-control,.input-group-lg>select.input-group-addon,.input-group-lg>.input-group-btn>select.btn{height:46px;line-height:46px}textarea.input-lg,.form-horizontal .form-group-lg textarea.form-control,.input-group-lg>textarea.form-control,.input-group-lg>textarea.input-group-addon,.input-group-lg>.input-group-btn>textarea.btn,select[multiple].input-lg,.form-horizontal .form-group-lg select.form-control[multiple],.input-group-lg>select.form-control[multiple],.input-group-lg>select.input-group-addon[multiple],.input-group-lg>.input-group-btn>select.btn[multiple]{height:auto}.has-feedback{position:relative}.has-feedback .form-control{padding-right:42.5px}.form-control-feedback{position:absolute;top:25px;right:0;z-index:2;display:block;width:34px;height:34px;line-height:34px;text-align:center}.input-lg+.form-control-feedback,.form-horizontal .form-group-lg .form-control+.form-control-feedback,.input-group-lg>.form-control+.form-control-feedback,.input-group-lg>.input-group-addon+.form-control-feedback,.input-group-lg>.input-group-btn>.btn+.form-control-feedback{width:46px;height:46px;line-height:46px}.input-sm+.form-control-feedback,.form-horizontal .form-group-sm .form-control+.form-control-feedback,.input-group-sm>.form-control+.form-control-feedback,.input-group-sm>.input-group-addon+.form-control-feedback,.input-group-sm>.input-group-btn>.btn+.form-control-feedback{width:30px;height:30px;line-height:30px}.has-success .help-block,.has-success .control-label,.has-success .radio,.has-success .checkbox,.has-success .radio-inline,.has-success .checkbox-inline{color:#3c763d}.has-success .form-control{border-color:#3c763d;box-shadow:inset 0 1px 1px rgba(0,0,0,.075)}.has-success .form-control:focus{border-color:#2b542c;box-shadow:inset 0 1px 1px rgba(0,0,0,.075),0 0 6px #67b168}.has-success .input-group-addon{color:#3c763d;border-color:#3c763d;background-color:#dff0d8}.has-success .form-control-feedback{color:#3c763d}.has-warning .help-block,.has-warning .control-label,.has-warning .radio,.has-warning .checkbox,.has-warning .radio-inline,.has-warning .checkbox-inline{color:#8a6d3b}.has-warning .form-control{border-color:#8a6d3b;box-shadow:inset 0 1px 1px rgba(0,0,0,.075)}.has-warning .form-control:focus{border-color:#66512c;box-shadow:inset 0 1px 1px rgba(0,0,0,.075),0 0 6px #c0a16b}.has-warning .input-group-addon{color:#8a6d3b;border-color:#8a6d3b;background-color:#fcf8e3}.has-warning .form-control-feedback{color:#8a6d3b}.has-error .help-block,.has-error .control-label,.has-error .radio,.has-error .checkbox,.has-error .radio-inline,.has-error .checkbox-inline{color:#a94442}.has-error .form-control{border-color:#a94442;box-shadow:inset 0 1px 1px rgba(0,0,0,.075)}.has-error .form-control:focus{border-color:#843534;box-shadow:inset 0 1px 1px rgba(0,0,0,.075),0 0 6px #ce8483}.has-error .input-group-addon{color:#a94442;border-color:#a94442;background-color:#f2dede}.has-error .form-control-feedback{color:#a94442}.has-feedback label.sr-only~.form-control-feedback{top:0}.help-block{display:block;margin-top:5px;margin-bottom:10px;color:#737373}.form-horizontal .radio,.form-horizontal .checkbox,.form-horizontal .radio-inline,.form-horizontal .checkbox-inline{margin-top:0;margin-bottom:0;padding-top:7px}.form-horizontal .radio,.form-horizontal .checkbox{min-height:27px}.form-horizontal .form-group{margin-left:-15px;margin-right:-15px}.form-horizontal .form-group:before{content:" ";display:table}.form-horizontal .form-group:after{content:" ";display:table;clear:both}.form-horizontal .has-feedback .form-control-feedback{top:0;right:15px}.btn{display:inline-block;vertical-align:middle;cursor:pointer;background-image:none;border:1px solid transparent;white-space:nowrap;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.btn:focus,.btn:active:focus,.btn.active:focus{outline:thin dotted;outline:5px auto -webkit-focus-ring-color;outline-offset:-2px}.btn:hover,.btn:focus{color:#333;text-decoration:none}.btn:active,.btn.active{outline:0;background-image:none;box-shadow:inset 0 3px 5px rgba(0,0,0,.125)}.btn.disabled,.btn[disabled],fieldset[disabled] .btn{cursor:not-allowed;pointer-events:none;opacity:.65;filter:alpha(opacity=65);box-shadow:none}.btn-default{color:#333;background-color:#fff;border-color:#ccc}.btn-default:hover,.btn-default:focus,.btn-default:active,.btn-default.active,.open>.btn-default.dropdown-toggle{color:#333;background-color:#e6e6e6;border-color:#adadad}.btn-default:active,.btn-default.active,.open>.btn-default.dropdown-toggle{background-image:none}.btn-default.disabled,.btn-default.disabled:hover,.btn-default.disabled:focus,.btn-default.disabled:active,.btn-default.disabled.active,.btn-default[disabled],.btn-default[disabled]:hover,.btn-default[disabled]:focus,.btn-default[disabled]:active,.btn-default[disabled].active,fieldset[disabled] .btn-default,fieldset[disabled] .btn-default:hover,fieldset[disabled] .btn-default:focus,fieldset[disabled] .btn-default:active,fieldset[disabled] .btn-default.active{background-color:#fff;border-color:#ccc}.btn-default .badge{color:#fff;background-color:#333}.btn-primary{color:#fff;background-color:#428bca;border-color:#357ebd}.btn-primary:hover,.btn-primary:focus,.btn-primary:active,.btn-primary.active,.open>.btn-primary.dropdown-toggle{color:#fff;background-color:#3071a9;border-color:#285e8e}.btn-primary:active,.btn-primary.active,.open>.btn-primary.dropdown-toggle{background-image:none}.btn-primary.disabled,.btn-primary.disabled:hover,.btn-primary.disabled:focus,.btn-primary.disabled:active,.btn-primary.disabled.active,.btn-primary[disabled],.btn-primary[disabled]:hover,.btn-primary[disabled]:focus,.btn-primary[disabled]:active,.btn-primary[disabled].active,fieldset[disabled] .btn-primary,fieldset[disabled] .btn-primary:hover,fieldset[disabled] .btn-primary:focus,fieldset[disabled] .btn-primary:active,fieldset[disabled] .btn-primary.active{background-color:#428bca;border-color:#357ebd}.btn-primary .badge{color:#428bca;background-color:#fff}.btn-success{color:#fff;background-color:#5cb85c;border-color:#4cae4c}.btn-success:hover,.btn-success:focus,.btn-success:active,.btn-success.active,.open>.btn-success.dropdown-toggle{color:#fff;background-color:#449d44;border-color:#398439}.btn-success:active,.btn-success.active,.open>.btn-success.dropdown-toggle{background-image:none}.btn-success.disabled,.btn-success.disabled:hover,.btn-success.disabled:focus,.btn-success.disabled:active,.btn-success.disabled.active,.btn-success[disabled],.btn-success[disabled]:hover,.btn-success[disabled]:focus,.btn-success[disabled]:active,.btn-success[disabled].active,fieldset[disabled] .btn-success,fieldset[disabled] .btn-success:hover,fieldset[disabled] .btn-success:focus,fieldset[disabled] .btn-success:active,fieldset[disabled] .btn-success.active{background-color:#5cb85c;border-color:#4cae4c}.btn-success .badge{color:#5cb85c;background-color:#fff}.btn-info{color:#fff;background-color:#5bc0de;border-color:#46b8da}.btn-info:hover,.btn-info:focus,.btn-info:active,.btn-info.active,.open>.btn-info.dropdown-toggle{color:#fff;background-color:#31b0d5;border-color:#269abc}.btn-info:active,.btn-info.active,.open>.btn-info.dropdown-toggle{background-image:none}.btn-info.disabled,.btn-info.disabled:hover,.btn-info.disabled:focus,.btn-info.disabled:active,.btn-info.disabled.active,.btn-info[disabled],.btn-info[disabled]:hover,.btn-info[disabled]:focus,.btn-info[disabled]:active,.btn-info[disabled].active,fieldset[disabled] .btn-info,fieldset[disabled] .btn-info:hover,fieldset[disabled] .btn-info:focus,fieldset[disabled] .btn-info:active,fieldset[disabled] .btn-info.active{background-color:#5bc0de;border-color:#46b8da}.btn-info .badge{color:#5bc0de;background-color:#fff}.btn-warning{color:#fff;background-color:#f0ad4e;border-color:#eea236}.btn-warning:hover,.btn-warning:focus,.btn-warning:active,.btn-warning.active,.open>.btn-warning.dropdown-toggle{color:#fff;background-color:#ec971f;border-color:#d58512}.btn-warning:active,.btn-warning.active,.open>.btn-warning.dropdown-toggle{background-image:none}.btn-warning.disabled,.btn-warning.disabled:hover,.btn-warning.disabled:focus,.btn-warning.disabled:active,.btn-warning.disabled.active,.btn-warning[disabled],.btn-warning[disabled]:hover,.btn-warning[disabled]:focus,.btn-warning[disabled]:active,.btn-warning[disabled].active,fieldset[disabled] .btn-warning,fieldset[disabled] .btn-warning:hover,fieldset[disabled] .btn-warning:focus,fieldset[disabled] .btn-warning:active,fieldset[disabled] .btn-warning.active{background-color:#f0ad4e;border-color:#eea236}.btn-warning .badge{color:#f0ad4e;background-color:#fff}.btn-danger{color:#fff;background-color:#d9534f;border-color:#d43f3a}.btn-danger:hover,.btn-danger:focus,.btn-danger:active,.btn-danger.active,.open>.btn-danger.dropdown-toggle{color:#fff;background-color:#c9302c;border-color:#ac2925}.btn-danger:active,.btn-danger.active,.open>.btn-danger.dropdown-toggle{background-image:none}.btn-danger.disabled,.btn-danger.disabled:hover,.btn-danger.disabled:focus,.btn-danger.disabled:active,.btn-danger.disabled.active,.btn-danger[disabled],.btn-danger[disabled]:hover,.btn-danger[disabled]:focus,.btn-danger[disabled]:active,.btn-danger[disabled].active,fieldset[disabled] .btn-danger,fieldset[disabled] .btn-danger:hover,fieldset[disabled] .btn-danger:focus,fieldset[disabled] .btn-danger:active,fieldset[disabled] .btn-danger.active{background-color:#d9534f;border-color:#d43f3a}.btn-danger .badge{color:#d9534f;background-color:#fff}.btn-link{color:#428bca;font-weight:400;cursor:pointer;border-radius:0}.btn-link,.btn-link:active,.btn-link[disabled],fieldset[disabled] .btn-link{background-color:transparent;box-shadow:none}.btn-link,.btn-link:hover,.btn-link:focus,.btn-link:active{border-color:transparent}.btn-link:hover,.btn-link:focus{color:#2a6496;text-decoration:underline;background-color:transparent}.btn-link[disabled]:hover,.btn-link[disabled]:focus,fieldset[disabled] .btn-link:hover,fieldset[disabled] .btn-link:focus{color:#777;text-decoration:none}.btn-lg{padding:10px 16px;font-size:18px;line-height:1.33;border-radius:6px}.btn-sm{padding:5px 10px}.btn-sm,.btn-xs{font-size:12px;line-height:1.5;border-radius:3px}.btn-xs{padding:1px 5px}.btn-block{display:block;width:100%}.btn-block+.btn-block{margin-top:5px}input[type="submit"].btn-block,input[type="reset"].btn-block,input[type="button"].btn-block{width:100%}.fade{opacity:0;transition:opacity .15s linear}.fade.in{opacity:1}.collapse{display:none}.collapse.in{display:block}tr.collapse.in{display:table-row}tbody.collapse.in{display:table-row-group}.collapsing{position:relative;height:0;overflow:hidden;transition:height .35s ease}.input-group{position:relative;display:table;border-collapse:separate}.input-group[class*="col-"]{float:none;padding-left:0;padding-right:0}.input-group .form-control{position:relative;z-index:2;float:left;width:100%;margin-bottom:0}.input-group-addon,.input-group-btn,.input-group .form-control{display:table-cell}.input-group-addon:not(:first-child):not(:last-child),.input-group-btn:not(:first-child):not(:last-child),.input-group .form-control:not(:first-child):not(:last-child){border-radius:0}.input-group-addon{white-space:nowrap}.input-group-addon,.input-group-btn{width:1%;vertical-align:middle}.input-group-addon{padding:6px 12px;font-size:14px;font-weight:400;line-height:1;color:#555;text-align:center;background-color:#eee;border:1px solid #ccc;border-radius:4px}.input-group-addon.input-sm,.form-horizontal .form-group-sm .input-group-addon.form-control,.input-group-sm>.input-group-addon,.input-group-sm>.input-group-btn>.input-group-addon.btn{padding:5px 10px;font-size:12px;border-radius:3px}.input-group-addon.input-lg,.form-horizontal .form-group-lg .input-group-addon.form-control,.input-group-lg>.input-group-addon,.input-group-lg>.input-group-btn>.input-group-addon.btn{padding:10px 16px;font-size:18px;border-radius:6px}.input-group-addon input[type="radio"],.input-group-addon input[type="checkbox"]{margin-top:0}.input-group .form-control:first-child,.input-group-addon:first-child,.input-group-btn:first-child>.btn,.input-group-btn:first-child>.btn-group>.btn,.input-group-btn:first-child>.dropdown-toggle,.input-group-btn:last-child>.btn:not(:last-child):not(.dropdown-toggle),.input-group-btn:last-child>.btn-group:not(:last-child)>.btn{border-bottom-right-radius:0;border-top-right-radius:0}.input-group-addon:first-child{border-right:0}.input-group .form-control:last-child,.input-group-addon:last-child,.input-group-btn:last-child>.btn,.input-group-btn:last-child>.btn-group>.btn,.input-group-btn:last-child>.dropdown-toggle,.input-group-btn:first-child>.btn:not(:first-child),.input-group-btn:first-child>.btn-group:not(:first-child)>.btn{border-bottom-left-radius:0;border-top-left-radius:0}.input-group-addon:last-child{border-left:0}.input-group-btn{font-size:0;white-space:nowrap}.input-group-btn,.input-group-btn>.btn{position:relative}.input-group-btn>.btn+.btn{margin-left:-1px}.input-group-btn>.btn:hover,.input-group-btn>.btn:focus,.input-group-btn>.btn:active{z-index:2}.input-group-btn:first-child>.btn,.input-group-btn:first-child>.btn-group{margin-right:-1px}.input-group-btn:last-child>.btn,.input-group-btn:last-child>.btn-group{margin-left:-1px}.pagination{display:inline-block;padding-left:0;margin:20px 0;border-radius:4px}.pagination>li{display:inline}.pagination>li>a,.pagination>li>span{position:relative;float:left;padding:6px 12px;line-height:1.4285714;text-decoration:none;color:#428bca;background-color:#fff;border:1px solid #ddd;margin-left:-1px}.pagination>li:first-child>a,.pagination>li:first-child>span{margin-left:0;border-bottom-left-radius:4px;border-top-left-radius:4px}.pagination>li:last-child>a,.pagination>li:last-child>span{border-bottom-right-radius:4px;border-top-right-radius:4px}.pagination>li>a:hover,.pagination>li>a:focus,.pagination>li>span:hover,.pagination>li>span:focus{color:#2a6496;background-color:#eee;border-color:#ddd}.pagination>.active>a,.pagination>.active>a:hover,.pagination>.active>a:focus,.pagination>.active>span,.pagination>.active>span:hover,.pagination>.active>span:focus{z-index:2;color:#fff;background-color:#428bca;border-color:#428bca;cursor:default}.pagination>.disabled>span,.pagination>.disabled>span:hover,.pagination>.disabled>span:focus,.pagination>.disabled>a,.pagination>.disabled>a:hover,.pagination>.disabled>a:focus{color:#777;background-color:#fff;border-color:#ddd;cursor:not-allowed}.pagination-lg>li>a,.pagination-lg>li>span{padding:10px 16px;font-size:18px}.pagination-lg>li:first-child>a,.pagination-lg>li:first-child>span{border-bottom-left-radius:6px;border-top-left-radius:6px}.pagination-lg>li:last-child>a,.pagination-lg>li:last-child>span{border-bottom-right-radius:6px;border-top-right-radius:6px}.pagination-sm>li>a,.pagination-sm>li>span{padding:5px 10px;font-size:12px}.pagination-sm>li:first-child>a,.pagination-sm>li:first-child>span{border-bottom-left-radius:3px;border-top-left-radius:3px}.pagination-sm>li:last-child>a,.pagination-sm>li:last-child>span{border-bottom-right-radius:3px;border-top-right-radius:3px}.close{float:right;font-size:21px;font-weight:700;line-height:1;color:#000;text-shadow:0 1px 0 #fff;opacity:.2;filter:alpha(opacity=20)}.close:hover,.close:focus{color:#000;text-decoration:none;cursor:pointer;opacity:.5;filter:alpha(opacity=50)}button.close{padding:0;cursor:pointer;background:0 0;border:0;-webkit-appearance:none}.modal-open,.modal{overflow:hidden}.modal{display:none;position:fixed;top:0;right:0;bottom:0;left:0;z-index:1050;-webkit-overflow-scrolling:touch;outline:0}.modal.fade .modal-dialog{transform:translate3d(0,-25%,0);transition:transform .3s ease-out}.modal.in .modal-dialog{transform:translate3d(0,0,0)}.modal-open .modal{overflow-x:hidden;overflow-y:auto}.modal-dialog{position:relative;width:auto;margin:10px}.modal-content{position:relative;background-color:#fff;border:1px solid #999;border:1px solid rgba(0,0,0,.2);border-radius:6px;box-shadow:0 3px 9px rgba(0,0,0,.5);background-clip:padding-box;outline:0}.modal-backdrop{position:fixed;top:0;right:0;bottom:0;left:0;z-index:1040;background-color:#000}.modal-backdrop.fade{opacity:0;filter:alpha(opacity=0)}.modal-backdrop.in{opacity:.5;filter:alpha(opacity=50)}.modal-header{padding:15px;border-bottom:1px solid #e5e5e5;min-height:16.4285714px}.modal-header .close{margin-top:-2px}.modal-title{margin:0;line-height:1.4285714}.modal-body{position:relative;padding:15px}.modal-footer{padding:15px;text-align:right;border-top:1px solid #e5e5e5}.modal-footer:before,.modal-footer:after{content:" ";display:table}.modal-footer:after{clear:both}.modal-footer .btn+.btn{margin-left:5px;margin-bottom:0}.modal-footer .btn-group .btn+.btn{margin-left:-1px}.modal-footer .btn-block+.btn-block{margin-left:0}.modal-scrollbar-measure{position:absolute;top:-9999px;width:50px;height:50px;overflow:scroll}.clearfix:before,.clearfix:after{content:" ";display:table}.clearfix:after{clear:both}.center-block{display:block;margin-left:auto;margin-right:auto}.pull-right{float:right!important}.pull-left{float:left!important}.hide{display:none!important}.show{display:block!important}.invisible{visibility:hidden}.text-hide{font:0/0 a;color:transparent;text-shadow:none;background-color:transparent;border:0}.hidden{display:none!important;visibility:hidden!important}.affix{position:fixed;transform:translate3d(0,0,0)}.hljs{display:block;overflow-x:auto;padding:.5em;background:#002b36;color:#839496;-webkit-text-size-adjust:none}.hljs-comment,.hljs-template_comment,.diff .hljs-header,.hljs-doctype,.hljs-pi,.lisp .hljs-string,.hljs-javadoc{color:#586e75}.hljs-keyword,.hljs-winutils,.method,.hljs-addition,.css .hljs-tag,.hljs-request,.hljs-status,.nginx .hljs-title{color:#859900}.hljs-number,.hljs-command,.hljs-string,.hljs-tag .hljs-value,.hljs-rules .hljs-value,.hljs-phpdoc,.hljs-dartdoc,.tex .hljs-formula,.hljs-regexp,.hljs-hexcolor,.hljs-link_url{color:#2aa198}.hljs-title,.hljs-localvars,.hljs-chunk,.hljs-decorator,.hljs-built_in,.hljs-identifier,.vhdl .hljs-literal,.hljs-id,.css .hljs-function{color:#268bd2}.hljs-attribute,.hljs-variable,.lisp .hljs-body,.smalltalk .hljs-number,.hljs-constant,.hljs-class .hljs-title,.hljs-parent,.hljs-type,.hljs-link_reference{color:#b58900}.hljs-preprocessor,.hljs-preprocessor .hljs-keyword,.hljs-pragma,.hljs-shebang,.hljs-symbol,.hljs-symbol .hljs-string,.diff .hljs-change,.hljs-special,.hljs-attr_selector,.hljs-subst,.hljs-cdata,.css .hljs-pseudo,.hljs-header{color:#cb4b16}.hljs-deletion,.hljs-important{color:#dc322f}.hljs-link_label{color:#6c71c4}.tex .hljs-formula{background:#073642}*,*:before,*:after{box-sizing:border-box}html{-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block;vertical-align:baseline}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background:0 0}a:active,a:hover{outline:0}abbr[title]{border-bottom:1px dotted}b,strong{font-weight:700}dfn{font-style:italic}h1{margin:.67em 0}mark{background:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sup{top:-.5em}sub{bottom:-.25em}images{border:0}svg:not(:root){overflow:hidden}figure{margin:1em 40px}hr{box-sizing:content-box;height:0}pre{overflow:auto}code,kbd{font-size:1em}code,kbd,pre,samp{font-family:monospace,monospace}samp{font-size:1em}button,input,optgroup,select,textarea{color:inherit;font:inherit;margin:0}button{overflow:visible}button,select{text-transform:none}button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}button[disabled],html input[disabled]{cursor:default}button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}input{line-height:normal}input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0;margin-right:5px}input[type="number"]::-webkit-inner-spin-button,input[type="number"]::-webkit-outer-spin-button{height:auto}input[type="search"]{-webkit-appearance:textfield;box-sizing:content-box}input[type="search"]::-webkit-search-cancel-button,input[type="search"]::-webkit-search-decoration{-webkit-appearance:none}fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}legend{border:0;padding:0}textarea{overflow:auto}optgroup{font-weight:700}table{border-collapse:collapse;border-spacing:0}.debug{background-color:#ffc0cb!important}.ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.ir{background-color:transparent;border:0;overflow:hidden}.ir::before{content:'';display:block;height:150%;width:0}html{font-size:.875em;background:#fff;color:#373D49}html,body{font-family:Georgia,Cambria,serif;height:100%}body{font-size:1rem;font-weight:400;line-height:2rem}ul,ol{margin-bottom:.83999rem;padding-top:.16001rem}li{font-feature-settings:'kern' 1,'onum' 1,'liga' 1;margin-left:1rem}li>ul,li>ol{margin-bottom:0}p{padding-top:.66001rem;font-feature-settings:'kern' 1,'onum' 1,'liga' 1;margin-top:0}p,pre{margin-bottom:1.33999rem}pre{font-size:1rem;padding:.66001rem 9.5px 9.5px;line-height:2rem;background:linear-gradient(to bottom,#fff 0,#fff .75rem,#f5f7fa .75rem,#f5f7fa 2.75rem,#fff 2.75rem,#fff 4rem);background-size:100% 4rem;border-color:#D3DAEA}blockquote{margin:0}blockquote p{font-size:1rem;margin-bottom:.33999rem;font-style:italic;padding:.66001rem 1rem 1rem;border-left:3px solid #A0AABF}th,td{padding:12px}h1,h2,h3,h4,h5,h6{font-family:"Source Sans Pro","Helvetica Neue",Helvetica,Arial,sans-serif;font-feature-settings:'dlig' 1,'liga' 1,'lnum' 1,'kern' 1;font-style:normal;font-weight:600;margin-top:0}h1{line-height:3rem;font-size:2.0571429rem;margin-bottom:.21999rem;padding-top:.78001rem}h2{font-size:1.953125rem;margin-bottom:.1835837rem;padding-top:.8164163rem}h2,h3{line-height:3rem}h3{font-size:1.6457143rem;margin-bottom:.07599rem;padding-top:.92401rem}h4{font-size:1.5625rem;margin-bottom:.546865rem;padding-top:.453135rem}h5{font-size:1.25rem;margin-bottom:-.56251rem;padding-top:.56251rem}h6{font-size:1rem;margin-bottom:-.65001rem;padding-top:.65001rem}a{cursor:pointer;color:#35D7BB;text-decoration:none}a:hover,a:focus{border-bottom-color:#35D7BB;color:#dff9f4}img{height:auto;max-width:100%}dt{font-style:italic;font-weight:600}.g{display:block}.g:after{clear:both;content:'';display:table}.g-b{float:left;margin:0;width:100%}.g{margin-left:-16px;margin-right:-16px}.g-b{padding-left:16px;padding-right:16px}.g-b--center{display:block;float:none;margin:0 auto}.g-b--right{float:right}.g-b--1of1{width:100%}.g-b--1of2,.g-b--2of4,.g-b--3of6,.g-b--4of8,.g-b--5of10,.g-b--6of12{width:50%}.g-b--1of3,.g-b--2of6,.g-b--4of12{width:33.333%}.g-b--2of3,.g-b--4of6,.g-b--8of12{width:66.666%}.g-b--1of4,.g-b--2of8,.g-b--3of12{width:25%}.g-b--3of4,.g-b--6of8,.g-b--9of12{width:75%}.g-b--1of5,.g-b--2of10{width:20%}.g-b--2of5,.g-b--4of10{width:40%}.g-b--3of5,.g-b--6of10{width:60%}.g-b--4of5,.g-b--8of10{width:80%}.g-b--1of6,.g-b--2of12{width:16.666%}.g-b--5of6,.g-b--10of12{width:83.333%}.g-b--1of8{width:12.5%}.g-b--3of8{width:37.5%}.g-b--5of8{width:62.5%}.g-b--7of8{width:87.5%}.g-b--1of10{width:10%}.g-b--3of10{width:30%}.g-b--7of10{width:70%}.g-b--9of10{width:90%}.g-b--1of12{width:8.333%}.g-b--5of12{width:41.666%}.g-b--7of12{width:58.333%}.g-b--11of12{width:91.666%}.g-b--push--1of1{margin-left:100%}.g-b--push--1of2,.g-b--push--2of4,.g-b--push--3of6,.g-b--push--4of8,.g-b--push--5of10,.g-b--push--6of12{margin-left:50%}.g-b--push--1of3,.g-b--push--2of6,.g-b--push--4of12{margin-left:33.333%}.g-b--push--2of3,.g-b--push--4of6,.g-b--push--8of12{margin-left:66.666%}.g-b--push--1of4,.g-b--push--2of8,.g-b--push--3of12{margin-left:25%}.g-b--push--3of4,.g-b--push--6of8,.g-b--push--9of12{margin-left:75%}.g-b--push--1of5,.g-b--push--2of10{margin-left:20%}.g-b--push--2of5,.g-b--push--4of10{margin-left:40%}.g-b--push--3of5,.g-b--push--6of10{margin-left:60%}.g-b--push--4of5,.g-b--push--8of10{margin-left:80%}.g-b--push--1of6,.g-b--push--2of12{margin-left:16.666%}.g-b--push--5of6,.g-b--push--10of12{margin-left:83.333%}.g-b--push--1of8{margin-left:12.5%}.g-b--push--3of8{margin-left:37.5%}.g-b--push--5of8{margin-left:62.5%}.g-b--push--7of8{margin-left:87.5%}.g-b--push--1of10{margin-left:10%}.g-b--push--3of10{margin-left:30%}.g-b--push--7of10{margin-left:70%}.g-b--push--9of10{margin-left:90%}.g-b--push--1of12{margin-left:8.333%}.g-b--push--5of12{margin-left:41.666%}.g-b--push--7of12{margin-left:58.333%}.g-b--push--11of12{margin-left:91.666%}.g-b--pull--1of1{margin-right:100%}.g-b--pull--1of2,.g-b--pull--2of4,.g-b--pull--3of6,.g-b--pull--4of8,.g-b--pull--5of10,.g-b--pull--6of12{margin-right:50%}.g-b--pull--1of3,.g-b--pull--2of6,.g-b--pull--4of12{margin-right:33.333%}.g-b--pull--2of3,.g-b--pull--4of6,.g-b--pull--8of12{margin-right:66.666%}.g-b--pull--1of4,.g-b--pull--2of8,.g-b--pull--3of12{margin-right:25%}.g-b--pull--3of4,.g-b--pull--6of8,.g-b--pull--9of12{margin-right:75%}.g-b--pull--1of5,.g-b--pull--2of10{margin-right:20%}.g-b--pull--2of5,.g-b--pull--4of10{margin-right:40%}.g-b--pull--3of5,.g-b--pull--6of10{margin-right:60%}.g-b--pull--4of5,.g-b--pull--8of10{margin-right:80%}.g-b--pull--1of6,.g-b--pull--2of12{margin-right:16.666%}.g-b--pull--5of6,.g-b--pull--10of12{margin-right:83.333%}.g-b--pull--1of8{margin-right:12.5%}.g-b--pull--3of8{margin-right:37.5%}.g-b--pull--5of8{margin-right:62.5%}.g-b--pull--7of8{margin-right:87.5%}.g-b--pull--1of10{margin-right:10%}.g-b--pull--3of10{margin-right:30%}.g-b--pull--7of10{margin-right:70%}.g-b--pull--9of10{margin-right:90%}.g-b--pull--1of12{margin-right:8.333%}.g-b--pull--5of12{margin-right:41.666%}.g-b--pull--7of12{margin-right:58.333%}.g-b--pull--11of12{margin-right:91.666%}.splashscreen{position:fixed;top:0;left:0;width:100%;height:100%;background-color:#373D49;z-index:22}.splashscreen-dillinger{width:260px;height:auto;display:block;margin:0 auto;padding-bottom:3rem}.splashscreen p{font-size:1.25rem;padding-top:.56251rem;font-family:"Source Sans Pro","Helvetica Neue",Helvetica,Arial,sans-serif;font-weight:400;text-align:center;max-width:500px;margin:0 auto;color:#FFF}.sp-center{position:relative;transform:translateY(-50%);top:50%}.open-menu>.wrapper{overflow-x:hidden}.page{margin:0 auto;position:relative;top:0;left:0;width:100%;height:100%;z-index:2;transition:all .25s ease-in-out;background-color:#fff;padding-top:51px;will-change:left}.open-menu .page{left:270px}.title{line-height:1rem;font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem;font-weight:500;color:#A0AABF;letter-spacing:1px;text-transform:uppercase;padding-left:16px;padding-right:16px;margin-top:1rem}.split-preview .title{padding-left:0}.title-document{line-height:1rem;font-size:1.25rem;margin-bottom:.89999rem;padding-top:.10001rem;font-weight:400;font-family:"Ubuntu Mono",Monaco;color:#373D49;padding-left:16px;padding-right:16px;width:80%;min-width:300px;outline:0;border:none}.icon{display:block;margin:0 auto;width:36px;height:36px;border-radius:3px;text-align:center}.icon svg{display:inline-block;margin-left:auto;margin-right:auto}.icon-preview{background-color:#373D49;line-height:40px}.icon-preview svg{width:19px;height:12px}.icon-settings{background-color:#373D49;line-height:44px}.icon-settings svg{width:18px;height:18px}.icon-link{width:16px;height:16px;line-height:1;margin-right:24px;text-align:right}.navbar{background-color:#373D49;height:51px;width:100%;position:fixed;top:0;left:0;z-index:6;transition:all .25s ease-in-out;will-change:left}.navbar:after{content:"";display:table;clear:both}.open-menu .navbar{left:270px}.navbar-brand{float:left;margin:0 0 0 24px;padding:0;line-height:42px}.navbar-brand svg{width:85px;height:11px}.nav-left{float:left}.nav-right{float:right}.nav-sidebar{width:100%}.menu{list-style:none;margin:0;padding:0}.menu a{border:0;color:#A0AABF;font-family:"Source Sans Pro","Helvetica Neue",Helvetica,Arial,sans-serif;outline:none;text-transform:uppercase}.menu a:hover{color:#35D7BB}.menu .menu-item{border:0;display:none;float:left;margin:0;position:relative}.menu .menu-item>a{display:block;font-size:12px;height:51px;letter-spacing:1px;line-height:51px;padding:0 24px}.menu .menu-item--settings,.menu .menu-item--preview,.menu .menu-item--save-to.in-sidebar,.menu .menu-item--import-from.in-sidebar,.menu .menu-item--link-unlink.in-sidebar,.menu .menu-item--documents.in-sidebar{display:block}.menu .menu-item--documents{padding-bottom:1rem}.menu .menu-item.open>a{background-color:#1D212A}.menu .menu-item-icon>a{height:auto;padding:0}.menu .menu-item-icon:hover>a{background-color:transparent}.menu .menu-link.open i{background-color:#1D212A}.menu .menu-link.open g{fill:#35D7BB}.menu .menu-link-preview,.menu .menu-link-settings{margin-top:8px;width:51px}.menu-sidebar{width:100%}.menu-sidebar .menu-item{float:none;margin-bottom:1px;width:100%}.menu-sidebar .menu-item.open>a{background-color:#373D49}.menu-sidebar .open .caret{transform:rotate(180deg)}.menu-sidebar>.menu-item:hover .dropdown a,.menu-sidebar>.menu-item:hover .settings a{background-color:transparent}.menu-sidebar .menu-link{background-color:#373D49;font-weight:600}.menu-sidebar .menu-link:after{content:"";display:table;clear:both}.menu-sidebar .menu-link>span{float:left}.menu-sidebar .menu-link>.caret{float:right;text-align:right;top:22px}.menu-sidebar .dropdown,.menu-sidebar .settings{background-color:transparent;position:static;width:100%}.dropdown{position:absolute;right:0;top:51px;width:188px}.dropdown,.settings{display:none;background-color:#1D212A}.dropdown{padding:0}.dropdown,.settings,.sidebar-list{list-style:none;margin:0}.sidebar-list{padding:0}.dropdown li{margin:32px 0;padding:0 0 0 32px}.dropdown li,.settings li{line-height:1}.sidebar-list li{line-height:1;margin:32px 0;padding:0 0 0 32px}.dropdown a{color:#D0D6E2}.dropdown a,.settings a,.sidebar-list a{display:block;text-transform:none}.sidebar-list a{color:#D0D6E2}.dropdown a:after,.settings a:after,.sidebar-list a:after{content:"";display:table;clear:both}.dropdown .icon,.settings .icon,.sidebar-list .icon{float:right}.open .dropdown,.open .settings,.open .sidebar-list{display:block}.open .dropdown.collapse,.open .collapse.settings,.open .sidebar-list.collapse{display:none}.open .dropdown.collapse.in,.open .collapse.in.settings,.open .sidebar-list.collapse.in{display:block}.dropdown .unlinked .icon,.settings .unlinked .icon,.sidebar-list .unlinked .icon{opacity:.3}.dropdown.documents li,.documents.settings li,.sidebar-list.documents li{background-image:url("../img/icons/file.svg");background-position:240px center;background-repeat:no-repeat;background-size:14px 16px;padding:3px 32px}.dropdown.documents li.octocat,.documents.settings li.octocat,.sidebar-list.documents li.octocat{background-image:url("../img/icons/octocat.svg");background-position:234px center;background-size:24px 24px}.dropdown.documents li:last-child,.documents.settings li:last-child,.sidebar-list.documents li:last-child{margin-bottom:1rem}.dropdown.documents li.active a,.documents.settings li.active a,.sidebar-list.documents li.active a{color:#35D7BB}.settings{position:fixed;top:67px;right:16px;border-radius:3px;width:288px;background-color:#373D49;padding:16px;z-index:7}.show-settings .settings{display:block}.settings .has-checkbox{float:left}.settings form{display:-ms-flexbox;display:flex;-ms-flex-direction:row;flex-direction:row;-ms-flex-pack:justify;justify-content:space-between}.settings input{width:20%}.settings a{font-size:1.25rem;font-family:"Source Sans Pro","Helvetica Neue",Helvetica,Arial,sans-serif;font-weight:400;-webkit-font-smoothing:antialiased;line-height:28px;color:#D0D6E2}.settings a:after{content:"";display:table;clear:both}.settings a:hover{color:#35D7BB}.settings li{border-bottom:1px solid #4F535B;margin:0;padding:16px 0}.settings li:last-child{border-bottom:none}.brand{border:none;display:block}.brand:hover g{fill:#35D7BB}.toggle{display:block;float:left;height:16px;padding:25px 16px 26px;width:40px}.toggle span:after,.toggle span:before{content:'';left:0;position:absolute;top:-6px}.toggle span:after{top:6px}.toggle span{display:block;position:relative}.toggle span,.toggle span:after,.toggle span:before{-webkit-backface-visibility:hidden;backface-visibility:hidden;background-color:#D3DAEA;height:2px;transition:all .3s;width:20px}.open-menu .toggle span{background-color:transparent}.open-menu .toggle span:before{transform:rotate(45deg) translate(3px,3px)}.open-menu .toggle span:after{transform:rotate(-45deg) translate(5px,-6px)}.caret{display:inline-block;width:0;height:0;margin-left:6px;vertical-align:middle;position:relative;top:-1px;border-top:4px solid;border-right:4px solid transparent;border-left:4px solid transparent}.sidebar{overflow:auto;height:100%;padding-right:15px;padding-bottom:15px;width:285px}.sidebar-wrapper{-webkit-overflow-scrolling:touch;background-color:#2B2F36;left:0;height:100%;overflow-y:hidden;position:fixed;top:0;width:285px;z-index:1}.sidebar-branding{width:160px;padding:0;margin:16px auto}.header{border-bottom:1px solid #E8E8E8;position:relative}.words,.characters{line-height:1rem;font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem;font-weight:500;font-family:"Source Sans Pro","Helvetica Neue",Helvetica,Arial,sans-serif;letter-spacing:1px;text-transform:uppercase;z-index:5;position:absolute;right:16px;top:0}.words span,.characters span{color:#A0AABF}.words .counter,.characters .counter{color:#000}.words+.characters{top:22px}.mr10{margin-right:10px}.btn{text-align:center;display:inline-block;width:100%;text-transform:uppercase;font-weight:600;font-family:"Source Sans Pro","Helvetica Neue",Helvetica,Arial,sans-serif;font-size:14px;text-shadow:0 1px 0 #1b8b77;padding:16px 24px;background-color:#35D7BB;border-radius:3px;margin:0 auto 16px;line-height:1;color:#fff;transition:all .15s linear;-webkit-font-smoothing:antialiased}.btn--new,.btn--save{display:block;width:238px}.btn--new:hover,.btn--new:focus,.btn--save:hover,.btn--save:focus{color:#fff;border-bottom-color:transparent;box-shadow:0 1px 3px #24b59c;text-shadow:0 1px 0 #24b59c}.btn--save{background-color:#4A5261;text-shadow:0 1px 1px #1e2127}.btn--save:hover,.btn--save:focus{color:#fff;border-bottom-color:transparent;box-shadow:0 1px 5px #08090a;text-shadow:none}.btn--delete{display:block;width:238px;background-color:transparent;font-size:12px;text-shadow:none}.btn--delete:hover,.btn--delete:focus{color:#fff;border-bottom-color:transparent;text-shadow:0 1px 0 #08090a;opacity:.8}.btn--delete-modal,.btn--ok,.btn--close{border-top:0;background-color:#4A5261;text-shadow:0 1px 0 #08090a;margin:0}.btn--delete-modal:hover,.btn--delete-modal:focus,.btn--ok:hover,.btn--ok:focus,.btn--close:hover,.btn--close:focus{color:#fff;background-color:#292d36;text-shadow:none}.btn--delete-modal{display:inline;width:auto}.overlay{position:absolute;top:0;left:0;width:100%;height:100%;background-color:rgba(55,61,73,.8);transition:all .25s ease-in-out;transition-timing-function:ease-out;will-change:left,opacity,visibility;z-index:5;opacity:0;visibility:hidden}.show-settings .overlay{visibility:visible;opacity:1}.switch{float:right;line-height:1}.switch input{display:none}.switch small{display:inline-block;cursor:pointer;padding:0 24px 0 0;transition:all ease .2s;background-color:#2B2F36;border-color:#2B2F36}.switch small,.switch small:before{border-radius:30px;box-shadow:inset 0 0 2px 0 #14171F}.switch small:before{display:block;content:'';width:28px;height:28px;background:#fff}.switch.checked small{padding-right:0;padding-left:24px;background-color:#35D7BB;box-shadow:none}.modal--dillinger.about .modal-dialog{font-size:1.25rem;max-width:500px}.modal--dillinger.scope .modal-dialog{max-width:300px;margin:5rem auto}.modal--dillinger .modal-dialog{max-width:600px;width:auto;margin:5rem auto}.modal--dillinger .modal-content{background:#373D49;border-radius:3px;box-shadow:0 2px 5px 0 #2C3B59;color:#fff;font-family:"Source Sans Pro","Helvetica Neue",Helvetica,Arial,sans-serif;font-weight:400;padding:2rem}.modal--dillinger ul{list-style-type:disc;margin:1rem 0;padding:0 0 0 1rem}.modal--dillinger li{padding:0;margin:0}.modal--dillinger .modal-header{border:0;padding:0}.modal--dillinger .modal-body{padding:0}.modal--dillinger .modal-footer{border:0;padding:0}.modal--dillinger .close{color:#fff;opacity:1}.modal-backdrop{background-color:#373D49}.pagination--dillinger{padding:0!important;margin:1.5rem 0!important;display:-ms-flexbox;display:flex;-ms-flex-pack:justify;justify-content:space-between;-ms-flex-direction:row;flex-direction:row;-ms-flex-align:center;align-items:center;-ms-flex-line-pack:stretch;align-content:stretch}.pagination--dillinger li{display:-ms-flexbox;display:flex;-ms-flex-positive:1;flex-grow:1;text-align:center}.pagination--dillinger li:first-child>a,.pagination--dillinger li.disabled>a,.pagination--dillinger li.disabled>a:hover,.pagination--dillinger li.disabled>a:focus,.pagination--dillinger li>a{background-color:transparent;border-color:#4F535B;border-right-color:transparent}.pagination--dillinger li.active>a,.pagination--dillinger li.active>a:hover,.pagination--dillinger li.active>a:focus{border-color:#4A5261;background-color:#4A5261;color:#fff}.pagination--dillinger li>a{float:none;color:#fff;width:100%;display:block;text-align:center;margin:0;border-right-color:transparent;padding:6px}.pagination--dillinger li>a:hover,.pagination--dillinger li>a:focus{border-color:#35D7BB;background-color:#35D7BB;color:#fff}.pagination--dillinger li:last-child a{border-color:#4F535B}.pagination--dillinger li:first-child a{border-right-color:transparent}.diNotify{position:absolute;z-index:9999;left:0;right:0;top:0;margin:0 auto;max-width:400px;text-align:center;transition:top .5s ease-in-out,opacity .5s ease-in-out;visibility:hidden}.diNotify-body{-webkit-font-smoothing:antialiased;background-color:#35D7BB;background:#666E7F;border-radius:3px;color:#fff;font-family:"Source Sans Pro","Helvetica Neue",Helvetica,Arial,sans-serif;font-weight:400;overflow:hidden;padding:1rem 2rem .5rem;display:-ms-flexbox;display:flex;-ms-flex-align:baseline;align-items:baseline;-ms-flex-pack:center;justify-content:center}.diNotify-icon{display:block;width:16px;height:16px;line-height:16px;position:relative;top:3px}.diNotify-message{padding-left:1rem}.zen-wrapper{position:fixed;top:0;left:0;right:0;bottom:0;width:100%;height:100%;z-index:10;background-color:#FFF;opacity:0;transition:opacity .25s ease-in-out}.zen-wrapper.on{opacity:1}.enter-zen-mode{background-image:url("../img/icons/enter-zen.svg");right:.5rem;top:.313rem;display:none}.enter-zen-mode,.close-zen-mode{font:0/0 a;color:transparent;text-shadow:none;background-color:transparent;border:0;background-repeat:no-repeat;width:32px;height:32px;display:block;position:absolute}.close-zen-mode{background-image:url("../img/icons/exit-zen.svg");right:1rem;top:1rem}.zen-page{position:relative;top:0;bottom:0;z-index:11;height:100%;width:100%}#zen{font-size:1.25rem;width:300px;height:80%;margin:0 auto;position:relative;top:10%}#zen:before,#zen:after{content:"";position:absolute;height:10%;width:100%;z-index:12;pointer-events:none}#preview .table{width:auto}.ui-resizable{position:relative}.ui-resizable-handle{position:absolute;font-size:.1px;z-index:99999;display:block}.ui-resizable-e{background-color:#666;border-right:8px solid #e8e8e8;border-left:1px solid #222;width:10px;z-index:88!important;position:relative}.ui-resizable-e:after{content:"-";display:block;position:absolute;top:calc(50% - 16px);left:0;height:25px;width:2px;background-color:rgba(0,0,0,.4);margin:3px}#editor{cursor:ew-resize;position:relative;z-index:auto}.profile-pic{float:left;width:250px}#_default_ a::before{color:#A0AABF}#_default_ img{display:none}#_default_ #_default_{display:block;float:left;max-width:38%;word-wrap:break-word}#_default_ .default-ad{display:none}#_default_ ._default_{display:block}#_default_ a{color:#35d7bb;text-decoration:none}#_default_ a:hover{color:#8ae8d8}#_default_ .default-image{display:none}#_default_ .default-title:after{content:" — "}#_default_ .default-title,#_default_ .default-text,#_default_ .default-description{display:inline}#_default_ .default-title{position:relative;font-weight:600;display:none}#_default_ a:before{position:relative;top:0;padding:5px;color:#a0aabf;content:"Ad";text-transform:uppercase;font-size:8px;font-family:Verdana,sans-serif}#_default_{display:block;float:left;max-width:38%;word-wrap:break-word}#_default_ ._default_{display:block;font-size:.75rem;height:51px;letter-spacing:1px;line-height:1rem;padding:18px 24px}body{max-width:1024px;margin:0 auto;overflow:auto;padding:2%}.split{overflow:scroll;padding:0!important;-webkit-overflow-scrolling:touch}.split-editor{padding-left:0;padding-right:0;position:relative;z-index:3}.show-preview .split-editor{display:none}.split-preview{background-color:#fff;display:none;top:0;position:relative;z-index:4}.show-preview .split-preview{display:block}#editor{font-size:1rem;font-family:"Ubuntu Mono",Monaco;font-weight:400;line-height:2rem;width:100%;height:100%}#editor .ace_gutter{-webkit-font-smoothing:antialiased}.editor-header{width:50%;float:left;border-bottom:1px solid #E8E8E8;position:relative}.editor-header--first{border-right:1px solid #E8E8E8}.editor-header .title{display:inline-block}.preview-html{padding:15px}.preview-html a{color:#A0AABF;text-decoration:underline}.preview-src{white-space:normal}.preview-mode-toggle-src{background-image:url("../img/icons/code.svg")}.preview-mode-toggle-src,.preview-mode-toggle-html{font:0/0 a;color:transparent;text-shadow:none;background-color:transparent;border:0;background-repeat:no-repeat;width:32px;height:32px;display:block;position:absolute;right:.5rem;top:.5rem;display:none}.preview-mode-toggle-html{background-image:url("../img/icons/eye.svg")}.sr-only{visibility:hidden;text-overflow:110%;overflow:hidden;top:-100px;position:absolute}.mnone{margin:0!important}@media screen and (min-width:27.5em){html{font-size:.875em}body{font-size:1rem}ul,ol{margin-bottom:.83999rem;padding-top:.16001rem}p{padding-top:.66001rem}p,pre{margin-bottom:1.33999rem}pre,blockquote p{font-size:1rem;padding-top:.66001rem}blockquote p{margin-bottom:.33999rem}h1{font-size:2.0571429rem;margin-bottom:.21999rem;padding-top:.78001rem}h2{font-size:1.953125rem;margin-bottom:.1835837rem;padding-top:.8164163rem}h3{font-size:1.6457143rem;margin-bottom:.07599rem;padding-top:.92401rem}h4{font-size:1.5625rem;margin-bottom:.546865rem;padding-top:.453135rem}h5{font-size:1.25rem;margin-bottom:-.56251rem;padding-top:.56251rem}h6{font-size:1rem;margin-bottom:-.65001rem;padding-top:.65001rem}.g{margin-left:-16px;margin-right:-16px}.g-b{padding-left:16px;padding-right:16px}.g-b--m1of1{width:100%}.g-b--m1of2,.g-b--m2of4,.g-b--m3of6,.g-b--m4of8,.g-b--m5of10,.g-b--m6of12{width:50%}.g-b--m1of3,.g-b--m2of6,.g-b--m4of12{width:33.333%}.g-b--m2of3,.g-b--m4of6,.g-b--m8of12{width:66.666%}.g-b--m1of4,.g-b--m2of8,.g-b--m3of12{width:25%}.g-b--m3of4,.g-b--m6of8,.g-b--m9of12{width:75%}.g-b--m1of5,.g-b--m2of10{width:20%}.g-b--m2of5,.g-b--m4of10{width:40%}.g-b--m3of5,.g-b--m6of10{width:60%}.g-b--m4of5,.g-b--m8of10{width:80%}.g-b--m1of6,.g-b--m2of12{width:16.666%}.g-b--m5of6,.g-b--m10of12{width:83.333%}.g-b--m1of8{width:12.5%}.g-b--m3of8{width:37.5%}.g-b--m5of8{width:62.5%}.g-b--m7of8{width:87.5%}.g-b--m1of10{width:10%}.g-b--m3of10{width:30%}.g-b--m7of10{width:70%}.g-b--m9of10{width:90%}.g-b--m1of12{width:8.333%}.g-b--m5of12{width:41.666%}.g-b--m7of12{width:58.333%}.g-b--m11of12{width:91.666%}.g-b--push--m1of1{margin-left:100%}.g-b--push--m1of2,.g-b--push--m2of4,.g-b--push--m3of6,.g-b--push--m4of8,.g-b--push--m5of10,.g-b--push--m6of12{margin-left:50%}.g-b--push--m1of3,.g-b--push--m2of6,.g-b--push--m4of12{margin-left:33.333%}.g-b--push--m2of3,.g-b--push--m4of6,.g-b--push--m8of12{margin-left:66.666%}.g-b--push--m1of4,.g-b--push--m2of8,.g-b--push--m3of12{margin-left:25%}.g-b--push--m3of4,.g-b--push--m6of8,.g-b--push--m9of12{margin-left:75%}.g-b--push--m1of5,.g-b--push--m2of10{margin-left:20%}.g-b--push--m2of5,.g-b--push--m4of10{margin-left:40%}.g-b--push--m3of5,.g-b--push--m6of10{margin-left:60%}.g-b--push--m4of5,.g-b--push--m8of10{margin-left:80%}.g-b--push--m1of6,.g-b--push--m2of12{margin-left:16.666%}.g-b--push--m5of6,.g-b--push--m10of12{margin-left:83.333%}.g-b--push--m1of8{margin-left:12.5%}.g-b--push--m3of8{margin-left:37.5%}.g-b--push--m5of8{margin-left:62.5%}.g-b--push--m7of8{margin-left:87.5%}.g-b--push--m1of10{margin-left:10%}.g-b--push--m3of10{margin-left:30%}.g-b--push--m7of10{margin-left:70%}.g-b--push--m9of10{margin-left:90%}.g-b--push--m1of12{margin-left:8.333%}.g-b--push--m5of12{margin-left:41.666%}.g-b--push--m7of12{margin-left:58.333%}.g-b--push--m11of12{margin-left:91.666%}.g-b--pull--m1of1{margin-right:100%}.g-b--pull--m1of2,.g-b--pull--m2of4,.g-b--pull--m3of6,.g-b--pull--m4of8,.g-b--pull--m5of10,.g-b--pull--m6of12{margin-right:50%}.g-b--pull--m1of3,.g-b--pull--m2of6,.g-b--pull--m4of12{margin-right:33.333%}.g-b--pull--m2of3,.g-b--pull--m4of6,.g-b--pull--m8of12{margin-right:66.666%}.g-b--pull--m1of4,.g-b--pull--m2of8,.g-b--pull--m3of12{margin-right:25%}.g-b--pull--m3of4,.g-b--pull--m6of8,.g-b--pull--m9of12{margin-right:75%}.g-b--pull--m1of5,.g-b--pull--m2of10{margin-right:20%}.g-b--pull--m2of5,.g-b--pull--m4of10{margin-right:40%}.g-b--pull--m3of5,.g-b--pull--m6of10{margin-right:60%}.g-b--pull--m4of5,.g-b--pull--m8of10{margin-right:80%}.g-b--pull--m1of6,.g-b--pull--m2of12{margin-right:16.666%}.g-b--pull--m5of6,.g-b--pull--m10of12{margin-right:83.333%}.g-b--pull--m1of8{margin-right:12.5%}.g-b--pull--m3of8{margin-right:37.5%}.g-b--pull--m5of8{margin-right:62.5%}.g-b--pull--m7of8{margin-right:87.5%}.g-b--pull--m1of10{margin-right:10%}.g-b--pull--m3of10{margin-right:30%}.g-b--pull--m7of10{margin-right:70%}.g-b--pull--m9of10{margin-right:90%}.g-b--pull--m1of12{margin-right:8.333%}.g-b--pull--m5of12{margin-right:41.666%}.g-b--pull--m7of12{margin-right:58.333%}.g-b--pull--m11of12{margin-right:91.666%}.splashscreen p{font-size:1.25rem;margin-bottom:1.43749rem;padding-top:.56251rem}.title{font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem}.title-document{margin-bottom:.89999rem;padding-top:.10001rem}.title-document,.settings a{font-size:1.25rem}.words,.characters{font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem}.modal--dillinger.about .modal-dialog,#zen{font-size:1.25rem}#zen{width:400px}#editor{font-size:1rem}}@media screen and (min-width:46.25em){html{font-size:.875em}body{font-size:1rem}ul,ol{margin-bottom:.83999rem;padding-top:.16001rem}p{padding-top:.66001rem}p,pre{margin-bottom:1.33999rem}pre,blockquote p{font-size:1rem;padding-top:.66001rem}blockquote p{margin-bottom:.33999rem}h1{font-size:2.0571429rem;margin-bottom:.21999rem;padding-top:.78001rem}h2{font-size:1.953125rem;margin-bottom:.1835837rem;padding-top:.8164163rem}h3{font-size:1.6457143rem;margin-bottom:.07599rem;padding-top:.92401rem}h4{font-size:1.5625rem;margin-bottom:.546865rem;padding-top:.453135rem}h5{font-size:1.25rem;margin-bottom:-.56251rem;padding-top:.56251rem}h6{font-size:1rem;margin-bottom:-.65001rem;padding-top:.65001rem}.g{margin-left:-16px;margin-right:-16px}.g-b{padding-left:16px;padding-right:16px}.g-b--t1of1{width:100%}.g-b--t1of2,.g-b--t2of4,.g-b--t3of6,.g-b--t4of8,.g-b--t5of10,.g-b--t6of12{width:50%}.g-b--t1of3,.g-b--t2of6,.g-b--t4of12{width:33.333%}.g-b--t2of3,.g-b--t4of6,.g-b--t8of12{width:66.666%}.g-b--t1of4,.g-b--t2of8,.g-b--t3of12{width:25%}.g-b--t3of4,.g-b--t6of8,.g-b--t9of12{width:75%}.g-b--t1of5,.g-b--t2of10{width:20%}.g-b--t2of5,.g-b--t4of10{width:40%}.g-b--t3of5,.g-b--t6of10{width:60%}.g-b--t4of5,.g-b--t8of10{width:80%}.g-b--t1of6,.g-b--t2of12{width:16.666%}.g-b--t5of6,.g-b--t10of12{width:83.333%}.g-b--t1of8{width:12.5%}.g-b--t3of8{width:37.5%}.g-b--t5of8{width:62.5%}.g-b--t7of8{width:87.5%}.g-b--t1of10{width:10%}.g-b--t3of10{width:30%}.g-b--t7of10{width:70%}.g-b--t9of10{width:90%}.g-b--t1of12{width:8.333%}.g-b--t5of12{width:41.666%}.g-b--t7of12{width:58.333%}.g-b--t11of12{width:91.666%}.g-b--push--t1of1{margin-left:100%}.g-b--push--t1of2,.g-b--push--t2of4,.g-b--push--t3of6,.g-b--push--t4of8,.g-b--push--t5of10,.g-b--push--t6of12{margin-left:50%}.g-b--push--t1of3,.g-b--push--t2of6,.g-b--push--t4of12{margin-left:33.333%}.g-b--push--t2of3,.g-b--push--t4of6,.g-b--push--t8of12{margin-left:66.666%}.g-b--push--t1of4,.g-b--push--t2of8,.g-b--push--t3of12{margin-left:25%}.g-b--push--t3of4,.g-b--push--t6of8,.g-b--push--t9of12{margin-left:75%}.g-b--push--t1of5,.g-b--push--t2of10{margin-left:20%}.g-b--push--t2of5,.g-b--push--t4of10{margin-left:40%}.g-b--push--t3of5,.g-b--push--t6of10{margin-left:60%}.g-b--push--t4of5,.g-b--push--t8of10{margin-left:80%}.g-b--push--t1of6,.g-b--push--t2of12{margin-left:16.666%}.g-b--push--t5of6,.g-b--push--t10of12{margin-left:83.333%}.g-b--push--t1of8{margin-left:12.5%}.g-b--push--t3of8{margin-left:37.5%}.g-b--push--t5of8{margin-left:62.5%}.g-b--push--t7of8{margin-left:87.5%}.g-b--push--t1of10{margin-left:10%}.g-b--push--t3of10{margin-left:30%}.g-b--push--t7of10{margin-left:70%}.g-b--push--t9of10{margin-left:90%}.g-b--push--t1of12{margin-left:8.333%}.g-b--push--t5of12{margin-left:41.666%}.g-b--push--t7of12{margin-left:58.333%}.g-b--push--t11of12{margin-left:91.666%}.g-b--pull--t1of1{margin-right:100%}.g-b--pull--t1of2,.g-b--pull--t2of4,.g-b--pull--t3of6,.g-b--pull--t4of8,.g-b--pull--t5of10,.g-b--pull--t6of12{margin-right:50%}.g-b--pull--t1of3,.g-b--pull--t2of6,.g-b--pull--t4of12{margin-right:33.333%}.g-b--pull--t2of3,.g-b--pull--t4of6,.g-b--pull--t8of12{margin-right:66.666%}.g-b--pull--t1of4,.g-b--pull--t2of8,.g-b--pull--t3of12{margin-right:25%}.g-b--pull--t3of4,.g-b--pull--t6of8,.g-b--pull--t9of12{margin-right:75%}.g-b--pull--t1of5,.g-b--pull--t2of10{margin-right:20%}.g-b--pull--t2of5,.g-b--pull--t4of10{margin-right:40%}.g-b--pull--t3of5,.g-b--pull--t6of10{margin-right:60%}.g-b--pull--t4of5,.g-b--pull--t8of10{margin-right:80%}.g-b--pull--t1of6,.g-b--pull--t2of12{margin-right:16.666%}.g-b--pull--t5of6,.g-b--pull--t10of12{margin-right:83.333%}.g-b--pull--t1of8{margin-right:12.5%}.g-b--pull--t3of8{margin-right:37.5%}.g-b--pull--t5of8{margin-right:62.5%}.g-b--pull--t7of8{margin-right:87.5%}.g-b--pull--t1of10{margin-right:10%}.g-b--pull--t3of10{margin-right:30%}.g-b--pull--t7of10{margin-right:70%}.g-b--pull--t9of10{margin-right:90%}.g-b--pull--t1of12{margin-right:8.333%}.g-b--pull--t5of12{margin-right:41.666%}.g-b--pull--t7of12{margin-right:58.333%}.g-b--pull--t11of12{margin-right:91.666%}.splashscreen-dillinger{width:500px}.splashscreen p{font-size:1.25rem;margin-bottom:1.43749rem;padding-top:.56251rem}.title{font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem}.title-document{font-size:1.25rem;margin-bottom:.89999rem;padding-top:.10001rem}.menu .menu-item--save-to,.menu .menu-item--import-from{display:block}.menu .menu-item--preview,.menu .menu-item--save-to.in-sidebar,.menu .menu-item--import-from.in-sidebar{display:none}.settings a{font-size:1.25rem}.words,.characters{font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem}.modal--dillinger.about .modal-dialog{font-size:1.25rem}.enter-zen-mode{display:block}.close-zen-mode{right:3rem;top:3rem}#zen{font-size:1.25rem;width:500px}.split-editor{border-right:1px solid #E8E8E8;float:left;padding-right:16px;width:50%}.show-preview .split-editor{display:block}.split-preview{display:block;float:right;position:relative;top:0;width:50%}#editor{font-size:1rem}.preview-mode-toggle-src,.preview-mode-toggle-html{display:block}}@media screen and (min-width:62.5em){html{font-size:.875em}body{font-size:1rem}ul,ol{margin-bottom:.83999rem;padding-top:.16001rem}p{padding-top:.66001rem}p,pre{margin-bottom:1.33999rem}pre,blockquote p{font-size:1rem;padding-top:.66001rem}blockquote p{margin-bottom:.33999rem}h1{font-size:2.0571429rem;margin-bottom:.21999rem;padding-top:.78001rem}h2{font-size:1.953125rem;margin-bottom:.1835837rem;padding-top:.8164163rem}h3{font-size:1.6457143rem;margin-bottom:.07599rem;padding-top:.92401rem}h4{font-size:1.5625rem;margin-bottom:.546865rem;padding-top:.453135rem}h5{font-size:1.25rem;margin-bottom:-.56251rem;padding-top:.56251rem}h6{font-size:1rem;margin-bottom:-.65001rem;padding-top:.65001rem}.g{margin-left:-16px;margin-right:-16px}.g-b{padding-left:16px;padding-right:16px}.g-b--d1of1{width:100%}.g-b--d1of2,.g-b--d2of4,.g-b--d3of6,.g-b--d4of8,.g-b--d5of10,.g-b--d6of12{width:50%}.g-b--d1of3,.g-b--d2of6,.g-b--d4of12{width:33.333%}.g-b--d2of3,.g-b--d4of6,.g-b--d8of12{width:66.666%}.g-b--d1of4,.g-b--d2of8,.g-b--d3of12{width:25%}.g-b--d3of4,.g-b--d6of8,.g-b--d9of12{width:75%}.g-b--d1of5,.g-b--d2of10{width:20%}.g-b--d2of5,.g-b--d4of10{width:40%}.g-b--d3of5,.g-b--d6of10{width:60%}.g-b--d4of5,.g-b--d8of10{width:80%}.g-b--d1of6,.g-b--d2of12{width:16.666%}.g-b--d5of6,.g-b--d10of12{width:83.333%}.g-b--d1of8{width:12.5%}.g-b--d3of8{width:37.5%}.g-b--d5of8{width:62.5%}.g-b--d7of8{width:87.5%}.g-b--d1of10{width:10%}.g-b--d3of10{width:30%}.g-b--d7of10{width:70%}.g-b--d9of10{width:90%}.g-b--d1of12{width:8.333%}.g-b--d5of12{width:41.666%}.g-b--d7of12{width:58.333%}.g-b--d11of12{width:91.666%}.g-b--push--d1of1{margin-left:100%}.g-b--push--d1of2,.g-b--push--d2of4,.g-b--push--d3of6,.g-b--push--d4of8,.g-b--push--d5of10,.g-b--push--d6of12{margin-left:50%}.g-b--push--d1of3,.g-b--push--d2of6,.g-b--push--d4of12{margin-left:33.333%}.g-b--push--d2of3,.g-b--push--d4of6,.g-b--push--d8of12{margin-left:66.666%}.g-b--push--d1of4,.g-b--push--d2of8,.g-b--push--d3of12{margin-left:25%}.g-b--push--d3of4,.g-b--push--d6of8,.g-b--push--d9of12{margin-left:75%}.g-b--push--d1of5,.g-b--push--d2of10{margin-left:20%}.g-b--push--d2of5,.g-b--push--d4of10{margin-left:40%}.g-b--push--d3of5,.g-b--push--d6of10{margin-left:60%}.g-b--push--d4of5,.g-b--push--d8of10{margin-left:80%}.g-b--push--d1of6,.g-b--push--d2of12{margin-left:16.666%}.g-b--push--d5of6,.g-b--push--d10of12{margin-left:83.333%}.g-b--push--d1of8{margin-left:12.5%}.g-b--push--d3of8{margin-left:37.5%}.g-b--push--d5of8{margin-left:62.5%}.g-b--push--d7of8{margin-left:87.5%}.g-b--push--d1of10{margin-left:10%}.g-b--push--d3of10{margin-left:30%}.g-b--push--d7of10{margin-left:70%}.g-b--push--d9of10{margin-left:90%}.g-b--push--d1of12{margin-left:8.333%}.g-b--push--d5of12{margin-left:41.666%}.g-b--push--d7of12{margin-left:58.333%}.g-b--push--d11of12{margin-left:91.666%}.g-b--pull--d1of1{margin-right:100%}.g-b--pull--d1of2,.g-b--pull--d2of4,.g-b--pull--d3of6,.g-b--pull--d4of8,.g-b--pull--d5of10,.g-b--pull--d6of12{margin-right:50%}.g-b--pull--d1of3,.g-b--pull--d2of6,.g-b--pull--d4of12{margin-right:33.333%}.g-b--pull--d2of3,.g-b--pull--d4of6,.g-b--pull--d8of12{margin-right:66.666%}.g-b--pull--d1of4,.g-b--pull--d2of8,.g-b--pull--d3of12{margin-right:25%}.g-b--pull--d3of4,.g-b--pull--d6of8,.g-b--pull--d9of12{margin-right:75%}.g-b--pull--d1of5,.g-b--pull--d2of10{margin-right:20%}.g-b--pull--d2of5,.g-b--pull--d4of10{margin-right:40%}.g-b--pull--d3of5,.g-b--pull--d6of10{margin-right:60%}.g-b--pull--d4of5,.g-b--pull--d8of10{margin-right:80%}.g-b--pull--d1of6,.g-b--pull--d2of12{margin-right:16.666%}.g-b--pull--d5of6,.g-b--pull--d10of12{margin-right:83.333%}.g-b--pull--d1of8{margin-right:12.5%}.g-b--pull--d3of8{margin-right:37.5%}.g-b--pull--d5of8{margin-right:62.5%}.g-b--pull--d7of8{margin-right:87.5%}.g-b--pull--d1of10{margin-right:10%}.g-b--pull--d3of10{margin-right:30%}.g-b--pull--d7of10{margin-right:70%}.g-b--pull--d9of10{margin-right:90%}.g-b--pull--d1of12{margin-right:8.333%}.g-b--pull--d5of12{margin-right:41.666%}.g-b--pull--d7of12{margin-right:58.333%}.g-b--pull--d11of12{margin-right:91.666%}.splashscreen-dillinger{width:700px}.splashscreen p{font-size:1.25rem;margin-bottom:1.43749rem;padding-top:.56251rem}.title{font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem}.title-document{font-size:1.25rem;margin-bottom:.89999rem;padding-top:.10001rem}.menu .menu-item--export-as{display:block}.menu .menu-item--preview{display:none}.settings a{font-size:1.25rem}.words,.characters{font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem}.modal--dillinger.about .modal-dialog,#zen{font-size:1.25rem}#zen{width:700px}#editor{font-size:1rem}}@media screen and (min-width:87.5em){html{font-size:.875em}body{font-size:1rem}ul,ol{margin-bottom:.83999rem;padding-top:.16001rem}p{padding-top:.66001rem}p,pre{margin-bottom:1.33999rem}pre,blockquote p{font-size:1rem;padding-top:.66001rem}blockquote p{margin-bottom:.33999rem}h1{font-size:2.0571429rem;margin-bottom:.21999rem;padding-top:.78001rem}h2{font-size:1.953125rem;margin-bottom:.1835837rem;padding-top:.8164163rem}h3{font-size:1.6457143rem;margin-bottom:.07599rem;padding-top:.92401rem}h4{font-size:1.5625rem;margin-bottom:.546865rem;padding-top:.453135rem}h5{font-size:1.25rem;margin-bottom:-.56251rem;padding-top:.56251rem}h6{font-size:1rem;margin-bottom:-.65001rem;padding-top:.65001rem}.splashscreen-dillinger{width:800px}.splashscreen p{font-size:1.25rem;margin-bottom:1.43749rem;padding-top:.56251rem}.title{font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem}.title-document{margin-bottom:.89999rem;padding-top:.10001rem}.title-document,.settings a{font-size:1.25rem}.words,.characters{font-size:.8rem;margin-bottom:.77999rem;padding-top:.22001rem}.modal--dillinger.about .modal-dialog,#zen{font-size:1.25rem}#editor{font-size:1rem}}@media (min-width:768px){.form-inline .form-group{display:inline-block;margin-bottom:0;vertical-align:middle}.form-inline .form-control{display:inline-block;width:auto;vertical-align:middle}.form-inline .input-group{display:inline-table;vertical-align:middle}.form-inline .input-group .input-group-addon,.form-inline .input-group .input-group-btn,.form-inline .input-group .form-control{width:auto}.form-inline .input-group>.form-control{width:100%}.form-inline .control-label{margin-bottom:0;vertical-align:middle}.form-inline .radio,.form-inline .checkbox{display:inline-block;margin-top:0;margin-bottom:0;vertical-align:middle}.form-inline .radio label,.form-inline .checkbox label{padding-left:0}.form-inline .radio input[type="radio"],.form-inline .checkbox input[type="checkbox"]{position:relative;margin-left:0}.form-inline .has-feedback .form-control-feedback{top:0}.form-horizontal .control-label{text-align:right;margin-bottom:0;padding-top:7px}.form-horizontal .form-group-lg .control-label{padding-top:14.3px}.form-horizontal .form-group-sm .control-label{padding-top:6px}.modal-dialog{width:600px;margin:30px auto}.modal-content{box-shadow:0 5px 15px rgba(0,0,0,.5)}.modal-sm{width:300px}}@media (min-width:992px){.modal-lg{width:900px}}@media screen and (max-width:1200px){#_default_{max-width:30%}#_default_ ._default_{font-size:.825rem;line-height:.875rem;padding:12px 12px 6px 24px;text-align:justify}}@media screen and (max-width:1100px){#_default_{max-width:27%}#_default_ ._default_{font-size:.8rem;line-height:.85rem;padding:12px 6px 6px 24px;text-align:justify}}@media screen and (max-width:1000px){#_default_{max-width:24%}#_default_ ._default_{font-size:.775rem;line-height:.8rem;padding:12px 6px 6px 24px;text-align:justify}}@media screen and (max-width:900px){#_default_{max-width:30%}}@media screen and (max-width:767px){.table-responsive{width:100%;margin-bottom:15px;overflow-y:hidden;overflow-x:auto;-ms-overflow-style:-ms-autohiding-scrollbar;border:1px solid #ddd;-webkit-overflow-scrolling:touch}.table-responsive>.table{margin-bottom:0}.table-responsive>.table>thead>tr>th,.table-responsive>.table>thead>tr>td,.table-responsive>.table>tbody>tr>th,.table-responsive>.table>tbody>tr>td,.table-responsive>.table>tfoot>tr>th,.table-responsive>.table>tfoot>tr>td{white-space:nowrap}.table-responsive>.table-bordered{border:0}.table-responsive>.table-bordered>thead>tr>th:first-child,.table-responsive>.table-bordered>thead>tr>td:first-child,.table-responsive>.table-bordered>tbody>tr>th:first-child,.table-responsive>.table-bordered>tbody>tr>td:first-child,.table-responsive>.table-bordered>tfoot>tr>th:first-child,.table-responsive>.table-bordered>tfoot>tr>td:first-child{border-left:0}.table-responsive>.table-bordered>thead>tr>th:last-child,.table-responsive>.table-bordered>thead>tr>td:last-child,.table-responsive>.table-bordered>tbody>tr>th:last-child,.table-responsive>.table-bordered>tbody>tr>td:last-child,.table-responsive>.table-bordered>tfoot>tr>th:last-child,.table-responsive>.table-bordered>tfoot>tr>td:last-child{border-right:0}.table-responsive>.table-bordered>tbody>tr:last-child>th,.table-responsive>.table-bordered>tbody>tr:last-child>td,.table-responsive>.table-bordered>tfoot>tr:last-child>th,.table-responsive>.table-bordered>tfoot>tr:last-child>td{border-bottom:0}}@media screen and (max-width:720px){#_default_{max-width:60%}#_default_ ._default_{font-size:.75rem;line-height:1rem;padding:12px 24px}}@media screen and (max-width:620px){#_default_{max-width:50%}#_default_ ._default_{font-size:.66rem;letter-spacing:1px;line-height:1rem;padding:10px 24px}}@media screen and (max-width:520px){#_default_ ._default_{font-size:.4rem;line-height:.875rem;padding:6px 12px 6px 24px;text-align:justify}}@media screen and (max-width:460px){#_default_{display:none}}@media screen and (max-width:46.1875em){.editor-header{display:none}.editor-header--first{display:block;width:100%}}</style></head><body id="preview">
<h1 class="code-line" data-line-start="0" data-line-end="1"><a id="InstructLab_0"></a>InstructLab</h1>
<p class="has-line-data" data-line-start="2" data-line-end="3">如今，大语言模型在很多场合蓬勃发展，我们都见证了人工智能给世界带来的重大影响，尤其是<em>ChatGPT</em>和其他大型语言模型的发布，我们都相信它将在未来几年改变我们的日常生活。但大多数模型仍是各自为政。虽然大语言模型 (LLM) 具有巨大的潜力，但它们也面临着挑战。使用 LLM 需要高质量的训练数据、专业技能和知识以及大量的计算资源。分叉和重新训练模型的过程也很耗时且成本高昂。开源社区通常为模型的生成贡献最多，但他们的贡献可能需要数月或数年才能合并回基础模型 （如果他们能回归的话）</p>
<h3 class="code-line" data-line-start="5" data-line-end="6"><a id="RAG__5"></a>传统的RAG （检索增强生成）流程</h3>
<p class="has-line-data" data-line-start="7" data-line-end="8">传统方式没有社区，没有办法贡献代码，也没有办法丰富数据集</p>
<p class="has-line-data" data-line-start="9" data-line-end="10">当前有许多项目正在采用开源的大语言模型，例如Llama、Mixtral，但它们遇到了三个主要挑战：</p>
<ol>
<li class="has-line-data" data-line-start="11" data-line-end="12">直接对LLMs进行贡献不容易。重新训练新的模型会导致分叉，这让使用者不得不在不易扩展的模型之间做出选择。对于模型创建者来说，维护这些分叉的成本高且困难。</li>
<li class="has-line-data" data-line-start="12" data-line-end="13">对于贡献想法需要具备人工智能/机器学习专业知识的限制。一个人必须学习如何分叉、训练和优化模型才能实现自己的想法，这是一个高门槛的要求。</li>
<li class="has-line-data" data-line-start="13" data-line-end="15">缺乏有关分叉模型的社区治理或最佳实践的指导、审查和分发。</li>
</ol>
<p class="has-line-data" data-line-start="15" data-line-end="16">例如：<em>HuggingFace</em>上发布的许多<em>LLM</em>仅包含用于推理的成果 — — 它们周围没有社区，没有办法贡献代码，也没有办法丰富数据集。</p>
<p class="has-line-data" data-line-start="17" data-line-end="18">所需的每个微调都<strong>完全依赖于用户的责任</strong>。</p>
<p class="has-line-data" data-line-start="20" data-line-end="21">如果你仔细想想，你会发现它极大地减缓了创新。开源给软件行业带来的真正文化转变之一是能够<strong>围绕</strong>开源项目进行协作，从而创造出更好的解决方案和产品。公司和个人可以共享和贡献代码、修复错误、构建新功能，作为一个拥有共同目标的社区的一部分——<strong>持续改进</strong>。</p>
<p class="has-line-data" data-line-start="22" data-line-end="23">人工智能的生命周期变得与传统软件非常相似——它是使用已知的编程语言和框架开发的，它可以被打包并构建到容器中，我们测试它，监控它，部署它。</p>
<p class="has-line-data" data-line-start="24" data-line-end="25">![传统方式没有社区，没有办法贡献代码，也没有办法丰富数据集]</p>
<p class="has-line-data" data-line-start="26" data-line-end="27">传统方式没有社区，没有办法贡献代码，也没有办法丰富数据集</p>
<p class="has-line-data" data-line-start="28" data-line-end="29">那么我们如何利用已经做的事情来创建人工智能领域的社区呢？</p>
<p class="has-line-data" data-line-start="30" data-line-end="31">我们如何合作、贡献知识和共享数据集以获得更好、更准确的人工智能模型？</p>
<p class="has-line-data" data-line-start="32" data-line-end="33"><a href="https://instructlab.ai/">InstructLab</a></p>
<h2 class="code-line" data-line-start="34" data-line-end="35"><a id="InstructLab__34"></a><strong>InstructLab 框架</strong></h2>
<p class="has-line-data" data-line-start="36" data-line-end="37">![InstructLab 社区模型将使用最新贡献进行更新，并定期在 Hugging Face 上分享。]</p>
<p class="has-line-data" data-line-start="38" data-line-end="39">InstructLab 社区模型将使用最新贡献进行更新，并定期在 Hugging Face 上分享。</p>
<p class="has-line-data" data-line-start="40" data-line-end="41">InstructLab 项目更像任何其他开源软件项目，提供了一种开源的生成式 AI 方法，它为社区提供了创建和合并 LLM 更改的工具，支持定期构建并且增强已有的预训练的大语言模型，而无需从头开始重新训练模型。这种方法不仅降低成本、消除测试和实验障碍，并且保证了一致性 — 即确保模型的答案准确、公正且符合其用户和创建者的目标。</p>
<p class="has-line-data" data-line-start="42" data-line-end="43">InstructLab 的工作原理是利用 LLM 生成的高质量示例来增强人工整理的数据，从而降低数据创建成本。然后可以使用 InstructLab 生成的数据来定制或改进基础模型，而无需重新训练它，从而节省更多成本。IBM Research 已使用 InstructLab 生成合成数据，以改进其用于语言和<a href="https://research.ibm.com/blog/granite-code-models-open-source">代码的开源</a><a href="https://www.ibm.com/blog/building-ai-for-business-ibms-granite-foundation-models/">Granite 模型</a>。</p>
<p class="has-line-data" data-line-start="44" data-line-end="45">它提供<strong>单一工具</strong>来下载、提供、测试和训练<em>LLM</em>，以便任何人都可以贡献和改进现有功能——无论是内部还是外部，面向更广泛的社区。</p>
<p class="has-line-data" data-line-start="46" data-line-end="47">InstructLab 使社区贡献者能够向特定模型<strong>添加额外的“技能”或“知识”</strong></p>
<p class="has-line-data" data-line-start="48" data-line-end="49">技能和知识的分类有助于识别所需能力的差距，然后在<strong>合成数据中</strong>生成足够的多样性以有效地调整基础模型。可以将InstructLab视为一个试验厨房，用于尝试和提交用于生成合成数据的新“配方”，以教授 LLM 新知识和技能。</p>
<p class="has-line-data" data-line-start="50" data-line-end="51"><strong>通过分类法</strong>，<a href="https://research.ibm.com/blog/LLM-generated-data">LAB</a>可以创建与您想要添加到模型中的任务相对应的高质量数据。分类法是迄今为止在 InstructLab 数据上调优的 LLM 所学到的知识的层次结构图，可轻松识别和填补漏洞。</p>
<p class="has-line-data" data-line-start="52" data-line-end="53">InstructLab 的训练方案将新信息吸收到模型中，而不会导致模型覆盖之前学到的内容。基础模型在漫长的预训练阶段注入了核心知识和能力。如果需要进行实质性改进，则必须重新训练预先训练的基础模型。</p>
<p class="has-line-data" data-line-start="54" data-line-end="55">**这个项目让不懂<a href="https://www.zhihu.com/search?q=transformer%E6%A8%A1%E5%9E%8B&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22699500296%22%7D">transformer模型</a>, 不懂LLM的人, 也能训练自己的模型。**共同构建更好的模型，就像参与开源软件项目一样。</p>
<p class="has-line-data" data-line-start="56" data-line-end="57">![<strong>L</strong>arge-scale <strong>A</strong>lignment for chat<strong>B</strong>ots]</p>
<p class="has-line-data" data-line-start="58" data-line-end="59"><strong>L</strong>arge-scale <strong>A</strong>lignment for chat<strong>B</strong>ots</p>
<hr>
<h3 class="code-line" data-line-start="62" data-line-end="63"><a id="Install_with_Apple_Metal_on_M1M2M3_Macs_62"></a><strong>Install with Apple Metal on M1/M2/M3 Macs</strong></h3>
<pre><code class="has-line-data" data-line-start="65" data-line-end="70" class="language-bash">python3 -m venv --upgrade-deps venv
<span class="hljs-built_in">source</span> venv/bin/activate
pip cache remove llama_cpp_python
pip install instructlab
</code></pre>
<pre><code class="has-line-data" data-line-start="72" data-line-end="83" class="language-bash">/Users/yehua/instructlab

~/instructlab  <span class="hljs-built_in">source</span> venv/bin/activate
(venv)  ~/instructlab  ilab chat
/Users/yehua/instructlab/venv/lib/python3.<span class="hljs-number">9</span>/site-packages/urllib3/__init__.py:<span class="hljs-number">35</span>: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL <span class="hljs-number">1.1</span>.<span class="hljs-number">1</span>+, currently the <span class="hljs-string">'ssl'</span> module is compiled with <span class="hljs-string">'LibreSSL 2.8.3'</span>. See: https://github.com/urllib3/urllib3/issues/<span class="hljs-number">3020</span>
  warnings.warn(
╭──────────────────────────────────────────────────────────────────────────────── system ────────────────────────────────────────────────────────────────────────────────╮
│ Welcome to InstructLab Chat w/ MODELS/MERLINITE-<span class="hljs-number">7</span>B-LAB-Q4_K_M.GGUF (<span class="hljs-built_in">type</span> /h <span class="hljs-keyword">for</span> <span class="hljs-built_in">help</span>)                                                                                  │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
&gt;&gt;&gt;                                                                                                                                                           [S][default]
</code></pre>
<h3 class="code-line" data-line-start="84" data-line-end="85"><a id="_IBM_Granite__84"></a>下载 IBM Granite 模型</h3>
<p class="has-line-data" data-line-start="86" data-line-end="87">先准备好 HF Token = <code>xxxxxxxxxxxxxxxxxx</code></p>
<p class="has-line-data" data-line-start="88" data-line-end="89"><a href="https://huggingface.co/settings/tokens">https://huggingface.co/settings/tokens</a></p>
<p class="has-line-data" data-line-start="90" data-line-end="91">如果需要，设置HF Token</p>
<pre><code class="has-line-data" data-line-start="93" data-line-end="95" class="language-bash">HF_TOKEN=&lt;YOUR HUGGINGFACE TOKEN GOES HERE&gt; ilab download --repository=TheBloke/Mixtral-<span class="hljs-number">8</span>x7B-Instruct-v0.<span class="hljs-number">1</span>-GGUF --filename=mixtral-<span class="hljs-number">8</span>x7b-instruct-v0.<span class="hljs-number">1</span>.Q4_K_M.gguf
</code></pre>
<p class="has-line-data" data-line-start="96" data-line-end="97">下载其他model</p>
<pre><code class="has-line-data" data-line-start="99" data-line-end="102" class="language-bash">$ ilab download --repository instructlab/granite-<span class="hljs-number">7</span>b-lab-GGUF --filename granite-<span class="hljs-number">7</span>b-lab-Q4_K_M.gguf <span class="hljs-comment">#IBM Granite</span>
$ ilab download --repository QuantFactory/Qwen2-<span class="hljs-number">7</span>B-Instruct-deccp-GGUF --filename Qwen2-<span class="hljs-number">7</span>B-Instruct-deccp.Q4_K_M.gguf <span class="hljs-comment">#通义千问</span>
</code></pre>
<p class="has-line-data" data-line-start="103" data-line-end="104">根据CPU核心，调整线程数</p>
<pre><code class="has-line-data" data-line-start="106" data-line-end="109" class="language-bash">$ ilab serve --model-path models/granite-<span class="hljs-number">7</span>b-lab-Q4_K_M.gguf --num-threads <span class="hljs-number">14</span> --max-ctx-size <span class="hljs-number">20480</span>
$ ilab chat -m models/granite-<span class="hljs-number">7</span>b-lab-Q4_K_M.gguf
</code></pre>
<h3 class="code-line" data-line-start="110" data-line-end="111"><a id="_Knowledge__taxonomies_110"></a>添加 Knowledge 到 taxonomies</h3>
<p class="has-line-data" data-line-start="112" data-line-end="113">The LAB method is driven by <a href="https://github.com/instructlab/taxonomy">taxonomies</a>, which are largely created manually and with care.</p>
<p class="has-line-data" data-line-start="114" data-line-end="115"><code>qna.yaml</code></p>
<pre><code class="has-line-data" data-line-start="117" data-line-end="131" class="language-yaml">version: 2
task_description: &lt;string&gt;
created_by: &lt;string&gt;
seed_examples: #需要提供至少两个相关的问答作为种子
  - question: &lt;string&gt;
    answer: |
      &lt;multi-line string&gt;
  - context: |
      &lt;multi-line string&gt;
    question: &lt;string&gt;
    answer: |
      &lt;multi-line string&gt;
  ...
</code></pre>
<p class="has-line-data" data-line-start="132" data-line-end="133"><strong>训练数据</strong></p>
<pre><code class="has-line-data" data-line-start="135" data-line-end="161" class="language-yaml">version: 2
task_description: Teach the Large Language Model about the movie Oppenheimer
created_by: IBM Ecosystems Engineering SI Lab
domain: movie
seed_examples:
- answer: |
    The movie “Oppenheimer” was written, directed, and produced by Christopher Nolan1.
  question: Who directed the movie “Oppenheimer”?
- answer: |
    The movie follows the life of J. Robert Oppenheimer, the American theoretical physicist who helped develop the first nuclear weapons during World War II.
  question: What is the movie “Oppenheimer” about?
- answer: |
    The movie starred Cillian Murphy as Oppenheimer, alongside Robert Downey Jr. as the United States Atomic Energy Commission member Lewis Strauss. The ensemble supporting cast includes Emily Blunt, Matt Damon, Florence Pugh, Josh Hartnett, Casey Affleck, Rami Malek, and Kenneth Branagh
  question: Who starred in the movie “Oppenheimer”?
- answer: |
    The movie “Oppenheimer” was released on July 21, 2023
  question: When was the movie “Oppenheimer” released?
- answer: |
    The movie “Oppenheimer” received critical acclaim and won seven Academy Awards, including Best Picture, Best Director for Nolan, Best Actor for Murphy and Best Supporting Actor for Downey. It grossed over $976 million worldwide, becoming the third-highest-grossing film of 2023, the highest-grossing World War II-related film, the highest-grossing biographical film and the second-highest-grossing R-rated film
  question: How was the movie “Oppenheimer” received?
document:
  repo: https://github.com/mikeyehua/instructlab.git
  commit: 76a2773
  patterns:
  - Oppenheimer.md
</code></pre>
<p class="has-line-data" data-line-start="162" data-line-end="163"><code>把上面的 问答文件 qna.yaml和 元数据文件 attribution.txt放在 taxonomy/knowledge/movies/oppenheimer 目录下</code></p>
<p class="has-line-data" data-line-start="164" data-line-end="165">原始参考数据来源<code>document</code>放在这里：</p>
<p class="has-line-data" data-line-start="166" data-line-end="167"><a href="https://github.com/mikeyehua/instructlab/blob/main/Oppenheimer.md">instructlab/Oppenheimer.md at main · mikeyehua/instructlab</a></p>
<p class="has-line-data" data-line-start="168" data-line-end="169">原文参考：<a href="https://github.com/syedaameena/InstructLab-Skill/blob/main/QnA.yaml?utm_source=ibm_developer&amp;utm_content=in_content_link&amp;utm_id=tutorials_awb-tuning-pretrained-llms-instructlab">https://github.com/syedaameena/InstructLab-Skill/blob/main/QnA.yaml?utm_source=ibm_developer&amp;utm_content=in_content_link&amp;utm_id=tutorials_awb-tuning-pretrained-llms-instructlab</a></p>
<ul>
<li class="has-line-data" data-line-start="170" data-line-end="175"><code>document</code>: The source of your knowledge contribution.
<ul>
<li class="has-line-data" data-line-start="171" data-line-end="172"><code>repo</code>: The URL to your repository that holds your knowledge markdown files.</li>
<li class="has-line-data" data-line-start="172" data-line-end="173"><code>commit</code>: The SHA of the commit in your repository with your knowledge markdown files.</li>
<li class="has-line-data" data-line-start="173" data-line-end="175"><code>patterns</code>: A list of glob patterns specifying the markdown files in your repository. Any glob pattern that starts with ``, such as <code>.md</code>, must be quoted due to YAML rules. For example, <code>&quot;*.md&quot;</code>.</li>
</ul>
</li>
</ul>
<p class="has-line-data" data-line-start="175" data-line-end="177">&lt;aside&gt;<br>
💡 YAML 测试站：</p>
<p class="has-line-data" data-line-start="178" data-line-end="179"><a href="https://www.yamllint.com/">https://www.yamllint.com/</a></p>
<p class="has-line-data" data-line-start="180" data-line-end="181">&lt;/aside&gt;</p>
<p class="has-line-data" data-line-start="182" data-line-end="183"><strong>Knowledge: YAML examples</strong></p>
<p class="has-line-data" data-line-start="184" data-line-end="186">&lt;aside&gt;<br>
💡 <strong>在没有生成新的训练数据集之前，用下面的问题测试，返回出的答案，可以看出来信息不正确，不能达到预期</strong></p>
<blockquote>
<p class="has-line-data" data-line-start="187" data-line-end="188">Who starred in the movie Oppenheimer?</p>
</blockquote>
<p class="has-line-data" data-line-start="190" data-line-end="192">elapsed 9.513 seconds -<br>
The movie “Oppenheimer” is <strong>an upcoming film</strong> adaptation of the Pulitzer Prize-winning biography “American Prometheus: The Life of J. Robert Oppenheimer” by Kira Davis. Cillian Murphy has been cast as the title character, J. Robert Oppenheimer. Other notable stars who have joined the cast include Matt Damon as Leslie Groves, the head of the Yale project and the father of Oppenheimer’s wife, Emily Blunt as Katherine Manning Oppenheimer, the wife of J. Robert Oppenheimer, and Alden Ehrenreich as Donald Oppenheimer, their son.</p>
<p class="has-line-data" data-line-start="193" data-line-end="194">The film is directed by Christopher Nolan, known for his work on blockbuster movies like “Batman Begins,” “The Dark Knight,” and “Interstellar.” “Oppenheimer” is set to explore the life of J. Robert Oppenheimer, the man who led the project that developed the first atomic bomb during World War II. The movie is expected to delve into the moral complexities of nuclear power and the human cost of scientific progress.</p>
<p class="has-line-data" data-line-start="195" data-line-end="196">While there isn’t much information available about the plot yet, I can provide you with some context on the historical figures and events that “Oppenheimer” might depict. The film will likely cover Oppenheimer’s early life, his education at the prestigious Institute for Advanced Studies in Princeton, New Jersey, and his work at the Los Alamos National Laboratory during the Manhattan Project. Additionally, the movie may explore Oppenheimer’s struggles with the moral implications of nuclear weapons and his eventual condemnation by the U.S. government during the Red Scare.</p>
<p class="has-line-data" data-line-start="197" data-line-end="198"><strong>Stay tuned for more updates</strong> on “Oppenheimer” as filming progresses and <strong>more information becomes available</strong>. If you have any questions about the cast, directors, or historical context of the movie, please let me know!</p>
<blockquote>
<p class="has-line-data" data-line-start="199" data-line-end="200">What are the release dates for Oppenheimer movie?</p>
</blockquote>
<p class="has-line-data" data-line-start="202" data-line-end="204">elapsed 5.935 seconds -<br>
As of my last update, the release date for “Oppenheimer” <strong>is not yet set</strong>. The film was announced in 2021, and production has recently started. Christopher Nolan, the director, often takes his time to perfect his films before releasing them. Given the complexity of the subject matter and the intricate plot, it’s likely that “Oppenheimer” will require a considerable amount of post-production work.</p>
<p class="has-line-data" data-line-start="205" data-line-end="206"><strong>Once filming is complete and the movie is ready for release</strong>, the production team will announce an official date. Typically, this happens several months before the release date. I recommend checking official film news sources and the film’s official website for the most up-to-date information on the release date.</p>
<p class="has-line-data" data-line-start="207" data-line-end="208">In the meantime, you can look forward to other movies released by Christopher Nolan or explore other fascinating historical figures and events that have been brought to the big screen. For example, “Dunkirk” (2017) and “Interstellar” (2014) are two of Nolan’s previous films that showcase his ability to create immersive and thought-provoking cinematic experiences.</p>
<p class="has-line-data" data-line-start="209" data-line-end="210">&lt;/aside&gt;</p>
<h3 class="code-line" data-line-start="211" data-line-end="212"><a id="_211"></a>验证数据</h3>
<p class="has-line-data" data-line-start="213" data-line-end="214"><code>ilab diff</code> 通过运行命令列出新数据并确保其在分类路径中正确注册，从而列出并验证新的数据。</p>
<h3 class="code-line" data-line-start="215" data-line-end="216"><a id="_215"></a>生成合成数据集</h3>
<p class="has-line-data" data-line-start="217" data-line-end="218">通过运行 <code>ilab generate</code> 命令来根据我们输入的问答生成更广泛的数据集，该命令根据 taxonomies 存储库中新添加的 knowledge 生成合成数据集。</p>
<pre><code class="has-line-data" data-line-start="220" data-line-end="222" class="language-yaml">ilab generate --model models/granite-7b-lab-Q4_K_M.gguf --num-instructions 100 --num-cpus 20 --server-ctx-size 20480
</code></pre>
<p class="has-line-data" data-line-start="223" data-line-end="224">可以看到生成合并数据的时候，系统调用了Apple Silicon的GPU加速</p>
<p class="has-line-data" data-line-start="225" data-line-end="226">您可以在输出中看到生成的新合成数据集。如果您对生成的数据集不满意，可以按 退出该过程<code>ctrl + c</code>。修改文件中的示例<code>qna.yaml</code>，然后重新运行<code>generate</code>命令。</p>
<p class="has-line-data" data-line-start="227" data-line-end="228">此过程将需要一些时间，具体取决于您的系统。在我的 M1 Mac Pro 上大约需要 21 分钟。您可以在输出中看到 ETA。</p>
<p class="has-line-data" data-line-start="230" data-line-end="231">生成合成数据后，您将看到已生成样本数和已丢弃样本数的摘要</p>
<pre><code class="has-line-data" data-line-start="233" data-line-end="241" class="language-yaml">INFO 2024-06-20 15:37:52,612 generate_data.py:608 101 instructions generated, 10 discarded due to format (see generated/discarded_granite-7b-lab-Q4_K_M_2024-06-20T15_16_44.log), 2 discarded due to rouge score
INFO 2024-06-20 15:37:52,612 generate_data.py:612 Generation took **1270.44s

-rw-r--r--  1 yehua  staff     9242  6 20 15:36 discarded_granite-7b-lab-Q4_K_M_2024-06-20T15_16_44.log #丢弃的数据集（日志文件）
-rw-r--r--  1 yehua  staff  6280617  6 20 15:37 generated_granite-7b-lab-Q4_K_M_2024-06-20T15_16_44.json #生成的数据集（json文件）
-rw-r--r--  1 yehua  staff     2511  6 20 15:37 test_granite-7b-lab-Q4_K_M_2024-06-20T15_16_44.jsonl #测试数据集（jsonl文件）
-rw-r--r--  1 yehua  staff    94763  6 20 15:37 train_granite-7b-lab-Q4_K_M_2024-06-20T15_16_44.jsonl #训练数据集（jsonl文件）**
</code></pre>
<h3 class="code-line" data-line-start="242" data-line-end="243"><a id="_242"></a>本地训练模型</h3>
<p class="has-line-data" data-line-start="244" data-line-end="245">一旦合成数据准备好了，您所要做的就是在终端中运行以下命令来训练模型：</p>
<p class="has-line-data" data-line-start="246" data-line-end="247">为了训练该模式，我们将使用<code>ilab</code>指向本地<code>GGUF</code>文件的 CLI — 训练用于<code>cuda-toolkit</code>与底层 NVIDIA GPU 进行交互。</p>
<p class="has-line-data" data-line-start="248" data-line-end="249"><strong><em>注意</em></strong>- 确保停止服务模型以便为训练阶段释放一些资源</p>
<p class="has-line-data" data-line-start="250" data-line-end="251"><code>ilab train --gguf-model-path models/granite-7b-lab-Q4_K_M.gguf</code></p>
<p class="has-line-data" data-line-start="252" data-line-end="253"><code>ilab train --gguf-model-path models/granite-7b-lab-Q4_K_M.gguf --device 'cuda'</code></p>
<pre><code class="has-line-data" data-line-start="255" data-line-end="314" class="language-yaml">ilab train --gguf-model-path models/granite-7b-lab-Q4_K_M.gguf
/Users/yehua/instructlab/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
[INFO] Loading
model-00003-of-00003.safetensors: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.54G/4.54G [05:06&lt;00:00, 8.66MB/s]
model-00001-of-00003.safetensors: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.94G/4.94G [05:09&lt;00:00, 8.71MB/s]
model-00002-of-00003.safetensors: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5.00G/5.00G [05:18&lt;00:00, 9.35MB/s]
Fetching 11 files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [05:20&lt;00:00, 29.18s/it]
/Users/yehua/instructlab/venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.███████████████████████████████████████████████████████████████████████████████| 5.00G/5.00G [05:18&lt;00:00, 25.9MB/s]
  warnings.warn(
config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 644/644 [00:00&lt;00:00, 628kB/s]
tokenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2.33k/2.33k [00:00&lt;00:00, 1.47MB/s]
tokenizer.model: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 493k/493k [00:00&lt;00:00, 668kB/s]
tokenizer.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.80M/1.80M [00:00&lt;00:00, 2.10MB/s]
added_tokens.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00&lt;00:00, 67.5kB/s]
special_tokens_map.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 655/655 [00:00&lt;00:00, 207kB/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
dtype=mlx.core.float16
[INFO] Quantizing
Using model_type='mistral'
Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total parameters 1244.079M
Trainable parameters 1.704M
Loading datasets
Training
Epoch 1: Iter 1: Val loss 2.087, Val took 26.183s
Iter 010: Train loss 1.848, It/sec 0.167, Tokens/sec 132.733
Epoch 1: Iter 10: Val loss 1.232, Val took 25.780s
Iter 10: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-010.npz.
Iter 020: Train loss 1.253, It/sec 0.195, Tokens/sec 133.049
Epoch 1: Iter 20: Val loss 1.053, Val took 25.881s
Iter 20: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-020.npz.
Iter 030: Train loss 0.924, It/sec 0.156, Tokens/sec 116.944
Epoch 2: Iter 30: Val loss 0.977, Val took 26.585s
Iter 30: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-030.npz.
Iter 040: Train loss 0.932, It/sec 0.167, Tokens/sec 121.478
Epoch 2: Iter 40: Val loss 0.937, Val took 26.198s
Iter 40: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-040.npz.
Iter 050: Train loss 0.763, It/sec 0.158, Tokens/sec 117.815
Epoch 3: Iter 50: Val loss 0.924, Val took 26.306s
Iter 50: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-050.npz.
Iter 060: Train loss 0.787, It/sec 0.165, Tokens/sec 119.941
Epoch 3: Iter 60: Val loss 0.903, Val took 27.079s
Iter 60: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-060.npz.
Iter 070: Train loss 0.668, It/sec 0.158, Tokens/sec 117.039
Epoch 4: Iter 70: Val loss 0.927, Val took 26.463s
Iter 70: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-070.npz.
Iter 080: Train loss 0.655, It/sec 0.132, Tokens/sec 96.649
Epoch 4: Iter 80: Val loss 0.911, Val took 26.960s
Iter 80: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-080.npz.
Iter 090: Train loss 0.599, It/sec 0.149, Tokens/sec 110.820
Epoch 5: Iter 90: Val loss 0.967, Val took 26.356s
Iter 90: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-090.npz.
Iter 100: Train loss 0.542, It/sec 0.152, Tokens/sec 111.259
Epoch 5: Iter 100: Val loss 0.947, Val took 28.326s
Iter 100: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-100.npz.
</code></pre>
<p class="has-line-data" data-line-start="315" data-line-end="316">此过程将需要一些时间，具体取决于您的系统 配置和迭代次数。在我的 M1 MacBook Pro 上完成 100 次迭代大约需要 30 分钟。您可以在输出中看到 ETA。</p>
<p class="has-line-data" data-line-start="317" data-line-end="318">目录中将创建一个新目录<code>ilab</code>，其名称类似于：<code>instructlab-merlinite-7b-lab</code>。此目录将包含新的模型权重和适配器。</p>
<pre><code class="has-line-data" data-line-start="320" data-line-end="323" class="language-yaml">drwxr-xr-x  14 yehua  staff   448  6 20 18:20 instructlab-merlinite-7b-lab
drwxr-xr-x  20 yehua  staff   640  6 20 18:36 instructlab-merlinite-7b-lab-mlx-q
</code></pre>
<h3 class="code-line" data-line-start="324" data-line-end="325"><a id="_324"></a>测试模型</h3>
<p class="has-line-data" data-line-start="326" data-line-end="327">通过运行命令来测试新训练的模型<code>ilab test</code>，以测试模型并验证其性能。</p>
<p class="has-line-data" data-line-start="328" data-line-end="329"><code>格式： ilab test --data-dir my-data --model-dir models/ibm/merlinite-7b</code></p>
<p class="has-line-data" data-line-start="330" data-line-end="331"><code>ilab test --data-dir ./taxonomy_data --model-dir instructlab-merlinite-7b-lab-mlx-q</code></p>
<pre><code class="has-line-data" data-line-start="333" data-line-end="497" class="language-yaml">ilab test

system prompt: You are an AI language model developed by IBM Research. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.
[1]
 user prompt: Who directed the movie “Oppenheimer”?
expected output: The movie “Oppenheimer” was written, directed, and produced by Christopher Nolan1.

-----model output BEFORE training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
LoRA init skipped
Total parameters 1242.375M
Trainable parameters 0.000M
Loading datasets
LoRA loading skipped
Generating
==========
Christopher Nolan
==========

-----model output AFTER training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total parameters 1244.079M
Trainable parameters 1.704M
Loading datasets
Generating
==========
Christopher Nolan directed the movie “Oppenheimer.” He is known for his involvement in the production of the film and was chosen by the studio to take on this project.
==========
[2]
 user prompt: What is the movie “Oppenheimer” about?
expected output: The movie follows the life of J. Robert Oppenheimer, the American theoretical physicist who helped develop the first nuclear weapons during World War II.

-----model output BEFORE training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
LoRA init skipped
Total parameters 1242.375M
Trainable parameters 0.000M
Loading datasets
LoRA loading skipped
Generating
==========
&quot;Oppenheimer&quot; is a film that delves into the life and times of J. Robert Oppenheimer, the brilliant physicist who led the Manhattan Project during World War II. The movie offers a gripping and thought-provoking portrayal of Oppenheimer's personal and professional journey while working on the development of the atomic bomb. Here are some key aspects of the movie and the historical context that you might find useful for your article:

1.
==========

-----model output AFTER training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total parameters 1244.079M
Trainable parameters 1.704M
Loading datasets
Generating
==========
&quot;Oppenheimer&quot; is a biographical film directed by Christopher Nolan that tells the story of J. Robert Oppenheimer, the father of the atomic bomb and the director of the Los Alamos Laboratory during World War II. The movie explores Oppenheimer's personal life, his contributions to the development of nuclear weapons, and the impact of his actions on the world.

The film portrays Oppenheimer's guilt over the destruction caused by the
==========
[3]
 user prompt: Who starred in the movie “Oppenheimer”?
expected output: The movie starred Cillian Murphy as Oppenheimer, alongside Robert Downey Jr. as the United States Atomic Energy Commission member Lewis Strauss. The ensemble supporting cast includes Emily Blunt, Matt Damon, Florence Pugh, Josh Hartnett, Casey Affleck, Rami Malek, and Kenneth Branagh

-----model output BEFORE training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
LoRA init skipped
Total parameters 1242.375M
Trainable parameters 0.000M
Loading datasets
LoRA loading skipped
Generating
==========
&quot;Oppenheimer&quot; is a historical drama film directed by Christopher Nolan, with an impressive cast and crew. The lead role of Dr. J. Robert Oppenheimer, the physicist who oversaw the Manhattan Project, is portrayed by a well-known actor, Cillian Murphy. Additionally, the film features other esteemed actors, including Emily Blunt as Katherine, Olivia Thirlby as Leona Woods, Matthew Goode as Klaus Fuch
==========

-----model output AFTER training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total parameters 1244.079M
Trainable parameters 1.704M
Loading datasets
Generating
==========
Cillian Murphy, Matt Damon, Emily Blunt, Robert Downey Jr., Gary Oldman, and Benny Safdie.
==========
[4]
 user prompt: When was the movie “Oppenheimer” released?
expected output: The movie “Oppenheimer” was released on July 21, 2023

-----model output BEFORE training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
LoRA init skipped
Total parameters 1242.375M
Trainable parameters 0.000M
Loading datasets
LoRA loading skipped
Generating
==========
I'm sorry for the confusion, but I couldn't find information about the release date of the movie &quot;Oppenheimer.&quot; Would you like me to search for more details about the movie or answer a different question?
==========

-----model output AFTER training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total parameters 1244.079M
Trainable parameters 1.704M
Loading datasets
Generating
==========
The movie was released in theaters on July 21, 2023.
==========
[5]
 user prompt: How was the movie “Oppenheimer” received?
expected output: The movie “Oppenheimer” received critical acclaim and won seven Academy Awards, including Best Picture, Best Director for Nolan, Best Actor for Murphy and Best Supporting Actor for Downey. It grossed over $976 million worldwide, becoming the third-highest-grossing film of 2023, the highest-grossing World War II-related film, the highest-grossing biographical film and the second-highest-grossing R-rated film

-----model output BEFORE training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
LoRA init skipped
Total parameters 1242.375M
Trainable parameters 0.000M
Loading datasets
LoRA loading skipped
Generating
==========
The movie &quot;Oppenheimer&quot; has received mixed reviews from critics and audiences alike. While some laud the film for its powerful performances and thought-provoking themes, others have criticized it for its pacing and structure.

Critics have praised the movie's direction, cinematography, and visual effects. The performances of the lead actors, particularly Cillian Murphy as J. Robert Oppenheimer, have been widely praised. However, some have criticized
==========

-----model output AFTER training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total parameters 1244.079M
Trainable parameters 1.704M
Loading datasets
Generating
==========
The movie &quot;Oppenheimer&quot; received mixed reviews from critics and audiences. It was praised for its visual effects and historical accuracy, but criticized for its length and slow pacing. Some viewers felt that the film could have been more engaging and emotionally resonant.
</code></pre>
<h3 class="code-line" data-line-start="498" data-line-end="499"><a id="_498"></a><strong>量化精细调整的模型</strong></h3>
<p class="has-line-data" data-line-start="500" data-line-end="501">现在训练已经完成，您应该期望<code>GGUF</code>在该模型路径下拥有新模型（在我们的例子中是模型目录）。</p>
<p class="has-line-data" data-line-start="502" data-line-end="503">为了在未来的文章中使用该模型，我们需要它具有合理的大小，接下来，我们将量化模型</p>
<p class="has-line-data" data-line-start="504" data-line-end="505"><code>ilab convert</code></p>
<pre><code class="has-line-data" data-line-start="507" data-line-end="1481" class="language-bash">ilab convert

Loading pretrained model
Using model_<span class="hljs-built_in">type</span>=<span class="hljs-string">'mistral'</span>
Special tokens have been added <span class="hljs-keyword">in</span> the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
INFO <span class="hljs-number">2024</span>-<span class="hljs-number">06</span>-<span class="hljs-number">20</span> <span class="hljs-number">18</span>:<span class="hljs-number">48</span>:<span class="hljs-number">22</span>,<span class="hljs-number">055</span> lab.py:<span class="hljs-number">1343</span> deleting instructlab-merlinite-<span class="hljs-number">7</span>b-lab-mlx-q...
[INFO] Loading
Special tokens have been added <span class="hljs-keyword">in</span> the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
dtype=&lt;class <span class="hljs-string">'numpy.float16'</span>&gt;
INFO <span class="hljs-number">2024</span>-<span class="hljs-number">06</span>-<span class="hljs-number">20</span> <span class="hljs-number">18</span>:<span class="hljs-number">50</span>:<span class="hljs-number">26</span>,<span class="hljs-number">422</span> lab.py:<span class="hljs-number">1352</span> deleting instructlab-merlinite-<span class="hljs-number">7</span>b-lab-mlx-q-fused...
Loading model file instructlab-merlinite-<span class="hljs-number">7</span>b-lab-trained/model.safetensors
params = Params(n_vocab=<span class="hljs-number">32008</span>, n_embd=<span class="hljs-number">4096</span>, n_layer=<span class="hljs-number">32</span>, n_ctx=<span class="hljs-number">32768</span>, n_ff=<span class="hljs-number">14336</span>, n_head=<span class="hljs-number">32</span>, n_head_kv=<span class="hljs-number">8</span>, n_experts=None, n_experts_used=None, f_norm_eps=<span class="hljs-number">1</span>e-<span class="hljs-number">05</span>, rope_scaling_<span class="hljs-built_in">type</span>=None, f_rope_freq_base=<span class="hljs-number">10000.0</span>, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath(<span class="hljs-string">'instructlab-merlinite-7b-lab-trained'</span>))
Found vocab files: {<span class="hljs-string">'spm'</span>: PosixPath(<span class="hljs-string">'instructlab-merlinite-7b-lab-trained/tokenizer.model'</span>), <span class="hljs-string">'bpe'</span>: None, <span class="hljs-string">'hfft'</span>: PosixPath(<span class="hljs-string">'instructlab-merlinite-7b-lab-trained/tokenizer.json'</span>)}
Loading vocab file PosixPath(<span class="hljs-string">'instructlab-merlinite-7b-lab-trained/tokenizer.model'</span>), <span class="hljs-built_in">type</span> <span class="hljs-string">'spm'</span>
Vocab info: &lt;SentencePieceVocab with <span class="hljs-number">32000</span> base tokens and <span class="hljs-number">5</span> added tokens&gt;
Special vocab info: &lt;SpecialVocab with <span class="hljs-number">0</span> merges, special tokens {<span class="hljs-string">'bos'</span>: <span class="hljs-number">1</span>, <span class="hljs-string">'eos'</span>: <span class="hljs-number">32000</span>, <span class="hljs-string">'unk'</span>: <span class="hljs-number">0</span>, <span class="hljs-string">'pad'</span>: <span class="hljs-number">32001</span>}, add special tokens {<span class="hljs-string">'bos'</span>: False, <span class="hljs-string">'eos'</span>: False}&gt;
Permuting layer <span class="hljs-number">0</span>
Permuting layer <span class="hljs-number">1</span>
Permuting layer <span class="hljs-number">2</span>
Permuting layer <span class="hljs-number">3</span>
Permuting layer <span class="hljs-number">4</span>
Permuting layer <span class="hljs-number">5</span>
Permuting layer <span class="hljs-number">6</span>
Permuting layer <span class="hljs-number">7</span>
Permuting layer <span class="hljs-number">8</span>
Permuting layer <span class="hljs-number">9</span>
Permuting layer <span class="hljs-number">10</span>
Permuting layer <span class="hljs-number">11</span>
Permuting layer <span class="hljs-number">12</span>
Permuting layer <span class="hljs-number">13</span>
Permuting layer <span class="hljs-number">14</span>
Permuting layer <span class="hljs-number">15</span>
Permuting layer <span class="hljs-number">16</span>
Permuting layer <span class="hljs-number">17</span>
Permuting layer <span class="hljs-number">18</span>
Permuting layer <span class="hljs-number">19</span>
Permuting layer <span class="hljs-number">20</span>
Permuting layer <span class="hljs-number">21</span>
Permuting layer <span class="hljs-number">22</span>
Permuting layer <span class="hljs-number">23</span>
Permuting layer <span class="hljs-number">24</span>
Permuting layer <span class="hljs-number">25</span>
Permuting layer <span class="hljs-number">26</span>
Permuting layer <span class="hljs-number">27</span>
Permuting layer <span class="hljs-number">28</span>
Permuting layer <span class="hljs-number">29</span>
Permuting layer <span class="hljs-number">30</span>
Permuting layer <span class="hljs-number">31</span>
lm_head.weight                                   -&gt; output.weight                            | F16    | [<span class="hljs-number">32008</span>, <span class="hljs-number">4096</span>]
model.embed_tokens.weight                        -&gt; token_embd.weight                        | F16    | [<span class="hljs-number">32008</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">0</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">0</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">0</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">0</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">0</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">0</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">0</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">0</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">0</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">0</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">0</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">0</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">0</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">0</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">0</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">0</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">0</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">0</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">1</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">1</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">1</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">1</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">1</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">1</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">1</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">1</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">1</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">1</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">1</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">1</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">1</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">1</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">1</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">1</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">1</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">1</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">10</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">10</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">10</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">10</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">10</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">10</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">10</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">10</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">10</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">10</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">10</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">10</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">10</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">10</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">10</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">10</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">10</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">10</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">11</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">11</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">11</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">11</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">11</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">11</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">11</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">11</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">11</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">11</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">11</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">11</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">11</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">11</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">11</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">11</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">11</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">11</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">12</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">12</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">12</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">12</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">12</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">12</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">12</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">12</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">12</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">12</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">12</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">12</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">12</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">12</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">12</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">12</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">12</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">12</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">13</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">13</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">13</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">13</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">13</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">13</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">13</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">13</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">13</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">13</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">13</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">13</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">13</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">13</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">13</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">13</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">13</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">13</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">14</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">14</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">14</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">14</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">14</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">14</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">14</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">14</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">14</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">14</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">14</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">14</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">14</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">14</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">14</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">14</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">14</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">14</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">15</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">15</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">15</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">15</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">15</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">15</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">15</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">15</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">15</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">15</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">15</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">15</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">15</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">15</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">15</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">15</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">15</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">15</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">16</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">16</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">16</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">16</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">16</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">16</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">16</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">16</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">16</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">16</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">16</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">16</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">16</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">16</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">16</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">16</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">16</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">16</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">17</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">17</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">17</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">17</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">17</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">17</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">17</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">17</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">17</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">17</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">17</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">17</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">17</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">17</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">17</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">17</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">17</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">17</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">18</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">18</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">18</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">18</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">18</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">18</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">18</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">18</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">18</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">18</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">18</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">18</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">18</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">18</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">18</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">18</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">18</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">18</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">19</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">19</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">19</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">19</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">19</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">19</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">19</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">19</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">19</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">19</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">19</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">19</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">19</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">19</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">19</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">19</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">19</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">19</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">2</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">2</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">2</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">2</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">2</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">2</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">2</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">2</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">2</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">2</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">2</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">2</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">2</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">2</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">2</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">2</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">2</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">2</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">20</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">20</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">20</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">20</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">20</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">20</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">20</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">20</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">20</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">20</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">20</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">20</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">20</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">20</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">20</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">20</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">20</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">20</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">21</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">21</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">21</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">21</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">21</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">21</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">21</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">21</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">21</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">21</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">21</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">21</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">21</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">21</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">21</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">21</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">21</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">21</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">22</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">22</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">22</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">22</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">22</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">22</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">22</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">22</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">22</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">22</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">22</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">22</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">22</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">22</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">22</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">22</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">22</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">22</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">23</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">23</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">23</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">23</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">23</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">23</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">23</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">23</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">23</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">23</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">23</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">23</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">23</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">23</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">23</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">23</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">23</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">23</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">24</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">24</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">24</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">24</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">24</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">24</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">24</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">24</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">24</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">24</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">24</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">24</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">24</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">24</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">24</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">24</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">24</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">24</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">25</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">25</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">25</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">25</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">25</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">25</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">25</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">25</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">25</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">25</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">25</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">25</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">25</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">25</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">25</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">25</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">25</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">25</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">26</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">26</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">26</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">26</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">26</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">26</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">26</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">26</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">26</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">26</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">26</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">26</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">26</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">26</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">26</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">26</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">26</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">26</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">27</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">27</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">27</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">27</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">27</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">27</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">27</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">27</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">27</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">27</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">27</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">27</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">27</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">27</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">27</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">27</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">27</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">27</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">28</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">28</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">28</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">28</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">28</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">28</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">28</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">28</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">28</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">28</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">28</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">28</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">28</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">28</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">28</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">28</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">28</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">28</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">29</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">29</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">29</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">29</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">29</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">29</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">29</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">29</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">29</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">29</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">29</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">29</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">29</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">29</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">29</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">29</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">29</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">29</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">3</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">3</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">3</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">3</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">3</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">3</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">3</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">3</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">3</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">3</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">3</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">3</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">3</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">3</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">3</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">3</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">3</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">3</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">30</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">30</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">30</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">30</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">30</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">30</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">30</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">30</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">30</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">30</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">30</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">30</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">30</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">30</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">30</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">30</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">30</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">30</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">31</span>.input_layernorm.weight           -&gt; blk.<span class="hljs-number">31</span>.attn_norm.weight                  | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">31</span>.mlp.down_proj.weight             -&gt; blk.<span class="hljs-number">31</span>.ffn_down.weight                   | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">31</span>.mlp.gate_proj.weight             -&gt; blk.<span class="hljs-number">31</span>.ffn_gate.weight                   | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">31</span>.mlp.up_proj.weight               -&gt; blk.<span class="hljs-number">31</span>.ffn_up.weight                     | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">31</span>.post_attention_layernorm.weight  -&gt; blk.<span class="hljs-number">31</span>.ffn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">31</span>.self_attn.k_proj.weight          -&gt; blk.<span class="hljs-number">31</span>.attn_k.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">31</span>.self_attn.o_proj.weight          -&gt; blk.<span class="hljs-number">31</span>.attn_output.weight                | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">31</span>.self_attn.q_proj.weight          -&gt; blk.<span class="hljs-number">31</span>.attn_q.weight                     | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">31</span>.self_attn.v_proj.weight          -&gt; blk.<span class="hljs-number">31</span>.attn_v.weight                     | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">4</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">4</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">4</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">4</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">4</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">4</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">4</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">4</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">4</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">4</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">4</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">4</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">4</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">4</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">4</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">4</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">4</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">4</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">5</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">5</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">5</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">5</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">5</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">5</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">5</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">5</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">5</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">5</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">5</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">5</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">5</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">5</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">5</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">5</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">5</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">5</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">6</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">6</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">6</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">6</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">6</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">6</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">6</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">6</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">6</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">6</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">6</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">6</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">6</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">6</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">6</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">6</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">6</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">6</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">7</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">7</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">7</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">7</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">7</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">7</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">7</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">7</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">7</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">7</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">7</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">7</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">7</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">7</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">7</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">7</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">7</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">7</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">8</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">8</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">8</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">8</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">8</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">8</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">8</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">8</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">8</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">8</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">8</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">8</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">8</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">8</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">8</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">8</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">8</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">8</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">9</span>.input_layernorm.weight            -&gt; blk.<span class="hljs-number">9</span>.attn_norm.weight                   | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">9</span>.mlp.down_proj.weight              -&gt; blk.<span class="hljs-number">9</span>.ffn_down.weight                    | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">14336</span>]
model.layers.<span class="hljs-number">9</span>.mlp.gate_proj.weight              -&gt; blk.<span class="hljs-number">9</span>.ffn_gate.weight                    | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">9</span>.mlp.up_proj.weight                -&gt; blk.<span class="hljs-number">9</span>.ffn_up.weight                      | F16    | [<span class="hljs-number">14336</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">9</span>.post_attention_layernorm.weight   -&gt; blk.<span class="hljs-number">9</span>.ffn_norm.weight                    | F16    | [<span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">9</span>.self_attn.k_proj.weight           -&gt; blk.<span class="hljs-number">9</span>.attn_k.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">9</span>.self_attn.o_proj.weight           -&gt; blk.<span class="hljs-number">9</span>.attn_output.weight                 | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">9</span>.self_attn.q_proj.weight           -&gt; blk.<span class="hljs-number">9</span>.attn_q.weight                      | F16    | [<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>]
model.layers.<span class="hljs-number">9</span>.self_attn.v_proj.weight           -&gt; blk.<span class="hljs-number">9</span>.attn_v.weight                      | F16    | [<span class="hljs-number">1024</span>, <span class="hljs-number">4096</span>]
model.norm.weight                                -&gt; output_norm.weight                       | F16    | [<span class="hljs-number">4096</span>]
Writing instructlab-merlinite-<span class="hljs-number">7</span>b-lab-trained/instructlab-merlinite-<span class="hljs-number">7</span>b-lab.gguf, format <span class="hljs-number">1</span>
Padding vocab with <span class="hljs-number">3</span> token(s) - &lt;dummy00001&gt; through &lt;dummy00003&gt;
gguf: This GGUF file is <span class="hljs-keyword">for</span> Little Endian only
gguf: Setting special token <span class="hljs-built_in">type</span> bos to <span class="hljs-number">1</span>
gguf: Setting special token <span class="hljs-built_in">type</span> eos to <span class="hljs-number">32000</span>
gguf: Setting special token <span class="hljs-built_in">type</span> unk to <span class="hljs-number">0</span>
gguf: Setting special token <span class="hljs-built_in">type</span> pad to <span class="hljs-number">32001</span>
gguf: Setting add_bos_token to False
gguf: Setting add_eos_token to False
gguf: Setting chat_template to {% <span class="hljs-keyword">for</span> message <span class="hljs-keyword">in</span> messages %}{% <span class="hljs-keyword">if</span> message[<span class="hljs-string">'role'</span>] == <span class="hljs-string">'system'</span> %}{{<span class="hljs-string">'&lt;|system|&gt;'</span>+ <span class="hljs-string">'
'</span> + message[<span class="hljs-string">'content'</span>] + <span class="hljs-string">'
'</span>}}{% <span class="hljs-keyword">elif</span> message[<span class="hljs-string">'role'</span>] == <span class="hljs-string">'user'</span> %}{{<span class="hljs-string">'&lt;|user|&gt;'</span> + <span class="hljs-string">'
'</span> + message[<span class="hljs-string">'content'</span>] + <span class="hljs-string">'
'</span>}}{% <span class="hljs-keyword">elif</span> message[<span class="hljs-string">'role'</span>] == <span class="hljs-string">'assistant'</span> %}{{<span class="hljs-string">'&lt;|assistant|&gt;'</span> + <span class="hljs-string">'
'</span> + message[<span class="hljs-string">'content'</span>] + <span class="hljs-string">'&lt;|endoftext|&gt;'</span> + (<span class="hljs-string">''</span> <span class="hljs-keyword">if</span> loop.last <span class="hljs-keyword">else</span> <span class="hljs-string">'
'</span>)}}{% endif %}{% endfor %}
[  <span class="hljs-number">1</span>/<span class="hljs-number">291</span>] Writing tensor output.weight                          | size  <span class="hljs-number">32008</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">0</span>
[  <span class="hljs-number">2</span>/<span class="hljs-number">291</span>] Writing tensor token_embd.weight                      | size  <span class="hljs-number">32008</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">0</span>
[  <span class="hljs-number">3</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">0</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">0</span>
[  <span class="hljs-number">4</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">0</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">0</span>
[  <span class="hljs-number">5</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">0</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">0</span>
[  <span class="hljs-number">6</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">0</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">0</span>
[  <span class="hljs-number">7</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">0</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">0</span>
[  <span class="hljs-number">8</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">0</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">0</span>
[  <span class="hljs-number">9</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">0</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">10</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">0</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">11</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">0</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">12</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">1</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">13</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">1</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">14</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">1</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">15</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">1</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">16</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">1</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">17</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">1</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">18</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">1</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">19</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">1</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">20</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">1</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">21</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">10</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">1</span>
[ <span class="hljs-number">22</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">10</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">23</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">10</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">24</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">10</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">25</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">10</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">26</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">10</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">27</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">10</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">28</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">10</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">29</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">10</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">30</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">11</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">31</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">11</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">32</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">11</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">33</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">11</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">34</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">11</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">35</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">11</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">36</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">11</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">37</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">11</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">38</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">11</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">39</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">12</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">2</span>
[ <span class="hljs-number">40</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">12</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">41</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">12</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">42</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">12</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">43</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">12</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">44</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">12</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">45</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">12</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">46</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">12</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">47</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">12</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">48</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">13</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">49</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">13</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">50</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">13</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">51</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">13</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">52</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">13</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">53</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">13</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">54</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">13</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">55</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">13</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">56</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">13</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">57</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">14</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">3</span>
[ <span class="hljs-number">58</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">14</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">59</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">14</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">60</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">14</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">61</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">14</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">62</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">14</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">63</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">14</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">64</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">14</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">65</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">14</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">66</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">15</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">67</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">15</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">68</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">15</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">69</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">15</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">70</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">15</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">71</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">15</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">72</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">15</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">73</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">15</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">74</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">15</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">75</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">16</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">4</span>
[ <span class="hljs-number">76</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">16</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">77</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">16</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">78</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">16</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">79</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">16</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">80</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">16</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">81</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">16</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">82</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">16</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">83</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">16</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">84</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">17</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">85</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">17</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">86</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">17</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">87</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">17</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">88</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">17</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">89</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">17</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">90</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">17</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">91</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">17</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">92</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">17</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">93</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">18</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">5</span>
[ <span class="hljs-number">94</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">18</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[ <span class="hljs-number">95</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">18</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[ <span class="hljs-number">96</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">18</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[ <span class="hljs-number">97</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">18</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">6</span>
[ <span class="hljs-number">98</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">18</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[ <span class="hljs-number">99</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">18</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">100</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">18</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">101</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">18</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">102</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">19</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">103</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">19</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">104</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">19</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">105</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">19</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">106</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">19</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">107</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">19</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">108</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">19</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">109</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">19</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">6</span>
[<span class="hljs-number">110</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">19</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">111</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">2</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">112</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">2</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">113</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">2</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">114</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">2</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">115</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">2</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">116</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">2</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">117</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">2</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">118</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">2</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">119</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">2</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">120</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">20</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">7</span>
[<span class="hljs-number">121</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">20</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">8</span>
[<span class="hljs-number">122</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">20</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">8</span>
[<span class="hljs-number">123</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">20</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">8</span>
[<span class="hljs-number">124</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">20</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">8</span>
[<span class="hljs-number">125</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">20</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">8</span>
[<span class="hljs-number">126</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">20</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">8</span>
[<span class="hljs-number">127</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">20</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">8</span>
[<span class="hljs-number">128</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">20</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">8</span>
[<span class="hljs-number">129</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">21</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">8</span>
[<span class="hljs-number">130</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">21</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">9</span>
[<span class="hljs-number">131</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">21</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">9</span>
[<span class="hljs-number">132</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">21</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">9</span>
[<span class="hljs-number">133</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">21</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">9</span>
[<span class="hljs-number">134</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">21</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">9</span>
[<span class="hljs-number">135</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">21</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">9</span>
[<span class="hljs-number">136</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">21</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">9</span>
[<span class="hljs-number">137</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">21</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+   <span class="hljs-number">9</span>
[<span class="hljs-number">138</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">22</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+   <span class="hljs-number">9</span>
[<span class="hljs-number">139</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">22</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">10</span>
[<span class="hljs-number">140</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">22</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">10</span>
[<span class="hljs-number">141</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">22</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">10</span>
[<span class="hljs-number">142</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">22</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">10</span>
[<span class="hljs-number">143</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">22</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">10</span>
[<span class="hljs-number">144</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">22</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">10</span>
[<span class="hljs-number">145</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">22</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">10</span>
[<span class="hljs-number">146</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">22</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">10</span>
[<span class="hljs-number">147</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">23</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">10</span>
[<span class="hljs-number">148</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">23</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">11</span>
[<span class="hljs-number">149</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">23</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">11</span>
[<span class="hljs-number">150</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">23</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">11</span>
[<span class="hljs-number">151</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">23</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">11</span>
[<span class="hljs-number">152</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">23</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">11</span>
[<span class="hljs-number">153</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">23</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">11</span>
[<span class="hljs-number">154</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">23</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">11</span>
[<span class="hljs-number">155</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">23</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">11</span>
[<span class="hljs-number">156</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">24</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">11</span>
[<span class="hljs-number">157</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">24</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">12</span>
[<span class="hljs-number">158</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">24</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">12</span>
[<span class="hljs-number">159</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">24</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">12</span>
[<span class="hljs-number">160</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">24</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">12</span>
[<span class="hljs-number">161</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">24</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">12</span>
[<span class="hljs-number">162</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">24</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">12</span>
[<span class="hljs-number">163</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">24</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">12</span>
[<span class="hljs-number">164</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">24</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">12</span>
[<span class="hljs-number">165</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">25</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">12</span>
[<span class="hljs-number">166</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">25</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">13</span>
[<span class="hljs-number">167</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">25</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">13</span>
[<span class="hljs-number">168</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">25</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">13</span>
[<span class="hljs-number">169</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">25</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">13</span>
[<span class="hljs-number">170</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">25</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">13</span>
[<span class="hljs-number">171</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">25</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">13</span>
[<span class="hljs-number">172</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">25</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">13</span>
[<span class="hljs-number">173</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">25</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">13</span>
[<span class="hljs-number">174</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">26</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">13</span>
[<span class="hljs-number">175</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">26</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">14</span>
[<span class="hljs-number">176</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">26</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">14</span>
[<span class="hljs-number">177</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">26</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">14</span>
[<span class="hljs-number">178</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">26</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">14</span>
[<span class="hljs-number">179</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">26</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">14</span>
[<span class="hljs-number">180</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">26</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">14</span>
[<span class="hljs-number">181</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">26</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">14</span>
[<span class="hljs-number">182</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">26</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">14</span>
[<span class="hljs-number">183</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">27</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">14</span>
[<span class="hljs-number">184</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">27</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">15</span>
[<span class="hljs-number">185</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">27</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">15</span>
[<span class="hljs-number">186</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">27</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">15</span>
[<span class="hljs-number">187</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">27</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">15</span>
[<span class="hljs-number">188</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">27</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">15</span>
[<span class="hljs-number">189</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">27</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">15</span>
[<span class="hljs-number">190</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">27</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">15</span>
[<span class="hljs-number">191</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">27</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">15</span>
[<span class="hljs-number">192</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">28</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">15</span>
[<span class="hljs-number">193</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">28</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">16</span>
[<span class="hljs-number">194</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">28</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">16</span>
[<span class="hljs-number">195</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">28</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">16</span>
[<span class="hljs-number">196</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">28</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">16</span>
[<span class="hljs-number">197</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">28</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">16</span>
[<span class="hljs-number">198</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">28</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">16</span>
[<span class="hljs-number">199</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">28</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">16</span>
[<span class="hljs-number">200</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">28</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">16</span>
[<span class="hljs-number">201</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">29</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">16</span>
[<span class="hljs-number">202</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">29</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">17</span>
[<span class="hljs-number">203</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">29</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">17</span>
[<span class="hljs-number">204</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">29</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">17</span>
[<span class="hljs-number">205</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">29</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">17</span>
[<span class="hljs-number">206</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">29</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">17</span>
[<span class="hljs-number">207</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">29</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">17</span>
[<span class="hljs-number">208</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">29</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">17</span>
[<span class="hljs-number">209</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">29</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">17</span>
[<span class="hljs-number">210</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">3</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">17</span>
[<span class="hljs-number">211</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">3</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">18</span>
[<span class="hljs-number">212</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">3</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">18</span>
[<span class="hljs-number">213</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">3</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">18</span>
[<span class="hljs-number">214</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">3</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">18</span>
[<span class="hljs-number">215</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">3</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">18</span>
[<span class="hljs-number">216</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">3</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">18</span>
[<span class="hljs-number">217</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">3</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">18</span>
[<span class="hljs-number">218</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">3</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">18</span>
[<span class="hljs-number">219</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">30</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">18</span>
[<span class="hljs-number">220</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">30</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">19</span>
[<span class="hljs-number">221</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">30</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">19</span>
[<span class="hljs-number">222</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">30</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">19</span>
[<span class="hljs-number">223</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">30</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">19</span>
[<span class="hljs-number">224</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">30</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">19</span>
[<span class="hljs-number">225</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">30</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">19</span>
[<span class="hljs-number">226</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">30</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">19</span>
[<span class="hljs-number">227</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">30</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">19</span>
[<span class="hljs-number">228</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">31</span>.attn_norm.weight                | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">19</span>
[<span class="hljs-number">229</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">31</span>.ffn_down.weight                 | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">20</span>
[<span class="hljs-number">230</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">31</span>.ffn_gate.weight                 | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">20</span>
[<span class="hljs-number">231</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">31</span>.ffn_up.weight                   | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">20</span>
[<span class="hljs-number">232</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">31</span>.ffn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">20</span>
[<span class="hljs-number">233</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">31</span>.attn_k.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">20</span>
[<span class="hljs-number">234</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">31</span>.attn_output.weight              | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">20</span>
[<span class="hljs-number">235</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">31</span>.attn_q.weight                   | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">20</span>
[<span class="hljs-number">236</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">31</span>.attn_v.weight                   | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">20</span>
[<span class="hljs-number">237</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">4</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">20</span>
[<span class="hljs-number">238</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">4</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">21</span>
[<span class="hljs-number">239</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">4</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">21</span>
[<span class="hljs-number">240</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">4</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">21</span>
[<span class="hljs-number">241</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">4</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">21</span>
[<span class="hljs-number">242</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">4</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">21</span>
[<span class="hljs-number">243</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">4</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">21</span>
[<span class="hljs-number">244</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">4</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">21</span>
[<span class="hljs-number">245</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">4</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">21</span>
[<span class="hljs-number">246</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">5</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">21</span>
[<span class="hljs-number">247</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">5</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">22</span>
[<span class="hljs-number">248</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">5</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">22</span>
[<span class="hljs-number">249</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">5</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">22</span>
[<span class="hljs-number">250</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">5</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">22</span>
[<span class="hljs-number">251</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">5</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">22</span>
[<span class="hljs-number">252</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">5</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">22</span>
[<span class="hljs-number">253</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">5</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">22</span>
[<span class="hljs-number">254</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">5</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">22</span>
[<span class="hljs-number">255</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">6</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">22</span>
[<span class="hljs-number">256</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">6</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">23</span>
[<span class="hljs-number">257</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">6</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">23</span>
[<span class="hljs-number">258</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">6</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">23</span>
[<span class="hljs-number">259</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">6</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">23</span>
[<span class="hljs-number">260</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">6</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">23</span>
[<span class="hljs-number">261</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">6</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">23</span>
[<span class="hljs-number">262</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">6</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">23</span>
[<span class="hljs-number">263</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">6</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">23</span>
[<span class="hljs-number">264</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">7</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">23</span>
[<span class="hljs-number">265</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">7</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">24</span>
[<span class="hljs-number">266</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">7</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">24</span>
[<span class="hljs-number">267</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">7</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">24</span>
[<span class="hljs-number">268</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">7</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">24</span>
[<span class="hljs-number">269</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">7</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">24</span>
[<span class="hljs-number">270</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">7</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">24</span>
[<span class="hljs-number">271</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">7</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">24</span>
[<span class="hljs-number">272</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">7</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">24</span>
[<span class="hljs-number">273</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">8</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">24</span>
[<span class="hljs-number">274</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">8</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">25</span>
[<span class="hljs-number">275</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">8</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">25</span>
[<span class="hljs-number">276</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">8</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">25</span>
[<span class="hljs-number">277</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">8</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">25</span>
[<span class="hljs-number">278</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">8</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">25</span>
[<span class="hljs-number">279</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">8</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">25</span>
[<span class="hljs-number">280</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">8</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">25</span>
[<span class="hljs-number">281</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">8</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">25</span>
[<span class="hljs-number">282</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">9</span>.attn_norm.weight                 | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">25</span>
[<span class="hljs-number">283</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">9</span>.ffn_down.weight                  | size   <span class="hljs-number">4096</span> x  <span class="hljs-number">14336</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">26</span>
[<span class="hljs-number">284</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">9</span>.ffn_gate.weight                  | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">26</span>
[<span class="hljs-number">285</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">9</span>.ffn_up.weight                    | size  <span class="hljs-number">14336</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">26</span>
[<span class="hljs-number">286</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">9</span>.ffn_norm.weight                  | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">26</span>
[<span class="hljs-number">287</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">9</span>.attn_k.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">26</span>
[<span class="hljs-number">288</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">9</span>.attn_output.weight               | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">26</span>
[<span class="hljs-number">289</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">9</span>.attn_q.weight                    | size   <span class="hljs-number">4096</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">26</span>
[<span class="hljs-number">290</span>/<span class="hljs-number">291</span>] Writing tensor blk.<span class="hljs-number">9</span>.attn_v.weight                    | size   <span class="hljs-number">1024</span> x   <span class="hljs-number">4096</span>  | <span class="hljs-built_in">type</span> F16  | T+  <span class="hljs-number">26</span>
[<span class="hljs-number">291</span>/<span class="hljs-number">291</span>] Writing tensor output_norm.weight                     | size   <span class="hljs-number">4096</span>           | <span class="hljs-built_in">type</span> F32  | T+  <span class="hljs-number">26</span>
Wrote instructlab-merlinite-<span class="hljs-number">7</span>b-lab-trained/instructlab-merlinite-<span class="hljs-number">7</span>b-lab.gguf
INFO <span class="hljs-number">2024</span>-<span class="hljs-number">06</span>-<span class="hljs-number">20</span> <span class="hljs-number">18</span>:<span class="hljs-number">50</span>:<span class="hljs-number">54</span>,<span class="hljs-number">938</span> lab.py:<span class="hljs-number">1362</span> deleting safetensors files from instructlab-merlinite-<span class="hljs-number">7</span>b-lab-trained...
main: build = <span class="hljs-number">1</span> (<span class="hljs-number">784</span>e11d)
main: built with Apple clang version <span class="hljs-number">15.0</span>.<span class="hljs-number">0</span> (clang-<span class="hljs-number">1500.0</span>.<span class="hljs-number">40.1</span>) <span class="hljs-keyword">for</span> arm64-apple-darwin23.<span class="hljs-number">4.0</span>
main: quantizing <span class="hljs-string">'instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab.gguf'</span> to <span class="hljs-string">'instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab-Q4_K_M.gguf'</span> as Q4_K_M
llama_model_loader: loaded meta data with <span class="hljs-number">23</span> key-value pairs and <span class="hljs-number">291</span> tensors from instructlab-merlinite-<span class="hljs-number">7</span>b-lab-trained/instructlab-merlinite-<span class="hljs-number">7</span>b-lab.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides <span class="hljs-keyword">do</span> not apply <span class="hljs-keyword">in</span> this output.
llama_model_loader: - kv   <span class="hljs-number">0</span>:                       general.architecture str              = llama
llama_model_loader: - kv   <span class="hljs-number">1</span>:                               general.name str              = .
llama_model_loader: - kv   <span class="hljs-number">2</span>:                       llama.context_length u32              = <span class="hljs-number">32768</span>
llama_model_loader: - kv   <span class="hljs-number">3</span>:                     llama.embedding_length u32              = <span class="hljs-number">4096</span>
llama_model_loader: - kv   <span class="hljs-number">4</span>:                          llama.block_count u32              = <span class="hljs-number">32</span>
llama_model_loader: - kv   <span class="hljs-number">5</span>:                  llama.feed_forward_length u32              = <span class="hljs-number">14336</span>
llama_model_loader: - kv   <span class="hljs-number">6</span>:                 llama.rope.dimension_count u32              = <span class="hljs-number">128</span>
llama_model_loader: - kv   <span class="hljs-number">7</span>:                 llama.attention.head_count u32              = <span class="hljs-number">32</span>
llama_model_loader: - kv   <span class="hljs-number">8</span>:              llama.attention.head_count_kv u32              = <span class="hljs-number">8</span>
llama_model_loader: - kv   <span class="hljs-number">9</span>:     llama.attention.layer_norm_rms_epsilon f32              = <span class="hljs-number">0.000010</span>
llama_model_loader: - kv  <span class="hljs-number">10</span>:                       llama.rope.freq_base f32              = <span class="hljs-number">10000.000000</span>
llama_model_loader: - kv  <span class="hljs-number">11</span>:                          general.file_<span class="hljs-built_in">type</span> u32              = <span class="hljs-number">1</span>
llama_model_loader: - kv  <span class="hljs-number">12</span>:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  <span class="hljs-number">13</span>:                      tokenizer.ggml.tokens arr[str,<span class="hljs-number">32008</span>]   = [<span class="hljs-string">"&lt;unk&gt;"</span>, <span class="hljs-string">"&lt;s&gt;"</span>, <span class="hljs-string">"&lt;/s&gt;"</span>, <span class="hljs-string">"&lt;0x00&gt;"</span>, <span class="hljs-string">"&lt;...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32008]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32008]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32001
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 735584 bytes
[   1/ 291]                        output.weight - [ 4096, 32008,     1,     1], type =    f16, converting to q6_K .. size =   250.06 MiB -&gt;   102.56 MiB
[   2/ 291]                    token_embd.weight - [ 4096, 32008,     1,     1], type =    f16, converting to q4_K .. size =   250.06 MiB -&gt;    70.33 MiB
[   3/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[   4/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[   5/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[   6/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[   7/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[   8/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[   9/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  10/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  11/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[  12/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  13/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[  14/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  15/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  16/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  17/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  18/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  19/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  20/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[  21/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  22/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[  23/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  24/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  25/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  26/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  27/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  28/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  29/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[  30/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  31/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[  32/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  33/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  34/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  35/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  36/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  37/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  38/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[  39/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  40/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  41/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  42/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  43/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  44/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  45/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  46/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  47/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  48/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  49/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  50/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  51/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  52/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  53/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  54/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  55/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  56/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  57/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  58/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[  59/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  60/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  61/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  62/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  63/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  64/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  65/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[  66/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  67/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  68/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  69/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  70/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  71/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  72/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  73/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  74/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  75/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  76/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  77/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  78/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  79/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  80/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  81/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  82/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  83/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  84/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  85/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[  86/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  87/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  88/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  89/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  90/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  91/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[  92/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[  93/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  94/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  95/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  96/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[  97/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  98/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[  99/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 100/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 101/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 102/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 103/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 104/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 105/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 106/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 107/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 108/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 109/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 110/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 111/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 112/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 113/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 114/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 115/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 116/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 117/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 118/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 119/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 120/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 121/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 122/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 123/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 124/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 125/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 126/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 127/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 128/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 129/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 130/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 131/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 132/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 133/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 134/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 135/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 136/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 137/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 138/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 139/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 140/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 141/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 142/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 143/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 144/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 145/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 146/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 147/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 148/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 149/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 150/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 151/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 152/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 153/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 154/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 155/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 156/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 157/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 158/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 159/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 160/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 161/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 162/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 163/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 164/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 165/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 166/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 167/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 168/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 169/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 170/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 171/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 172/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 173/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 174/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 175/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 176/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 177/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 178/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 179/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 180/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 181/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 182/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 183/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 184/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 185/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 186/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 187/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 188/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 189/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 190/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 191/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 192/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 193/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 194/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 195/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 196/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 197/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 198/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 199/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 200/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 201/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 202/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 203/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 204/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 205/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 206/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 207/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 208/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 209/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 210/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 211/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 212/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 213/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 214/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 215/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 216/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 217/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 218/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 219/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 220/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 221/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 222/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 223/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 224/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 225/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 226/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 227/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 228/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 229/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 230/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 231/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 232/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 233/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 234/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 235/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 236/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 237/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 238/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 239/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 240/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 241/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 242/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 243/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 244/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 245/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 246/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 247/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 248/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 249/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 250/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 251/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 252/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 253/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 254/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 255/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 256/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 257/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 258/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 259/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 260/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 261/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 262/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 263/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 264/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 265/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 266/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 267/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 268/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 269/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 270/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 271/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 272/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 273/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 274/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 275/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 276/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 277/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 278/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 279/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 280/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 281/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 282/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 283/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB -&gt;    45.94 MiB
[ 284/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 285/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB -&gt;    31.50 MiB
[ 286/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 287/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB -&gt;     2.25 MiB
[ 288/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 289/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB -&gt;     9.00 MiB
[ 290/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB -&gt;     3.28 MiB
[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
llama_model_quantize_internal: model size  = 13813.14 MB
llama_model_quantize_internal: quant size  =  4165.41 MB
INFO 2024-06-20 18:51:53,586 lab.py:1372 deleting instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab.gguf...
</span></code></pre>
<p class="has-line-data" data-line-start="1482" data-line-end="1483">运行该命令后，所有权重和适配器都将转换为量化的 gguf 模型。目录中将创建一个目录<code>ilab</code>，其名称类似于：<code>instructlab-merlinite-7b-lab-trained</code>。</p>
<p class="has-line-data" data-line-start="1484" data-line-end="1486">&lt;aside&gt;<br>
💡 <code>Llama.cpp</code> 的主要优势在于其静态分析和代码生成功能。将深度学习模型量化，并且在低配置机型（没有GPU）上就可以做到模型推理。合并权重，生成全量模型权重。</p>
<p class="has-line-data" data-line-start="1487" data-line-end="1488">&lt;/aside&gt;</p>
<h3 class="code-line" data-line-start="1489" data-line-end="1490"><a id="_1489"></a><strong>验证测试训练好的模型</strong></h3>
<p class="has-line-data" data-line-start="1491" data-line-end="1492">运行新训练的模型</p>
<p class="has-line-data" data-line-start="1493" data-line-end="1494"><code>ilab serve --model-path instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab-Q4_K_M.gguf --num-threads 14 --max-ctx-size 20480</code></p>
<p class="has-line-data" data-line-start="1495" data-line-end="1496">在生成合成数据集、训练模型、测试模型的第二个终端中，运行以下命令与模型聊天：</p>
<p class="has-line-data" data-line-start="1497" data-line-end="1498"><code>ilab chat -gm -m instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab-Q4_K_M.gguf</code></p>
<p class="has-line-data" data-line-start="1499" data-line-end="1501">&lt;aside&gt;<br>
💡 Greedy decoding 模型每一步都选择具有最高概率的词作为输出，而不考虑整个句子的全局最优性。在每一步生成时，模型根据当前输入和上下文计算每个可能词的概率，然后选择概率最高的词作为当前步的输出。计算速度很快，不需要复杂的搜索算法，因此在实际应用中能快速生成文本，但是通常缺乏多样性，可能会产生重复或单调的句子。只考虑局部最优，可能会错过一些更好的句子结构或上下文，导致生成的文本在全局上不是最优的。总的来说，greedy decoding 是一种简单快速但可能不完美的文本生成策略，适用于对生成速度有较高要求但对文本多样性要求不高的应用场景。</p>
<p class="has-line-data" data-line-start="1502" data-line-end="1503">&lt;/aside&gt;</p>
<hr>
<p class="has-line-data" data-line-start="1506" data-line-end="1508"><strong>在没有生成新的训练数据集之前，用下面的问题测试，返回出的答案，可以看出来信息不正确，不能达到预期</strong><br>
<code>ilab chat -m models/granite-7b-lab-Q4_K_M.gguf</code></p>
<p class="has-line-data" data-line-start="1509" data-line-end="1510"><strong>Who starred in the movie Oppenheimer?</strong></p>
<p class="has-line-data" data-line-start="1511" data-line-end="1512">The movie “Oppenheimer” is <strong>an upcoming film</strong> adaptation of the Pulitzer Prize-winning biography “American Prometheus: The Life of J. Robert Oppenheimer” by Kira Davis. Cillian Murphy has been cast as the title character, J. Robert Oppenheimer. Other notable stars who have joined the cast include Matt Damon as Leslie Groves, the head of the Yale project and the father of Oppenheimer’s wife, Emily Blunt as Katherine Manning Oppenheimer, the wife of J. Robert Oppenheimer, and Alden Ehrenreich as Donald Oppenheimer, their son.</p>
<p class="has-line-data" data-line-start="1513" data-line-end="1514">The film is directed by Christopher Nolan, known for his work on blockbuster movies like “Batman Begins,” “The Dark Knight,” and “Interstellar.” “Oppenheimer” is set to explore the life of J. Robert Oppenheimer, the man who led the project that developed the first atomic bomb during World War II. The movie is expected to delve into the moral complexities of nuclear power and the human cost of scientific progress.</p>
<p class="has-line-data" data-line-start="1515" data-line-end="1516">While there isn’t much information available about the plot yet, I can provide you with some context on the historical figures and events that “Oppenheimer” might depict. The film will likely cover Oppenheimer’s early life, his education at the prestigious Institute for Advanced Studies in Princeton, New Jersey, and his work at the Los Alamos National Laboratory during the Manhattan Project. Additionally, the movie may explore Oppenheimer’s struggles with the moral implications of nuclear weapons and his eventual condemnation by the U.S. government during the Red Scare.</p>
<p class="has-line-data" data-line-start="1517" data-line-end="1518"><strong>Stay tuned for more updates</strong> on “Oppenheimer” as filming progresses and <strong>more information becomes available</strong>. If you have any questions about the cast, directors, or historical context of the movie, please let me know!</p>
<p class="has-line-data" data-line-start="1519" data-line-end="1520"><strong>What are the release dates for Oppenheimer movie?</strong></p>
<p class="has-line-data" data-line-start="1521" data-line-end="1522">As of my last update, the release date for “Oppenheimer” <strong>is not yet set</strong>. The film was announced in 2021, and production has recently started. Christopher Nolan, the director, often takes his time to perfect his films before releasing them. Given the complexity of the subject matter and the intricate plot, it’s likely that “Oppenheimer” will require a considerable amount of post-production work.</p>
<p class="has-line-data" data-line-start="1523" data-line-end="1524"><strong>Once filming is complete and the movie is ready for release</strong>, the production team will announce an official date. Typically, this happens several months before the release date. I recommend checking official film news sources and the film’s official website for the most up-to-date information on the release date.</p>
<p class="has-line-data" data-line-start="1525" data-line-end="1526">In the meantime, you can look forward to other movies released by Christopher Nolan or explore other fascinating historical figures and events that have been brought to the big screen. For example, “Dunkirk” (2017) and “Interstellar” (2014) are two of Nolan’s previous films that showcase his ability to create immersive and thought-provoking cinematic experiences.</p>
<p class="has-line-data" data-line-start="1527" data-line-end="1529"><strong>在加入了knowledge和上下文参考数据，并且使用训练的新模型之后，答案基本可以达到预期</strong><br>
<code>ilab chat -gm -m instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab-Q4_K_M.gguf</code></p>
<p class="has-line-data" data-line-start="1530" data-line-end="1531"><strong>Who starred in the movie Oppenheimer?</strong></p>
<p class="has-line-data" data-line-start="1532" data-line-end="1534">Sean Penn portrayed J. Robert Oppenheimer in the movie “Oppenheimer.” He was nominated for an Academy Award │<br>
│ for Best Actor for his performance.</p>
<p class="has-line-data" data-line-start="1535" data-line-end="1536"><strong>What are the release dates for Oppenheimer movie?</strong></p>
<p class="has-line-data" data-line-start="1537" data-line-end="1538">The film Oppenheimer was released on March 17, 2023, in the United States after being shown at the Cannes Film Festival in 2022. It has been available to stream on HBO Max since its release.</p>
<pre><code class="has-line-data" data-line-start="1540" data-line-end="1556" class="language-bash">
╭─────────────────────────────────────────────────────────────────────────────────────────────── system ───────────────────────────────────────────────────────────────────────────────────────────────╮
│ Welcome to InstructLab Chat w/ INSTRUCTLAB-MERLINITE-<span class="hljs-number">7</span>B-LAB-TRAINED/INSTRUCTLAB-MERLINITE-<span class="hljs-number">7</span>B-LAB-Q4_K_M.GGUF (<span class="hljs-built_in">type</span> /h <span class="hljs-keyword">for</span> <span class="hljs-built_in">help</span>)                                                                      │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
&gt;&gt;&gt; Who starred <span class="hljs-keyword">in</span> the movie Oppenheimer?                                                                                                                                                   [S][default]
╭─────────────────────────────────────────────────────────── instructlab-merlinite-<span class="hljs-number">7</span>b-lab-trained/instructlab-merlinite-<span class="hljs-number">7</span>b-lab-Q4_K_M.gguf ────────────────────────────────────────────────────────────╮
│ Oppenheimer was written and directed by James Schamus, and it features Daniel Day-Lewis <span class="hljs-keyword">in</span> the title role. Other notable actors <span class="hljs-keyword">in</span> this film include Emily Blunt as Katherine <span class="hljs-string">"Kitty"</span> Oppenheimer,   │
│ Robert Wagner as General Leslie Groves, and Michael Nyqvist as Albert Einstein. This movie is a historical drama that depicts the life of J. Robert Oppenheimer during his time at Los Alamos, <span class="hljs-built_in">where</span> │
│ he led the development of the atomic bomb during World War II.                                                                                                                                       │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── elapsed <span class="hljs-number">2.466</span> seconds ─╯
&gt;&gt;&gt; What are the release dates <span class="hljs-keyword">for</span> Oppenheimer movie?                                                                                                                                       [S][default]
╭─────────────────────────────────────────────────────────── instructlab-merlinite-<span class="hljs-number">7</span>b-lab-trained/instructlab-merlinite-<span class="hljs-number">7</span>b-lab-Q4_K_M.gguf ────────────────────────────────────────────────────────────╮
│ The Oppenheimer movie was released <span class="hljs-keyword">in</span> <span class="hljs-number">2023</span>. It premiered on March <span class="hljs-number">14</span>th and started to stream on Hulu on April <span class="hljs-number">7</span>th.                                                                                   │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── elapsed <span class="hljs-number">0.829</span> seconds ─╯
&gt;&gt;&gt;
</code></pre>
<p class="has-line-data" data-line-start="1557" data-line-end="1558">参考文章：</p>
<p class="has-line-data" data-line-start="1559" data-line-end="1560"><a href="https://developer.ibm.com/tutorials/awb-train-open-source-llms-collected-knowledge-instructlab/">IBM Developer</a></p>
</body></html>