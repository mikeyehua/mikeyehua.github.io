# InstructLab

å¦‚ä»Šï¼Œå¤§è¯­è¨€æ¨¡å‹åœ¨å¾ˆå¤šåœºåˆè“¬å‹ƒå‘å±•ï¼Œæˆ‘ä»¬éƒ½è§è¯äº†äººå·¥æ™ºèƒ½ç»™ä¸–ç•Œå¸¦æ¥çš„é‡å¤§å½±å“ï¼Œå°¤å…¶æ˜¯*ChatGPT*å’Œå…¶ä»–å¤§å‹è¯­è¨€æ¨¡å‹çš„å‘å¸ƒï¼Œæˆ‘ä»¬éƒ½ç›¸ä¿¡å®ƒå°†åœ¨æœªæ¥å‡ å¹´æ”¹å˜æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ã€‚ä½†å¤§å¤šæ•°æ¨¡å‹ä»æ˜¯å„è‡ªä¸ºæ”¿ã€‚è™½ç„¶å¤§è¯­è¨€æ¨¡å‹ (LLM) å…·æœ‰å·¨å¤§çš„æ½œåŠ›ï¼Œä½†å®ƒä»¬ä¹Ÿé¢ä¸´ç€æŒ‘æˆ˜ã€‚ä½¿ç”¨ LLM éœ€è¦é«˜è´¨é‡çš„è®­ç»ƒæ•°æ®ã€ä¸“ä¸šæŠ€èƒ½å’ŒçŸ¥è¯†ä»¥åŠå¤§é‡çš„è®¡ç®—èµ„æºã€‚åˆ†å‰å’Œé‡æ–°è®­ç»ƒæ¨¡å‹çš„è¿‡ç¨‹ä¹Ÿå¾ˆè€—æ—¶ä¸”æˆæœ¬é«˜æ˜‚ã€‚å¼€æºç¤¾åŒºé€šå¸¸ä¸ºæ¨¡å‹çš„ç”Ÿæˆè´¡çŒ®æœ€å¤šï¼Œä½†ä»–ä»¬çš„è´¡çŒ®å¯èƒ½éœ€è¦æ•°æœˆæˆ–æ•°å¹´æ‰èƒ½åˆå¹¶å›åŸºç¡€æ¨¡å‹ ï¼ˆå¦‚æœä»–ä»¬èƒ½å›å½’çš„è¯ï¼‰ 


### ä¼ ç»Ÿçš„RAG ï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æµç¨‹

ä¼ ç»Ÿæ–¹å¼æ²¡æœ‰ç¤¾åŒºï¼Œæ²¡æœ‰åŠæ³•è´¡çŒ®ä»£ç ï¼Œä¹Ÿæ²¡æœ‰åŠæ³•ä¸°å¯Œæ•°æ®é›†

å½“å‰æœ‰è®¸å¤šé¡¹ç›®æ­£åœ¨é‡‡ç”¨å¼€æºçš„å¤§è¯­è¨€æ¨¡å‹ï¼Œä¾‹å¦‚Llamaã€Mixtralï¼Œä½†å®ƒä»¬é‡åˆ°äº†ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ï¼š

1. ç›´æ¥å¯¹LLMsè¿›è¡Œè´¡çŒ®ä¸å®¹æ˜“ã€‚é‡æ–°è®­ç»ƒæ–°çš„æ¨¡å‹ä¼šå¯¼è‡´åˆ†å‰ï¼Œè¿™è®©ä½¿ç”¨è€…ä¸å¾—ä¸åœ¨ä¸æ˜“æ‰©å±•çš„æ¨¡å‹ä¹‹é—´åšå‡ºé€‰æ‹©ã€‚å¯¹äºæ¨¡å‹åˆ›å»ºè€…æ¥è¯´ï¼Œç»´æŠ¤è¿™äº›åˆ†å‰çš„æˆæœ¬é«˜ä¸”å›°éš¾ã€‚
2. å¯¹äºè´¡çŒ®æƒ³æ³•éœ€è¦å…·å¤‡äººå·¥æ™ºèƒ½/æœºå™¨å­¦ä¹ ä¸“ä¸šçŸ¥è¯†çš„é™åˆ¶ã€‚ä¸€ä¸ªäººå¿…é¡»å­¦ä¹ å¦‚ä½•åˆ†å‰ã€è®­ç»ƒå’Œä¼˜åŒ–æ¨¡å‹æ‰èƒ½å®ç°è‡ªå·±çš„æƒ³æ³•ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜é—¨æ§›çš„è¦æ±‚ã€‚
3. ç¼ºä¹æœ‰å…³åˆ†å‰æ¨¡å‹çš„ç¤¾åŒºæ²»ç†æˆ–æœ€ä½³å®è·µçš„æŒ‡å¯¼ã€å®¡æŸ¥å’Œåˆ†å‘ã€‚

ä¾‹å¦‚ï¼š*HuggingFace*ä¸Šå‘å¸ƒçš„è®¸å¤š*LLM*ä»…åŒ…å«ç”¨äºæ¨ç†çš„æˆæœ â€” â€” å®ƒä»¬å‘¨å›´æ²¡æœ‰ç¤¾åŒºï¼Œæ²¡æœ‰åŠæ³•è´¡çŒ®ä»£ç ï¼Œä¹Ÿæ²¡æœ‰åŠæ³•ä¸°å¯Œæ•°æ®é›†ã€‚

æ‰€éœ€çš„æ¯ä¸ªå¾®è°ƒéƒ½**å®Œå…¨ä¾èµ–äºç”¨æˆ·çš„è´£ä»»**ã€‚


å¦‚æœä½ ä»”ç»†æƒ³æƒ³ï¼Œä½ ä¼šå‘ç°å®ƒæå¤§åœ°å‡ç¼“äº†åˆ›æ–°ã€‚å¼€æºç»™è½¯ä»¶è¡Œä¸šå¸¦æ¥çš„çœŸæ­£æ–‡åŒ–è½¬å˜ä¹‹ä¸€æ˜¯èƒ½å¤Ÿ**å›´ç»•**å¼€æºé¡¹ç›®è¿›è¡Œåä½œï¼Œä»è€Œåˆ›é€ å‡ºæ›´å¥½çš„è§£å†³æ–¹æ¡ˆå’Œäº§å“ã€‚å…¬å¸å’Œä¸ªäººå¯ä»¥å…±äº«å’Œè´¡çŒ®ä»£ç ã€ä¿®å¤é”™è¯¯ã€æ„å»ºæ–°åŠŸèƒ½ï¼Œä½œä¸ºä¸€ä¸ªæ‹¥æœ‰å…±åŒç›®æ ‡çš„ç¤¾åŒºçš„ä¸€éƒ¨åˆ†â€”â€”**æŒç»­æ”¹è¿›**ã€‚

äººå·¥æ™ºèƒ½çš„ç”Ÿå‘½å‘¨æœŸå˜å¾—ä¸ä¼ ç»Ÿè½¯ä»¶éå¸¸ç›¸ä¼¼â€”â€”å®ƒæ˜¯ä½¿ç”¨å·²çŸ¥çš„ç¼–ç¨‹è¯­è¨€å’Œæ¡†æ¶å¼€å‘çš„ï¼Œå®ƒå¯ä»¥è¢«æ‰“åŒ…å¹¶æ„å»ºåˆ°å®¹å™¨ä¸­ï¼Œæˆ‘ä»¬æµ‹è¯•å®ƒï¼Œç›‘æ§å®ƒï¼Œéƒ¨ç½²å®ƒã€‚

![ä¼ ç»Ÿæ–¹å¼æ²¡æœ‰ç¤¾åŒºï¼Œæ²¡æœ‰åŠæ³•è´¡çŒ®ä»£ç ï¼Œä¹Ÿæ²¡æœ‰åŠæ³•ä¸°å¯Œæ•°æ®é›†]

ä¼ ç»Ÿæ–¹å¼æ²¡æœ‰ç¤¾åŒºï¼Œæ²¡æœ‰åŠæ³•è´¡çŒ®ä»£ç ï¼Œä¹Ÿæ²¡æœ‰åŠæ³•ä¸°å¯Œæ•°æ®é›†

é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•åˆ©ç”¨å·²ç»åšçš„äº‹æƒ…æ¥åˆ›å»ºäººå·¥æ™ºèƒ½é¢†åŸŸçš„ç¤¾åŒºå‘¢ï¼Ÿ

æˆ‘ä»¬å¦‚ä½•åˆä½œã€è´¡çŒ®çŸ¥è¯†å’Œå…±äº«æ•°æ®é›†ä»¥è·å¾—æ›´å¥½ã€æ›´å‡†ç¡®çš„äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Ÿ

[InstructLab](https://instructlab.ai/)

## **InstructLab æ¡†æ¶**

![InstructLab ç¤¾åŒºæ¨¡å‹å°†ä½¿ç”¨æœ€æ–°è´¡çŒ®è¿›è¡Œæ›´æ–°ï¼Œå¹¶å®šæœŸåœ¨ Hugging Face ä¸Šåˆ†äº«ã€‚]

InstructLab ç¤¾åŒºæ¨¡å‹å°†ä½¿ç”¨æœ€æ–°è´¡çŒ®è¿›è¡Œæ›´æ–°ï¼Œå¹¶å®šæœŸåœ¨ Hugging Face ä¸Šåˆ†äº«ã€‚

InstructLab é¡¹ç›®æ›´åƒä»»ä½•å…¶ä»–å¼€æºè½¯ä»¶é¡¹ç›®ï¼Œæä¾›äº†ä¸€ç§å¼€æºçš„ç”Ÿæˆå¼ AI æ–¹æ³•ï¼Œå®ƒä¸ºç¤¾åŒºæä¾›äº†åˆ›å»ºå’Œåˆå¹¶ LLM æ›´æ”¹çš„å·¥å…·ï¼Œæ”¯æŒå®šæœŸæ„å»ºå¹¶ä¸”å¢å¼ºå·²æœ‰çš„é¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ï¼Œè€Œæ— éœ€ä»å¤´å¼€å§‹é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚è¿™ç§æ–¹æ³•ä¸ä»…é™ä½æˆæœ¬ã€æ¶ˆé™¤æµ‹è¯•å’Œå®éªŒéšœç¢ï¼Œå¹¶ä¸”ä¿è¯äº†ä¸€è‡´æ€§ â€” å³ç¡®ä¿æ¨¡å‹çš„ç­”æ¡ˆå‡†ç¡®ã€å…¬æ­£ä¸”ç¬¦åˆå…¶ç”¨æˆ·å’Œåˆ›å»ºè€…çš„ç›®æ ‡ã€‚

InstructLab çš„å·¥ä½œåŸç†æ˜¯åˆ©ç”¨ LLM ç”Ÿæˆçš„é«˜è´¨é‡ç¤ºä¾‹æ¥å¢å¼ºäººå·¥æ•´ç†çš„æ•°æ®ï¼Œä»è€Œé™ä½æ•°æ®åˆ›å»ºæˆæœ¬ã€‚ç„¶åå¯ä»¥ä½¿ç”¨ InstructLab ç”Ÿæˆçš„æ•°æ®æ¥å®šåˆ¶æˆ–æ”¹è¿›åŸºç¡€æ¨¡å‹ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒå®ƒï¼Œä»è€ŒèŠ‚çœæ›´å¤šæˆæœ¬ã€‚IBM Research å·²ä½¿ç”¨ InstructLab ç”Ÿæˆåˆæˆæ•°æ®ï¼Œä»¥æ”¹è¿›å…¶ç”¨äºè¯­è¨€å’Œ[ä»£ç çš„å¼€æº](https://research.ibm.com/blog/granite-code-models-open-source)[Granite æ¨¡å‹](https://www.ibm.com/blog/building-ai-for-business-ibms-granite-foundation-models/)ã€‚

å®ƒæä¾›**å•ä¸€å·¥å…·**æ¥ä¸‹è½½ã€æä¾›ã€æµ‹è¯•å’Œè®­ç»ƒ*LLM*ï¼Œä»¥ä¾¿ä»»ä½•äººéƒ½å¯ä»¥è´¡çŒ®å’Œæ”¹è¿›ç°æœ‰åŠŸèƒ½â€”â€”æ— è®ºæ˜¯å†…éƒ¨è¿˜æ˜¯å¤–éƒ¨ï¼Œé¢å‘æ›´å¹¿æ³›çš„ç¤¾åŒºã€‚

InstructLab ä½¿ç¤¾åŒºè´¡çŒ®è€…èƒ½å¤Ÿå‘ç‰¹å®šæ¨¡å‹**æ·»åŠ é¢å¤–çš„â€œæŠ€èƒ½â€æˆ–â€œçŸ¥è¯†â€**

æŠ€èƒ½å’ŒçŸ¥è¯†çš„åˆ†ç±»æœ‰åŠ©äºè¯†åˆ«æ‰€éœ€èƒ½åŠ›çš„å·®è·ï¼Œç„¶ååœ¨**åˆæˆæ•°æ®ä¸­**ç”Ÿæˆè¶³å¤Ÿçš„å¤šæ ·æ€§ä»¥æœ‰æ•ˆåœ°è°ƒæ•´åŸºç¡€æ¨¡å‹ã€‚å¯ä»¥å°†InstructLabè§†ä¸ºä¸€ä¸ªè¯•éªŒå¨æˆ¿ï¼Œç”¨äºå°è¯•å’Œæäº¤ç”¨äºç”Ÿæˆåˆæˆæ•°æ®çš„æ–°â€œé…æ–¹â€ï¼Œä»¥æ•™æˆ LLM æ–°çŸ¥è¯†å’ŒæŠ€èƒ½ã€‚

**é€šè¿‡åˆ†ç±»æ³•**ï¼Œ[LAB](https://research.ibm.com/blog/LLM-generated-data)å¯ä»¥åˆ›å»ºä¸æ‚¨æƒ³è¦æ·»åŠ åˆ°æ¨¡å‹ä¸­çš„ä»»åŠ¡ç›¸å¯¹åº”çš„é«˜è´¨é‡æ•°æ®ã€‚åˆ†ç±»æ³•æ˜¯è¿„ä»Šä¸ºæ­¢åœ¨ InstructLab æ•°æ®ä¸Šè°ƒä¼˜çš„ LLM æ‰€å­¦åˆ°çš„çŸ¥è¯†çš„å±‚æ¬¡ç»“æ„å›¾ï¼Œå¯è½»æ¾è¯†åˆ«å’Œå¡«è¡¥æ¼æ´ã€‚

InstructLab çš„è®­ç»ƒæ–¹æ¡ˆå°†æ–°ä¿¡æ¯å¸æ”¶åˆ°æ¨¡å‹ä¸­ï¼Œè€Œä¸ä¼šå¯¼è‡´æ¨¡å‹è¦†ç›–ä¹‹å‰å­¦åˆ°çš„å†…å®¹ã€‚åŸºç¡€æ¨¡å‹åœ¨æ¼«é•¿çš„é¢„è®­ç»ƒé˜¶æ®µæ³¨å…¥äº†æ ¸å¿ƒçŸ¥è¯†å’Œèƒ½åŠ›ã€‚å¦‚æœéœ€è¦è¿›è¡Œå®è´¨æ€§æ”¹è¿›ï¼Œåˆ™å¿…é¡»é‡æ–°è®­ç»ƒé¢„å…ˆè®­ç»ƒçš„åŸºç¡€æ¨¡å‹ã€‚

**è¿™ä¸ªé¡¹ç›®è®©ä¸æ‡‚[transformeræ¨¡å‹](https://www.zhihu.com/search?q=transformer%E6%A8%A1%E5%9E%8B&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%22699500296%22%7D), ä¸æ‡‚LLMçš„äºº, ä¹Ÿèƒ½è®­ç»ƒè‡ªå·±çš„æ¨¡å‹ã€‚**å…±åŒæ„å»ºæ›´å¥½çš„æ¨¡å‹ï¼Œå°±åƒå‚ä¸å¼€æºè½¯ä»¶é¡¹ç›®ä¸€æ ·ã€‚

![**L**arge-scaleÂ **A**lignment for chat**B**ots]

**L**arge-scaleÂ **A**lignment for chat**B**ots

---

### **Install with Apple Metal on M1/M2/M3 Macs**

```bash
python3 -m venv --upgrade-deps venv
source venv/bin/activate
pip cache remove llama_cpp_python
pip install instructlab
```

```bash
/Users/yehua/instructlab

~/instructlab  source venv/bin/activate
(venv)  ~/instructlab  ilab chat
/Users/yehua/instructlab/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ system â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Welcome to InstructLab Chat w/ MODELS/MERLINITE-7B-LAB-Q4_K_M.GGUF (type /h for help)                                                                                  â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
>>>                                                                                                                                                           [S][default]
```

### ä¸‹è½½ IBM Granite æ¨¡å‹

å…ˆå‡†å¤‡å¥½ HF Token = `xxxxxxxxxxxxxxxxxx`

https://huggingface.co/settings/tokens

å¦‚æœéœ€è¦ï¼Œè®¾ç½®HF Token

```bash
HF_TOKEN=<YOUR HUGGINGFACE TOKEN GOES HERE> ilab download --repository=TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF --filename=mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf
```

ä¸‹è½½å…¶ä»–model

```bash
$ ilab download --repository instructlab/granite-7b-lab-GGUF --filename granite-7b-lab-Q4_K_M.gguf #IBM Granite
$ ilab download --repository QuantFactory/Qwen2-7B-Instruct-deccp-GGUF --filename Qwen2-7B-Instruct-deccp.Q4_K_M.gguf #é€šä¹‰åƒé—®
```

æ ¹æ®CPUæ ¸å¿ƒï¼Œè°ƒæ•´çº¿ç¨‹æ•°

```bash
$ ilab serve --model-path models/granite-7b-lab-Q4_K_M.gguf --num-threads 14 --max-ctx-size 20480
$ ilab chat -m models/granite-7b-lab-Q4_K_M.gguf
```

### æ·»åŠ  Knowledge åˆ° taxonomies

The LAB method is driven by [taxonomies](https://github.com/instructlab/taxonomy), which are largely created manually and with care.

`qna.yaml`

```yaml
version: 2
task_description: <string>
created_by: <string>
seed_examples: #éœ€è¦æä¾›è‡³å°‘ä¸¤ä¸ªç›¸å…³çš„é—®ç­”ä½œä¸ºç§å­
  - question: <string>
    answer: |
      <multi-line string>
  - context: |
      <multi-line string>
    question: <string>
    answer: |
      <multi-line string>
  ...
```

**è®­ç»ƒæ•°æ®**

```yaml
version: 2
task_description: Teach the Large Language Model about the movie Oppenheimer
created_by: IBM Ecosystems Engineering SI Lab
domain: movie
seed_examples:
- answer: |
    The movie â€œOppenheimerâ€ was written, directed, and produced by Christopher Nolan1.
  question: Who directed the movie â€œOppenheimerâ€?
- answer: |
    The movie follows the life of J. Robert Oppenheimer, the American theoretical physicist who helped develop the first nuclear weapons during World War II.
  question: What is the movie â€œOppenheimerâ€ about?
- answer: |
    The movie starred Cillian Murphy as Oppenheimer, alongside Robert Downey Jr. as the United States Atomic Energy Commission member Lewis Strauss. The ensemble supporting cast includes Emily Blunt, Matt Damon, Florence Pugh, Josh Hartnett, Casey Affleck, Rami Malek, and Kenneth Branagh
  question: Who starred in the movie â€œOppenheimerâ€?
- answer: |
    The movie â€œOppenheimerâ€ was released on July 21, 2023
  question: When was the movie â€œOppenheimerâ€ released?
- answer: |
    The movie â€œOppenheimerâ€ received critical acclaim and won seven Academy Awards, including Best Picture, Best Director for Nolan, Best Actor for Murphy and Best Supporting Actor for Downey. It grossed over $976 million worldwide, becoming the third-highest-grossing film of 2023, the highest-grossing World War II-related film, the highest-grossing biographical film and the second-highest-grossing R-rated film
  question: How was the movie â€œOppenheimerâ€ received?
document:
  repo: https://github.com/mikeyehua/instructlab.git
  commit: 76a2773
  patterns:
  - Oppenheimer.md
```

`æŠŠä¸Šé¢çš„ é—®ç­”æ–‡ä»¶ qna.yamlå’Œ å…ƒæ•°æ®æ–‡ä»¶ attribution.txtæ”¾åœ¨ taxonomy/knowledge/movies/oppenheimer ç›®å½•ä¸‹`

åŸå§‹å‚è€ƒæ•°æ®æ¥æº`document`æ”¾åœ¨è¿™é‡Œï¼š

[instructlab/Oppenheimer.md at main Â· mikeyehua/instructlab](https://github.com/mikeyehua/instructlab/blob/main/Oppenheimer.md)

åŸæ–‡å‚è€ƒï¼šhttps://github.com/syedaameena/InstructLab-Skill/blob/main/QnA.yaml?utm_source=ibm_developer&utm_content=in_content_link&utm_id=tutorials_awb-tuning-pretrained-llms-instructlab

- `document`: The source of your knowledge contribution.
    - `repo`: The URL to your repository that holds your knowledge markdown files.
    - `commit`: The SHA of the commit in your repository with your knowledge markdown files.
    - `patterns`: A list of glob patterns specifying the markdown files in your repository. Any glob pattern that starts withÂ ``, such asÂ `.md`, must be quoted due to YAML rules. For example,Â `"*.md"`.

<aside>
ğŸ’¡ YAML æµ‹è¯•ç«™ï¼š

https://www.yamllint.com/

</aside>

**Knowledge: YAML examples**

<aside>
ğŸ’¡ **åœ¨æ²¡æœ‰ç”Ÿæˆæ–°çš„è®­ç»ƒæ•°æ®é›†ä¹‹å‰ï¼Œç”¨ä¸‹é¢çš„é—®é¢˜æµ‹è¯•ï¼Œè¿”å›å‡ºçš„ç­”æ¡ˆï¼Œå¯ä»¥çœ‹å‡ºæ¥ä¿¡æ¯ä¸æ­£ç¡®ï¼Œä¸èƒ½è¾¾åˆ°é¢„æœŸ**

> Who starred in the movie Oppenheimer?
> 

elapsed 9.513 seconds -
The movie "Oppenheimer" is **an upcoming film** adaptation of the Pulitzer Prize-winning biography "American Prometheus: The Life of J. Robert Oppenheimer" by Kira Davis. Cillian Murphy has been cast as the title character, J. Robert Oppenheimer. Other notable stars who have joined the cast include Matt Damon as Leslie Groves, the head of the Yale project and the father of Oppenheimer's wife, Emily Blunt as Katherine Manning Oppenheimer, the wife of J. Robert Oppenheimer, and Alden Ehrenreich as Donald Oppenheimer, their son.

The film is directed by Christopher Nolan, known for his work on blockbuster movies like "Batman Begins," "The Dark Knight," and "Interstellar." "Oppenheimer" is set to explore the life of J. Robert Oppenheimer, the man who led the project that developed the first atomic bomb during World War II. The movie is expected to delve into the moral complexities of nuclear power and the human cost of scientific progress.

While there isn't much information available about the plot yet, I can provide you with some context on the historical figures and events that "Oppenheimer" might depict. The film will likely cover Oppenheimer's early life, his education at the prestigious Institute for Advanced Studies in Princeton, New Jersey, and his work at the Los Alamos National Laboratory during the Manhattan Project. Additionally, the movie may explore Oppenheimer's struggles with the moral implications of nuclear weapons and his eventual condemnation by the U.S. government during the Red Scare.

**Stay tuned for more updates** on "Oppenheimer" as filming progresses and **more information becomes available**. If you have any questions about the cast, directors, or historical context of the movie, please let me know!

> What are the release dates for Oppenheimer movie?
> 

elapsed 5.935 seconds -
As of my last update, the release date for "Oppenheimer" **is not yet set**. The film was announced in 2021, and production has recently started. Christopher Nolan, the director, often takes his time to perfect his films before releasing them. Given the complexity of the subject matter and the intricate plot, it's likely that "Oppenheimer" will require a considerable amount of post-production work.

**Once filming is complete and the movie is ready for release**, the production team will announce an official date. Typically, this happens several months before the release date. I recommend checking official film news sources and the film's official website for the most up-to-date information on the release date.

In the meantime, you can look forward to other movies released by Christopher Nolan or explore other fascinating historical figures and events that have been brought to the big screen. For example, "Dunkirk" (2017) and "Interstellar" (2014) are two of Nolan's previous films that showcase his ability to create immersive and thought-provoking cinematic experiences.

</aside>

### éªŒè¯æ•°æ®

`ilab diff` é€šè¿‡è¿è¡Œå‘½ä»¤åˆ—å‡ºæ–°æ•°æ®å¹¶ç¡®ä¿å…¶åœ¨åˆ†ç±»è·¯å¾„ä¸­æ­£ç¡®æ³¨å†Œï¼Œä»è€Œåˆ—å‡ºå¹¶éªŒè¯æ–°çš„æ•°æ®ã€‚

### ç”Ÿæˆåˆæˆæ•°æ®é›†

é€šè¿‡è¿è¡Œ `ilab generate` å‘½ä»¤æ¥æ ¹æ®æˆ‘ä»¬è¾“å…¥çš„é—®ç­”ç”Ÿæˆæ›´å¹¿æ³›çš„æ•°æ®é›†ï¼Œè¯¥å‘½ä»¤æ ¹æ® taxonomies å­˜å‚¨åº“ä¸­æ–°æ·»åŠ çš„  knowledge ç”Ÿæˆåˆæˆæ•°æ®é›†ã€‚

```yaml
ilab generate --model models/granite-7b-lab-Q4_K_M.gguf --num-instructions 100 --num-cpus 20 --server-ctx-size 20480
```

å¯ä»¥çœ‹åˆ°ç”Ÿæˆåˆå¹¶æ•°æ®çš„æ—¶å€™ï¼Œç³»ç»Ÿè°ƒç”¨äº†Apple Siliconçš„GPUåŠ é€Ÿ

æ‚¨å¯ä»¥åœ¨è¾“å‡ºä¸­çœ‹åˆ°ç”Ÿæˆçš„æ–°åˆæˆæ•°æ®é›†ã€‚å¦‚æœæ‚¨å¯¹ç”Ÿæˆçš„æ•°æ®é›†ä¸æ»¡æ„ï¼Œå¯ä»¥æŒ‰ é€€å‡ºè¯¥è¿‡ç¨‹`ctrl + c`ã€‚ä¿®æ”¹æ–‡ä»¶ä¸­çš„ç¤ºä¾‹`qna.yaml`ï¼Œç„¶åé‡æ–°è¿è¡Œ`generate`å‘½ä»¤ã€‚

æ­¤è¿‡ç¨‹å°†éœ€è¦ä¸€äº›æ—¶é—´ï¼Œå…·ä½“å–å†³äºæ‚¨çš„ç³»ç»Ÿã€‚åœ¨æˆ‘çš„ M1 Mac Pro ä¸Šå¤§çº¦éœ€è¦ 21 åˆ†é’Ÿã€‚æ‚¨å¯ä»¥åœ¨è¾“å‡ºä¸­çœ‹åˆ° ETAã€‚


ç”Ÿæˆåˆæˆæ•°æ®åï¼Œæ‚¨å°†çœ‹åˆ°å·²ç”Ÿæˆæ ·æœ¬æ•°å’Œå·²ä¸¢å¼ƒæ ·æœ¬æ•°çš„æ‘˜è¦

```yaml
INFO 2024-06-20 15:37:52,612 generate_data.py:608 101 instructions generated, 10 discarded due to format (see generated/discarded_granite-7b-lab-Q4_K_M_2024-06-20T15_16_44.log), 2 discarded due to rouge score
INFO 2024-06-20 15:37:52,612 generate_data.py:612 Generation took **1270.44s

-rw-r--r--  1 yehua  staff     9242  6 20 15:36 discarded_granite-7b-lab-Q4_K_M_2024-06-20T15_16_44.log #ä¸¢å¼ƒçš„æ•°æ®é›†ï¼ˆæ—¥å¿—æ–‡ä»¶ï¼‰
-rw-r--r--  1 yehua  staff  6280617  6 20 15:37 generated_granite-7b-lab-Q4_K_M_2024-06-20T15_16_44.json #ç”Ÿæˆçš„æ•°æ®é›†ï¼ˆjsonæ–‡ä»¶ï¼‰
-rw-r--r--  1 yehua  staff     2511  6 20 15:37 test_granite-7b-lab-Q4_K_M_2024-06-20T15_16_44.jsonl #æµ‹è¯•æ•°æ®é›†ï¼ˆjsonlæ–‡ä»¶ï¼‰
-rw-r--r--  1 yehua  staff    94763  6 20 15:37 train_granite-7b-lab-Q4_K_M_2024-06-20T15_16_44.jsonl #è®­ç»ƒæ•°æ®é›†ï¼ˆjsonlæ–‡ä»¶ï¼‰**
```

### æœ¬åœ°è®­ç»ƒæ¨¡å‹

ä¸€æ—¦åˆæˆæ•°æ®å‡†å¤‡å¥½äº†ï¼Œæ‚¨æ‰€è¦åšçš„å°±æ˜¯åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥è®­ç»ƒæ¨¡å‹ï¼š

ä¸ºäº†è®­ç»ƒè¯¥æ¨¡å¼ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`ilab`æŒ‡å‘æœ¬åœ°`GGUF`æ–‡ä»¶çš„ CLI â€” è®­ç»ƒç”¨äº`cuda-toolkit`ä¸åº•å±‚ NVIDIA GPU è¿›è¡Œäº¤äº’ã€‚

***æ³¨æ„***- ç¡®ä¿åœæ­¢æœåŠ¡æ¨¡å‹ä»¥ä¾¿ä¸ºè®­ç»ƒé˜¶æ®µé‡Šæ”¾ä¸€äº›èµ„æº

`ilab train --gguf-model-path models/granite-7b-lab-Q4_K_M.gguf`

`ilab train --gguf-model-path models/granite-7b-lab-Q4_K_M.gguf --device 'cuda'`

```yaml
ilab train --gguf-model-path models/granite-7b-lab-Q4_K_M.gguf
/Users/yehua/instructlab/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
[INFO] Loading
model-00003-of-00003.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54G/4.54G [05:06<00:00, 8.66MB/s]
model-00001-of-00003.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.94G/4.94G [05:09<00:00, 8.71MB/s]
model-00002-of-00003.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.00G/5.00G [05:18<00:00, 9.35MB/s]
Fetching 11 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [05:20<00:00, 29.18s/it]
/Users/yehua/instructlab/venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.00G/5.00G [05:18<00:00, 25.9MB/s]
  warnings.warn(
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 644/644 [00:00<00:00, 628kB/s]
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.33k/2.33k [00:00<00:00, 1.47MB/s]
tokenizer.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 493k/493k [00:00<00:00, 668kB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.80M/1.80M [00:00<00:00, 2.10MB/s]
added_tokens.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:00<00:00, 67.5kB/s]
special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 655/655 [00:00<00:00, 207kB/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
dtype=mlx.core.float16
[INFO] Quantizing
Using model_type='mistral'
Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total parameters 1244.079M
Trainable parameters 1.704M
Loading datasets
Training
Epoch 1: Iter 1: Val loss 2.087, Val took 26.183s
Iter 010: Train loss 1.848, It/sec 0.167, Tokens/sec 132.733
Epoch 1: Iter 10: Val loss 1.232, Val took 25.780s
Iter 10: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-010.npz.
Iter 020: Train loss 1.253, It/sec 0.195, Tokens/sec 133.049
Epoch 1: Iter 20: Val loss 1.053, Val took 25.881s
Iter 20: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-020.npz.
Iter 030: Train loss 0.924, It/sec 0.156, Tokens/sec 116.944
Epoch 2: Iter 30: Val loss 0.977, Val took 26.585s
Iter 30: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-030.npz.
Iter 040: Train loss 0.932, It/sec 0.167, Tokens/sec 121.478
Epoch 2: Iter 40: Val loss 0.937, Val took 26.198s
Iter 40: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-040.npz.
Iter 050: Train loss 0.763, It/sec 0.158, Tokens/sec 117.815
Epoch 3: Iter 50: Val loss 0.924, Val took 26.306s
Iter 50: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-050.npz.
Iter 060: Train loss 0.787, It/sec 0.165, Tokens/sec 119.941
Epoch 3: Iter 60: Val loss 0.903, Val took 27.079s
Iter 60: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-060.npz.
Iter 070: Train loss 0.668, It/sec 0.158, Tokens/sec 117.039
Epoch 4: Iter 70: Val loss 0.927, Val took 26.463s
Iter 70: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-070.npz.
Iter 080: Train loss 0.655, It/sec 0.132, Tokens/sec 96.649
Epoch 4: Iter 80: Val loss 0.911, Val took 26.960s
Iter 80: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-080.npz.
Iter 090: Train loss 0.599, It/sec 0.149, Tokens/sec 110.820
Epoch 5: Iter 90: Val loss 0.967, Val took 26.356s
Iter 90: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-090.npz.
Iter 100: Train loss 0.542, It/sec 0.152, Tokens/sec 111.259
Epoch 5: Iter 100: Val loss 0.947, Val took 28.326s
Iter 100: Saved adapter weights to instructlab-merlinite-7b-lab-mlx-q/adapters-100.npz.
```

æ­¤è¿‡ç¨‹å°†éœ€è¦ä¸€äº›æ—¶é—´ï¼Œå…·ä½“å–å†³äºæ‚¨çš„ç³»ç»Ÿ é…ç½®å’Œè¿­ä»£æ¬¡æ•°ã€‚åœ¨æˆ‘çš„ M1 MacBook Pro ä¸Šå®Œæˆ 100 æ¬¡è¿­ä»£å¤§çº¦éœ€è¦ 30 åˆ†é’Ÿã€‚æ‚¨å¯ä»¥åœ¨è¾“å‡ºä¸­çœ‹åˆ° ETAã€‚

ç›®å½•ä¸­å°†åˆ›å»ºä¸€ä¸ªæ–°ç›®å½•`ilab`ï¼Œå…¶åç§°ç±»ä¼¼äºï¼š`instructlab-merlinite-7b-lab`ã€‚æ­¤ç›®å½•å°†åŒ…å«æ–°çš„æ¨¡å‹æƒé‡å’Œé€‚é…å™¨ã€‚

```yaml
drwxr-xr-x  14 yehua  staff   448  6 20 18:20 instructlab-merlinite-7b-lab
drwxr-xr-x  20 yehua  staff   640  6 20 18:36 instructlab-merlinite-7b-lab-mlx-q
```

### æµ‹è¯•æ¨¡å‹

é€šè¿‡è¿è¡Œå‘½ä»¤æ¥æµ‹è¯•æ–°è®­ç»ƒçš„æ¨¡å‹`ilab test`ï¼Œä»¥æµ‹è¯•æ¨¡å‹å¹¶éªŒè¯å…¶æ€§èƒ½ã€‚

`æ ¼å¼ï¼š ilab test --data-dir my-data --model-dir models/ibm/merlinite-7b`

`ilab test --data-dir ./taxonomy_data --model-dir instructlab-merlinite-7b-lab-mlx-q`

```yaml
ilab test

system prompt: You are an AI language model developed by IBM Research. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior.
[1]
 user prompt: Who directed the movie â€œOppenheimerâ€?
expected output: The movie â€œOppenheimerâ€ was written, directed, and produced by Christopher Nolan1.

-----model output BEFORE training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
LoRA init skipped
Total parameters 1242.375M
Trainable parameters 0.000M
Loading datasets
LoRA loading skipped
Generating
==========
Christopher Nolan
==========

-----model output AFTER training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total parameters 1244.079M
Trainable parameters 1.704M
Loading datasets
Generating
==========
Christopher Nolan directed the movie â€œOppenheimer.â€ He is known for his involvement in the production of the film and was chosen by the studio to take on this project.
==========
[2]
 user prompt: What is the movie â€œOppenheimerâ€ about?
expected output: The movie follows the life of J. Robert Oppenheimer, the American theoretical physicist who helped develop the first nuclear weapons during World War II.

-----model output BEFORE training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
LoRA init skipped
Total parameters 1242.375M
Trainable parameters 0.000M
Loading datasets
LoRA loading skipped
Generating
==========
"Oppenheimer" is a film that delves into the life and times of J. Robert Oppenheimer, the brilliant physicist who led the Manhattan Project during World War II. The movie offers a gripping and thought-provoking portrayal of Oppenheimer's personal and professional journey while working on the development of the atomic bomb. Here are some key aspects of the movie and the historical context that you might find useful for your article:

1.
==========

-----model output AFTER training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total parameters 1244.079M
Trainable parameters 1.704M
Loading datasets
Generating
==========
"Oppenheimer" is a biographical film directed by Christopher Nolan that tells the story of J. Robert Oppenheimer, the father of the atomic bomb and the director of the Los Alamos Laboratory during World War II. The movie explores Oppenheimer's personal life, his contributions to the development of nuclear weapons, and the impact of his actions on the world.

The film portrays Oppenheimer's guilt over the destruction caused by the
==========
[3]
 user prompt: Who starred in the movie â€œOppenheimerâ€?
expected output: The movie starred Cillian Murphy as Oppenheimer, alongside Robert Downey Jr. as the United States Atomic Energy Commission member Lewis Strauss. The ensemble supporting cast includes Emily Blunt, Matt Damon, Florence Pugh, Josh Hartnett, Casey Affleck, Rami Malek, and Kenneth Branagh

-----model output BEFORE training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
LoRA init skipped
Total parameters 1242.375M
Trainable parameters 0.000M
Loading datasets
LoRA loading skipped
Generating
==========
"Oppenheimer" is a historical drama film directed by Christopher Nolan, with an impressive cast and crew. The lead role of Dr. J. Robert Oppenheimer, the physicist who oversaw the Manhattan Project, is portrayed by a well-known actor, Cillian Murphy. Additionally, the film features other esteemed actors, including Emily Blunt as Katherine, Olivia Thirlby as Leona Woods, Matthew Goode as Klaus Fuch
==========

-----model output AFTER training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total parameters 1244.079M
Trainable parameters 1.704M
Loading datasets
Generating
==========
Cillian Murphy, Matt Damon, Emily Blunt, Robert Downey Jr., Gary Oldman, and Benny Safdie.
==========
[4]
 user prompt: When was the movie â€œOppenheimerâ€ released?
expected output: The movie â€œOppenheimerâ€ was released on July 21, 2023

-----model output BEFORE training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
LoRA init skipped
Total parameters 1242.375M
Trainable parameters 0.000M
Loading datasets
LoRA loading skipped
Generating
==========
I'm sorry for the confusion, but I couldn't find information about the release date of the movie "Oppenheimer." Would you like me to search for more details about the movie or answer a different question?
==========

-----model output AFTER training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total parameters 1244.079M
Trainable parameters 1.704M
Loading datasets
Generating
==========
The movie was released in theaters on July 21, 2023.
==========
[5]
 user prompt: How was the movie â€œOppenheimerâ€ received?
expected output: The movie â€œOppenheimerâ€ received critical acclaim and won seven Academy Awards, including Best Picture, Best Director for Nolan, Best Actor for Murphy and Best Supporting Actor for Downey. It grossed over $976 million worldwide, becoming the third-highest-grossing film of 2023, the highest-grossing World War II-related film, the highest-grossing biographical film and the second-highest-grossing R-rated film

-----model output BEFORE training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
LoRA init skipped
Total parameters 1242.375M
Trainable parameters 0.000M
Loading datasets
LoRA loading skipped
Generating
==========
The movie "Oppenheimer" has received mixed reviews from critics and audiences alike. While some laud the film for its powerful performances and thought-provoking themes, others have criticized it for its pacing and structure.

Critics have praised the movie's direction, cinematography, and visual effects. The performances of the lead actors, particularly Cillian Murphy as J. Robert Oppenheimer, have been widely praised. However, some have criticized
==========

-----model output AFTER training----:

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Total parameters 1244.079M
Trainable parameters 1.704M
Loading datasets
Generating
==========
The movie "Oppenheimer" received mixed reviews from critics and audiences. It was praised for its visual effects and historical accuracy, but criticized for its length and slow pacing. Some viewers felt that the film could have been more engaging and emotionally resonant.
```

### **é‡åŒ–ç²¾ç»†è°ƒæ•´çš„æ¨¡å‹**

ç°åœ¨è®­ç»ƒå·²ç»å®Œæˆï¼Œæ‚¨åº”è¯¥æœŸæœ›`GGUF`åœ¨è¯¥æ¨¡å‹è·¯å¾„ä¸‹æ‹¥æœ‰æ–°æ¨¡å‹ï¼ˆåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­æ˜¯æ¨¡å‹ç›®å½•ï¼‰ã€‚

ä¸ºäº†åœ¨æœªæ¥çš„æ–‡ç« ä¸­ä½¿ç”¨è¯¥æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦å®ƒå…·æœ‰åˆç†çš„å¤§å°ï¼Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†é‡åŒ–æ¨¡å‹

`ilab convert`

```bash
ilab convert

Loading pretrained model
Using model_type='mistral'
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
INFO 2024-06-20 18:48:22,055 lab.py:1343 deleting instructlab-merlinite-7b-lab-mlx-q...
[INFO] Loading
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
dtype=<class 'numpy.float16'>
INFO 2024-06-20 18:50:26,422 lab.py:1352 deleting instructlab-merlinite-7b-lab-mlx-q-fused...
Loading model file instructlab-merlinite-7b-lab-trained/model.safetensors
params = Params(n_vocab=32008, n_embd=4096, n_layer=32, n_ctx=32768, n_ff=14336, n_head=32, n_head_kv=8, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('instructlab-merlinite-7b-lab-trained'))
Found vocab files: {'spm': PosixPath('instructlab-merlinite-7b-lab-trained/tokenizer.model'), 'bpe': None, 'hfft': PosixPath('instructlab-merlinite-7b-lab-trained/tokenizer.json')}
Loading vocab file PosixPath('instructlab-merlinite-7b-lab-trained/tokenizer.model'), type 'spm'
Vocab info: <SentencePieceVocab with 32000 base tokens and 5 added tokens>
Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 32000, 'unk': 0, 'pad': 32001}, add special tokens {'bos': False, 'eos': False}>
Permuting layer 0
Permuting layer 1
Permuting layer 2
Permuting layer 3
Permuting layer 4
Permuting layer 5
Permuting layer 6
Permuting layer 7
Permuting layer 8
Permuting layer 9
Permuting layer 10
Permuting layer 11
Permuting layer 12
Permuting layer 13
Permuting layer 14
Permuting layer 15
Permuting layer 16
Permuting layer 17
Permuting layer 18
Permuting layer 19
Permuting layer 20
Permuting layer 21
Permuting layer 22
Permuting layer 23
Permuting layer 24
Permuting layer 25
Permuting layer 26
Permuting layer 27
Permuting layer 28
Permuting layer 29
Permuting layer 30
Permuting layer 31
lm_head.weight                                   -> output.weight                            | F16    | [32008, 4096]
model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32008, 4096]
model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]
model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 14336]
model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [14336, 4096]
model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [14336, 4096]
model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]
model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [1024, 4096]
model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]
model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]
model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [1024, 4096]
model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]
model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 14336]
model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [14336, 4096]
model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [14336, 4096]
model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]
model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [1024, 4096]
model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]
model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]
model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [1024, 4096]
model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]
model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]
model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [1024, 4096]
model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]
model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]
model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [1024, 4096]
model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]
model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]
model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [1024, 4096]
model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]
model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]
model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [1024, 4096]
model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]
model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]
model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [1024, 4096]
model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]
model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]
model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [1024, 4096]
model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]
model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]
model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [1024, 4096]
model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]
model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]
model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [1024, 4096]
model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]
model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]
model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [1024, 4096]
model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]
model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]
model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [1024, 4096]
model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]
model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]
model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [1024, 4096]
model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]
model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]
model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [1024, 4096]
model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]
model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]
model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [1024, 4096]
model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]
model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]
model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [1024, 4096]
model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]
model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]
model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [1024, 4096]
model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]
model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]
model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [1024, 4096]
model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]
model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]
model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [1024, 4096]
model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]
model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]
model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [1024, 4096]
model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]
model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]
model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [1024, 4096]
model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]
model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]
model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [1024, 4096]
model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]
model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 14336]
model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [14336, 4096]
model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [14336, 4096]
model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]
model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [1024, 4096]
model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]
model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]
model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [1024, 4096]
model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]
model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]
model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [1024, 4096]
model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]
model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]
model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [1024, 4096]
model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]
model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]
model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [1024, 4096]
model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]
model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]
model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [1024, 4096]
model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]
model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]
model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [1024, 4096]
model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]
model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]
model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [1024, 4096]
model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]
model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]
model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [1024, 4096]
model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]
model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]
model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [1024, 4096]
model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]
model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]
model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [1024, 4096]
model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]
model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]
model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [1024, 4096]
model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]
model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]
model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [1024, 4096]
model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]
model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]
model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [1024, 4096]
model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]
model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]
model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [1024, 4096]
model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]
model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]
model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [1024, 4096]
model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]
model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]
model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [1024, 4096]
model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]
model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]
model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [1024, 4096]
model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]
model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]
model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [1024, 4096]
model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]
model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]
model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [1024, 4096]
model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]
model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]
model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [1024, 4096]
model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]
model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]
model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [1024, 4096]
model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]
model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 14336]
model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [14336, 4096]
model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [14336, 4096]
model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]
model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [1024, 4096]
model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]
model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]
model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [1024, 4096]
model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]
model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]
model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [1024, 4096]
model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]
model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]
model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [1024, 4096]
model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]
model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 14336]
model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [14336, 4096]
model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [14336, 4096]
model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]
model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [1024, 4096]
model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]
model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]
model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [1024, 4096]
model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]
model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 14336]
model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [14336, 4096]
model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [14336, 4096]
model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]
model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [1024, 4096]
model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]
model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [1024, 4096]
model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]
model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 14336]
model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [14336, 4096]
model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [14336, 4096]
model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]
model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [1024, 4096]
model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]
model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]
model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [1024, 4096]
model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]
model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 14336]
model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [14336, 4096]
model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [14336, 4096]
model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]
model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [1024, 4096]
model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]
model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]
model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [1024, 4096]
model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]
model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 14336]
model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [14336, 4096]
model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [14336, 4096]
model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]
model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [1024, 4096]
model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]
model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]
model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [1024, 4096]
model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]
model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 14336]
model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [14336, 4096]
model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [14336, 4096]
model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]
model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [1024, 4096]
model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]
model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]
model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [1024, 4096]
model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]
model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 14336]
model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [14336, 4096]
model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [14336, 4096]
model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]
model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [1024, 4096]
model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]
model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]
model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [1024, 4096]
model.norm.weight                                -> output_norm.weight                       | F16    | [4096]
Writing instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab.gguf, format 1
Padding vocab with 3 token(s) - <dummy00001> through <dummy00003>
gguf: This GGUF file is for Little Endian only
gguf: Setting special token type bos to 1
gguf: Setting special token type eos to 32000
gguf: Setting special token type unk to 0
gguf: Setting special token type pad to 32001
gguf: Setting add_bos_token to False
gguf: Setting add_eos_token to False
gguf: Setting chat_template to {% for message in messages %}{% if message['role'] == 'system' %}{{'<|system|>'+ '
' + message['content'] + '
'}}{% elif message['role'] == 'user' %}{{'<|user|>' + '
' + message['content'] + '
'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>' + '
' + message['content'] + '<|endoftext|>' + ('' if loop.last else '
')}}{% endif %}{% endfor %}
[  1/291] Writing tensor output.weight                          | size  32008 x   4096  | type F16  | T+   0
[  2/291] Writing tensor token_embd.weight                      | size  32008 x   4096  | type F16  | T+   0
[  3/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   0
[  4/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   0
[  5/291] Writing tensor blk.0.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   0
[  6/291] Writing tensor blk.0.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   0
[  7/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   0
[  8/291] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   0
[  9/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1
[ 10/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 11/291] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   1
[ 12/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   1
[ 13/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   1
[ 14/291] Writing tensor blk.1.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   1
[ 15/291] Writing tensor blk.1.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   1
[ 16/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   1
[ 17/291] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   1
[ 18/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   1
[ 19/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   1
[ 20/291] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   1
[ 21/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+   1
[ 22/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   2
[ 23/291] Writing tensor blk.10.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   2
[ 24/291] Writing tensor blk.10.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   2
[ 25/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+   2
[ 26/291] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   2
[ 27/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+   2
[ 28/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   2
[ 29/291] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   2
[ 30/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+   2
[ 31/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   2
[ 32/291] Writing tensor blk.11.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   2
[ 33/291] Writing tensor blk.11.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   2
[ 34/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+   2
[ 35/291] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   2
[ 36/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+   2
[ 37/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   2
[ 38/291] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   2
[ 39/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+   2
[ 40/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   3
[ 41/291] Writing tensor blk.12.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   3
[ 42/291] Writing tensor blk.12.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   3
[ 43/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+   3
[ 44/291] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   3
[ 45/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3
[ 46/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3
[ 47/291] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   3
[ 48/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+   3
[ 49/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   3
[ 50/291] Writing tensor blk.13.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   3
[ 51/291] Writing tensor blk.13.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   3
[ 52/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+   3
[ 53/291] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   3
[ 54/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3
[ 55/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3
[ 56/291] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   3
[ 57/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+   3
[ 58/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   4
[ 59/291] Writing tensor blk.14.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   4
[ 60/291] Writing tensor blk.14.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   4
[ 61/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+   4
[ 62/291] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   4
[ 63/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4
[ 64/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4
[ 65/291] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   4
[ 66/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+   4
[ 67/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   4
[ 68/291] Writing tensor blk.15.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   4
[ 69/291] Writing tensor blk.15.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   4
[ 70/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+   4
[ 71/291] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   4
[ 72/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+   4
[ 73/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   4
[ 74/291] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   4
[ 75/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+   4
[ 76/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   5
[ 77/291] Writing tensor blk.16.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   5
[ 78/291] Writing tensor blk.16.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   5
[ 79/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+   5
[ 80/291] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   5
[ 81/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5
[ 82/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5
[ 83/291] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   5
[ 84/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+   5
[ 85/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   5
[ 86/291] Writing tensor blk.17.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   5
[ 87/291] Writing tensor blk.17.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   5
[ 88/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+   5
[ 89/291] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   5
[ 90/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+   5
[ 91/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   5
[ 92/291] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   5
[ 93/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+   5
[ 94/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   6
[ 95/291] Writing tensor blk.18.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   6
[ 96/291] Writing tensor blk.18.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   6
[ 97/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+   6
[ 98/291] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   6
[ 99/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6
[100/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6
[101/291] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   6
[102/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+   6
[103/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   6
[104/291] Writing tensor blk.19.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   6
[105/291] Writing tensor blk.19.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   6
[106/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+   6
[107/291] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   6
[108/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6
[109/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6
[110/291] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   7
[111/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   7
[112/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   7
[113/291] Writing tensor blk.2.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   7
[114/291] Writing tensor blk.2.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   7
[115/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   7
[116/291] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   7
[117/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   7
[118/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   7
[119/291] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   7
[120/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+   7
[121/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   8
[122/291] Writing tensor blk.20.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   8
[123/291] Writing tensor blk.20.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   8
[124/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+   8
[125/291] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   8
[126/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8
[127/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8
[128/291] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   8
[129/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+   8
[130/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   9
[131/291] Writing tensor blk.21.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   9
[132/291] Writing tensor blk.21.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   9
[133/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+   9
[134/291] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   9
[135/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9
[136/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9
[137/291] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   9
[138/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+   9
[139/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  10
[140/291] Writing tensor blk.22.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  10
[141/291] Writing tensor blk.22.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  10
[142/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  10
[143/291] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  10
[144/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+  10
[145/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  10
[146/291] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  10
[147/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  10
[148/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  11
[149/291] Writing tensor blk.23.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  11
[150/291] Writing tensor blk.23.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  11
[151/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+  11
[152/291] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  11
[153/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+  11
[154/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  11
[155/291] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  11
[156/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+  11
[157/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  12
[158/291] Writing tensor blk.24.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  12
[159/291] Writing tensor blk.24.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  12
[160/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+  12
[161/291] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  12
[162/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+  12
[163/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  12
[164/291] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  12
[165/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+  12
[166/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  13
[167/291] Writing tensor blk.25.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  13
[168/291] Writing tensor blk.25.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  13
[169/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+  13
[170/291] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  13
[171/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+  13
[172/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  13
[173/291] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  13
[174/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+  13
[175/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  14
[176/291] Writing tensor blk.26.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  14
[177/291] Writing tensor blk.26.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  14
[178/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+  14
[179/291] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  14
[180/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+  14
[181/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  14
[182/291] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  14
[183/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  14
[184/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  15
[185/291] Writing tensor blk.27.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  15
[186/291] Writing tensor blk.27.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  15
[187/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+  15
[188/291] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  15
[189/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+  15
[190/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  15
[191/291] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  15
[192/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  15
[193/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  16
[194/291] Writing tensor blk.28.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  16
[195/291] Writing tensor blk.28.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  16
[196/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  16
[197/291] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  16
[198/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+  16
[199/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  16
[200/291] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  16
[201/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  16
[202/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  17
[203/291] Writing tensor blk.29.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  17
[204/291] Writing tensor blk.29.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  17
[205/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  17
[206/291] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  17
[207/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+  17
[208/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  17
[209/291] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  17
[210/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  17
[211/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  18
[212/291] Writing tensor blk.3.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  18
[213/291] Writing tensor blk.3.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  18
[214/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  18
[215/291] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  18
[216/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+  18
[217/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  18
[218/291] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  18
[219/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+  18
[220/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  19
[221/291] Writing tensor blk.30.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  19
[222/291] Writing tensor blk.30.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  19
[223/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+  19
[224/291] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  19
[225/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+  19
[226/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  19
[227/291] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  19
[228/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+  19
[229/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  20
[230/291] Writing tensor blk.31.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  20
[231/291] Writing tensor blk.31.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  20
[232/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  20
[233/291] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  20
[234/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+  20
[235/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  20
[236/291] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  20
[237/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  20
[238/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  21
[239/291] Writing tensor blk.4.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  21
[240/291] Writing tensor blk.4.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  21
[241/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  21
[242/291] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  21
[243/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+  21
[244/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  21
[245/291] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  21
[246/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  21
[247/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  22
[248/291] Writing tensor blk.5.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  22
[249/291] Writing tensor blk.5.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  22
[250/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  22
[251/291] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  22
[252/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+  22
[253/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  22
[254/291] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  22
[255/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  22
[256/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  23
[257/291] Writing tensor blk.6.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  23
[258/291] Writing tensor blk.6.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  23
[259/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  23
[260/291] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  23
[261/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+  23
[262/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  23
[263/291] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  23
[264/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  23
[265/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  24
[266/291] Writing tensor blk.7.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  24
[267/291] Writing tensor blk.7.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  24
[268/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  24
[269/291] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  24
[270/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+  24
[271/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  24
[272/291] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  24
[273/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  24
[274/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  25
[275/291] Writing tensor blk.8.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  25
[276/291] Writing tensor blk.8.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  25
[277/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  25
[278/291] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  25
[279/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+  25
[280/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  25
[281/291] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  25
[282/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  25
[283/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  26
[284/291] Writing tensor blk.9.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  26
[285/291] Writing tensor blk.9.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  26
[286/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  26
[287/291] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  26
[288/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+  26
[289/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  26
[290/291] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  26
[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+  26
Wrote instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab.gguf
INFO 2024-06-20 18:50:54,938 lab.py:1362 deleting safetensors files from instructlab-merlinite-7b-lab-trained...
main: build = 1 (784e11d)
main: built with Apple clang version 15.0.0 (clang-1500.0.40.1) for arm64-apple-darwin23.4.0
main: quantizing 'instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab.gguf' to 'instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab-Q4_K_M.gguf' as Q4_K_M
llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = .
llama_model_loader: - kv   2:                       llama.context_length u32              = 32768
llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   4:                          llama.block_count u32              = 32
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000
llama_model_loader: - kv  11:                          general.file_type u32              = 1
llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama
llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32008]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32008]   = [0.000000, 0.000000, 0.000000, 0.0000...
llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32008]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000
llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32001
llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
llama_model_quantize_internal: meta size = 735584 bytes
[   1/ 291]                        output.weight - [ 4096, 32008,     1,     1], type =    f16, converting to q6_K .. size =   250.06 MiB ->   102.56 MiB
[   2/ 291]                    token_embd.weight - [ 4096, 32008,     1,     1], type =    f16, converting to q4_K .. size =   250.06 MiB ->    70.33 MiB
[   3/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[   4/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[   5/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[   6/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[   7/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[   8/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[   9/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  10/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  11/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  12/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  13/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[  14/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  15/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  16/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  17/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  18/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  19/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  20/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  21/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  22/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[  23/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  24/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  25/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  26/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  27/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  28/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  29/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  30/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  31/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[  32/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  33/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  34/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  35/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  36/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  37/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  38/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  39/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  40/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  41/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  42/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  43/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  44/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  45/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  46/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  47/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  48/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  49/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  50/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  51/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  52/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  53/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  54/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  55/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  56/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  57/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  58/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[  59/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  60/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  61/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  62/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  63/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  64/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  65/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  66/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  67/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  68/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  69/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  70/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  71/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  72/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  73/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  74/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  75/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  76/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  77/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  78/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  79/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  80/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  81/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  82/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  83/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  84/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  85/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[  86/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  87/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  88/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  89/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  90/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  91/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[  92/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  93/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  94/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  95/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  96/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[  97/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  98/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[  99/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 100/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 101/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 102/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 103/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 104/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 105/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 106/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 107/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 108/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 109/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 110/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 111/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 112/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 113/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 114/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 115/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 116/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 117/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 118/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 119/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 120/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 121/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 122/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 123/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 124/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 125/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 126/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 127/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 128/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 129/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 130/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 131/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 132/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 133/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 134/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 135/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 136/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 137/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 138/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 139/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 140/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 141/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 142/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 143/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 144/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 145/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 146/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 147/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 148/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 149/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 150/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 151/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 152/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 153/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 154/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 155/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 156/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 157/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 158/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 159/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 160/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 161/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 162/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 163/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 164/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 165/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 166/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 167/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 168/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 169/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 170/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 171/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 172/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 173/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 174/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 175/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 176/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 177/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 178/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 179/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 180/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 181/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 182/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 183/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 184/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 185/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 186/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 187/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 188/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 189/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 190/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 191/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 192/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 193/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 194/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 195/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 196/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 197/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 198/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 199/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 200/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 201/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 202/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 203/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 204/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 205/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 206/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 207/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 208/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 209/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 210/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 211/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 212/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 213/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 214/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 215/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 216/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 217/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 218/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 219/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 220/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 221/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 222/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 223/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 224/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 225/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 226/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 227/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 228/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 229/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 230/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 231/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 232/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 233/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 234/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 235/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 236/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 237/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 238/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 239/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 240/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 241/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 242/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 243/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 244/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 245/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 246/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 247/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 248/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 249/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 250/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 251/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 252/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 253/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 254/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 255/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 256/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 257/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 258/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 259/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 260/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 261/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 262/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 263/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 264/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 265/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 266/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 267/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 268/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 269/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 270/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 271/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 272/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 273/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 274/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 275/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 276/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 277/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 278/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 279/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 280/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 281/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 282/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 283/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 284/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 285/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q4_K .. size =   112.00 MiB ->    31.50 MiB
[ 286/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 287/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q4_K .. size =     8.00 MiB ->     2.25 MiB
[ 288/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 289/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q4_K .. size =    32.00 MiB ->     9.00 MiB
[ 290/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
llama_model_quantize_internal: model size  = 13813.14 MB
llama_model_quantize_internal: quant size  =  4165.41 MB
INFO 2024-06-20 18:51:53,586 lab.py:1372 deleting instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab.gguf...
```

è¿è¡Œè¯¥å‘½ä»¤åï¼Œæ‰€æœ‰æƒé‡å’Œé€‚é…å™¨éƒ½å°†è½¬æ¢ä¸ºé‡åŒ–çš„ gguf æ¨¡å‹ã€‚ç›®å½•ä¸­å°†åˆ›å»ºä¸€ä¸ªç›®å½•`ilab`ï¼Œå…¶åç§°ç±»ä¼¼äºï¼š`instructlab-merlinite-7b-lab-trained`ã€‚

<aside>
ğŸ’¡ `Llama.cpp` çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºå…¶é™æ€åˆ†æå’Œä»£ç ç”ŸæˆåŠŸèƒ½ã€‚å°†æ·±åº¦å­¦ä¹ æ¨¡å‹é‡åŒ–ï¼Œå¹¶ä¸”åœ¨ä½é…ç½®æœºå‹ï¼ˆæ²¡æœ‰GPUï¼‰ä¸Šå°±å¯ä»¥åšåˆ°æ¨¡å‹æ¨ç†ã€‚åˆå¹¶æƒé‡ï¼Œç”Ÿæˆå…¨é‡æ¨¡å‹æƒé‡ã€‚

</aside>

### **éªŒè¯æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹**

è¿è¡Œæ–°è®­ç»ƒçš„æ¨¡å‹

`ilab serve --model-path instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab-Q4_K_M.gguf --num-threads 14 --max-ctx-size 20480`

åœ¨ç”Ÿæˆåˆæˆæ•°æ®é›†ã€è®­ç»ƒæ¨¡å‹ã€æµ‹è¯•æ¨¡å‹çš„ç¬¬äºŒä¸ªç»ˆç«¯ä¸­ï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ä¸æ¨¡å‹èŠå¤©ï¼š

`ilab chat -gm -m instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab-Q4_K_M.gguf`

<aside>
ğŸ’¡ Greedy decoding æ¨¡å‹æ¯ä¸€æ­¥éƒ½é€‰æ‹©å…·æœ‰æœ€é«˜æ¦‚ç‡çš„è¯ä½œä¸ºè¾“å‡ºï¼Œè€Œä¸è€ƒè™‘æ•´ä¸ªå¥å­çš„å…¨å±€æœ€ä¼˜æ€§ã€‚åœ¨æ¯ä¸€æ­¥ç”Ÿæˆæ—¶ï¼Œæ¨¡å‹æ ¹æ®å½“å‰è¾“å…¥å’Œä¸Šä¸‹æ–‡è®¡ç®—æ¯ä¸ªå¯èƒ½è¯çš„æ¦‚ç‡ï¼Œç„¶åé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯ä½œä¸ºå½“å‰æ­¥çš„è¾“å‡ºã€‚è®¡ç®—é€Ÿåº¦å¾ˆå¿«ï¼Œä¸éœ€è¦å¤æ‚çš„æœç´¢ç®—æ³•ï¼Œå› æ­¤åœ¨å®é™…åº”ç”¨ä¸­èƒ½å¿«é€Ÿç”Ÿæˆæ–‡æœ¬ï¼Œä½†æ˜¯é€šå¸¸ç¼ºä¹å¤šæ ·æ€§ï¼Œå¯èƒ½ä¼šäº§ç”Ÿé‡å¤æˆ–å•è°ƒçš„å¥å­ã€‚åªè€ƒè™‘å±€éƒ¨æœ€ä¼˜ï¼Œå¯èƒ½ä¼šé”™è¿‡ä¸€äº›æ›´å¥½çš„å¥å­ç»“æ„æˆ–ä¸Šä¸‹æ–‡ï¼Œå¯¼è‡´ç”Ÿæˆçš„æ–‡æœ¬åœ¨å…¨å±€ä¸Šä¸æ˜¯æœ€ä¼˜çš„ã€‚æ€»çš„æ¥è¯´ï¼Œgreedy decoding æ˜¯ä¸€ç§ç®€å•å¿«é€Ÿä½†å¯èƒ½ä¸å®Œç¾çš„æ–‡æœ¬ç”Ÿæˆç­–ç•¥ï¼Œé€‚ç”¨äºå¯¹ç”Ÿæˆé€Ÿåº¦æœ‰è¾ƒé«˜è¦æ±‚ä½†å¯¹æ–‡æœ¬å¤šæ ·æ€§è¦æ±‚ä¸é«˜çš„åº”ç”¨åœºæ™¯ã€‚

</aside>

---

**åœ¨æ²¡æœ‰ç”Ÿæˆæ–°çš„è®­ç»ƒæ•°æ®é›†ä¹‹å‰ï¼Œç”¨ä¸‹é¢çš„é—®é¢˜æµ‹è¯•ï¼Œè¿”å›å‡ºçš„ç­”æ¡ˆï¼Œå¯ä»¥çœ‹å‡ºæ¥ä¿¡æ¯ä¸æ­£ç¡®ï¼Œä¸èƒ½è¾¾åˆ°é¢„æœŸ**
`ilab chat -m models/granite-7b-lab-Q4_K_M.gguf`

**Who starred in the movie Oppenheimer?**

The movie "Oppenheimer" is **an upcoming film** adaptation of the Pulitzer Prize-winning biography "American Prometheus: The Life of J. Robert Oppenheimer" by Kira Davis. Cillian Murphy has been cast as the title character, J. Robert Oppenheimer. Other notable stars who have joined the cast include Matt Damon as Leslie Groves, the head of the Yale project and the father of Oppenheimer's wife, Emily Blunt as Katherine Manning Oppenheimer, the wife of J. Robert Oppenheimer, and Alden Ehrenreich as Donald Oppenheimer, their son.

The film is directed by Christopher Nolan, known for his work on blockbuster movies like "Batman Begins," "The Dark Knight," and "Interstellar." "Oppenheimer" is set to explore the life of J. Robert Oppenheimer, the man who led the project that developed the first atomic bomb during World War II. The movie is expected to delve into the moral complexities of nuclear power and the human cost of scientific progress.

While there isn't much information available about the plot yet, I can provide you with some context on the historical figures and events that "Oppenheimer" might depict. The film will likely cover Oppenheimer's early life, his education at the prestigious Institute for Advanced Studies in Princeton, New Jersey, and his work at the Los Alamos National Laboratory during the Manhattan Project. Additionally, the movie may explore Oppenheimer's struggles with the moral implications of nuclear weapons and his eventual condemnation by the U.S. government during the Red Scare.

**Stay tuned for more updates** on "Oppenheimer" as filming progresses and **more information becomes available**. If you have any questions about the cast, directors, or historical context of the movie, please let me know!

**What are the release dates for Oppenheimer movie?**

As of my last update, the release date for "Oppenheimer" **is not yet set**. The film was announced in 2021, and production has recently started. Christopher Nolan, the director, often takes his time to perfect his films before releasing them. Given the complexity of the subject matter and the intricate plot, it's likely that "Oppenheimer" will require a considerable amount of post-production work.

**Once filming is complete and the movie is ready for release**, the production team will announce an official date. Typically, this happens several months before the release date. I recommend checking official film news sources and the film's official website for the most up-to-date information on the release date.

In the meantime, you can look forward to other movies released by Christopher Nolan or explore other fascinating historical figures and events that have been brought to the big screen. For example, "Dunkirk" (2017) and "Interstellar" (2014) are two of Nolan's previous films that showcase his ability to create immersive and thought-provoking cinematic experiences.

**åœ¨åŠ å…¥äº†knowledgeå’Œä¸Šä¸‹æ–‡å‚è€ƒæ•°æ®ï¼Œå¹¶ä¸”ä½¿ç”¨è®­ç»ƒçš„æ–°æ¨¡å‹ä¹‹åï¼Œç­”æ¡ˆåŸºæœ¬å¯ä»¥è¾¾åˆ°é¢„æœŸ**
 `ilab chat -gm -m instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab-Q4_K_M.gguf`

**Who starred in the movie Oppenheimer?**

Sean Penn portrayed J. Robert Oppenheimer in the movie "Oppenheimer." He was nominated for an Academy Award â”‚
â”‚ for Best Actor for his performance.

**What are the release dates for Oppenheimer movie?**

The film Oppenheimer was released on March 17, 2023, in the United States after being shown at the Cannes Film Festival in 2022. It has been available to stream on HBO Max since its release.

```bash

â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ system â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Welcome to InstructLab Chat w/ INSTRUCTLAB-MERLINITE-7B-LAB-TRAINED/INSTRUCTLAB-MERLINITE-7B-LAB-Q4_K_M.GGUF (type /h for help)                                                                      â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
>>> Who starred in the movie Oppenheimer?                                                                                                                                                   [S][default]
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab-Q4_K_M.gguf â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Oppenheimer was written and directed by James Schamus, and it features Daniel Day-Lewis in the title role. Other notable actors in this film include Emily Blunt as Katherine "Kitty" Oppenheimer,   â”‚
â”‚ Robert Wagner as General Leslie Groves, and Michael Nyqvist as Albert Einstein. This movie is a historical drama that depicts the life of J. Robert Oppenheimer during his time at Los Alamos, where â”‚
â”‚ he led the development of the atomic bomb during World War II.                                                                                                                                       â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ elapsed 2.466 seconds â”€â•¯
>>> What are the release dates for Oppenheimer movie?                                                                                                                                       [S][default]
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ instructlab-merlinite-7b-lab-trained/instructlab-merlinite-7b-lab-Q4_K_M.gguf â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ The Oppenheimer movie was released in 2023. It premiered on March 14th and started to stream on Hulu on April 7th.                                                                                   â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ elapsed 0.829 seconds â”€â•¯
>>>
```

å‚è€ƒæ–‡ç« ï¼š

[IBM Developer](https://developer.ibm.com/tutorials/awb-train-open-source-llms-collected-knowledge-instructlab/)
